{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from osgeo import ogr, osr\n",
    "from sentinelhub import WmsRequest, WcsRequest, MimeType, CRS, BBox, constants, DataSource, CustomUrlParam\n",
    "from s2cloudless import S2PixelCloudDetector, CloudMaskRequest\n",
    "import logging\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from skimage.transform import resize\n",
    "\n",
    "import scipy.sparse as sparse\n",
    "from scipy.sparse.linalg import splu\n",
    "\n",
    "\n",
    "with open(\"../config.yaml\", 'r') as stream:\n",
    "        key = (yaml.safe_load(stream))\n",
    "        API_KEY = key['key'] \n",
    "        \n",
    "%run ../src/slope.py\n",
    "%run ../src/utils-bilinear.py\n",
    "%run ../src/dsen2/utils/DSen2Net.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = ('2018-12-15', '2020-01-15')\n",
    "EPSG = CRS.WGS84\n",
    "IMSIZE = 48\n",
    "cloud_detector = S2PixelCloudDetector(threshold=0.4, average_over=4, dilation_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions (to be moved to a utils file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbolic Model Created.\n"
     ]
    }
   ],
   "source": [
    "def calculate_proximal_steps_index(date, satisfactory):\n",
    "    arg_before = None\n",
    "    arg_after = None\n",
    "    if date > 0:\n",
    "        idx_before = satisfactory - date\n",
    "        arg_before = idx_before[np.where(idx_before < 0, idx_before, -np.inf).argmax()]\n",
    "    if date < np.max(satisfactory):\n",
    "        idx_after = satisfactory - date\n",
    "        arg_after = idx_after[np.where(idx_after > 0, idx_after, np.inf).argmin()]\n",
    "    if not arg_after and not arg_before:\n",
    "        arg_after = date\n",
    "        arg_before = date\n",
    "    if not arg_after:\n",
    "        arg_after = arg_before\n",
    "    if not arg_before:\n",
    "        arg_before = arg_after\n",
    "    return arg_before, arg_after\n",
    "\n",
    "def speyediff(N, d, format = 'csc'):\n",
    "    shape = (N-d, N)\n",
    "    diagonals = np.zeros(2*d + 1)\n",
    "    diagonals[d] = 1.\n",
    "    for i in range(d):\n",
    "        diff = diagonals[:-1] - diagonals[1:]\n",
    "        diagonals = diff\n",
    "    offsets = np.arange(d+1)\n",
    "    spmat = sparse.diags(diagonals, offsets, shape, format = format)\n",
    "    return spmat\n",
    "\n",
    "def smooth(y, lmbd, d = 2):\n",
    "    m = len(y)\n",
    "    E = sparse.eye(m, format = 'csc')\n",
    "    D = speyediff(m, d, format = 'csc')\n",
    "    coefmat = E + lmbd * D.conj().T.dot(D)\n",
    "    z = splu(coefmat).solve(y)\n",
    "    return z\n",
    "\n",
    "MDL_PATH = \"../src/dsen2/models/\"\n",
    "\n",
    "input_shape = ((4, None, None), (6, None, None))\n",
    "model = s2model(input_shape, num_layers=6, feature_size=128)\n",
    "predict_file = MDL_PATH+'s2_032_lr_1e-04.hdf5'\n",
    "print('Symbolic Model Created.')\n",
    "\n",
    "model.load_weights(predict_file)\n",
    "\n",
    "def DSen2(d10, d20):\n",
    "    test = [d10, d20]\n",
    "    input_shape = ((4, None, None), (6, None, None))\n",
    "    prediction = _predict(test, input_shape, deep=False)\n",
    "    #prediction *= 5\n",
    "    return prediction\n",
    "\n",
    "def _predict(test, input_shape, model = model, deep=False, run_60=False):\n",
    "    \n",
    "    print(\"Predicting using file: {}\".format(predict_file))\n",
    "    prediction = model.predict(test, verbose=1)\n",
    "    return prediction\n",
    "\n",
    "c_arr = np.array([[1, 1, 1, 1, 1,],\n",
    "                  [1, 2, 2, 2, 1,],\n",
    "                  [1, 2, 3, 2, 1,],\n",
    "                  [1, 2, 2, 2, 1,],\n",
    "                  [1, 1, 1, 1, 1,],])\n",
    "                  \n",
    "c_arr = c_arr / 3\n",
    "o_arr = 1 - c_arr\n",
    "c_arr = np.tile(c_arr[:, :, np.newaxis], (1, 1, 11))\n",
    "o_arr = np.tile(o_arr[:, :, np.newaxis], (1, 1, 11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertCoords(xy, src='', targ=''):\n",
    "\n",
    "    srcproj = osr.SpatialReference()\n",
    "    srcproj.ImportFromEPSG(src)\n",
    "    targproj = osr.SpatialReference()\n",
    "    if isinstance(targ, str):\n",
    "        targproj.ImportFromProj4(targ)\n",
    "    else:\n",
    "        targproj.ImportFromEPSG(targ)\n",
    "    transform = osr.CoordinateTransformation(srcproj, targproj)\n",
    "\n",
    "    pt = ogr.Geometry(ogr.wkbPoint)\n",
    "    pt.AddPoint(xy[0], xy[1])\n",
    "    pt.Transform(transform)\n",
    "    return([pt.GetX(), pt.GetY()])\n",
    "\n",
    "def calc_bbox(plot_id, df):\n",
    "    subs = df[df['PLOT_ID'] == plot_id]\n",
    "    # TOP, LEFT, BOTTOM, RIGHT\n",
    "    # (min x, min y), (max x, max y)\n",
    "    return [(min(subs['LON']), min(subs['LAT'])),\n",
    "            (max(subs['LON']), max(subs['LAT']))]\n",
    "\n",
    "def bounding_box(points, expansion = 160):\n",
    "    # LONG, LAT FOR SOME REASON\n",
    "    bl = list(points[0])\n",
    "    tr = list(points[1])\n",
    "    \n",
    "    if 78 <= tr[0] <= 84:\n",
    "        epsg = 32644 if tr[1] > 0 else 32744\n",
    "    if 72 <= tr[0] <= 78:\n",
    "        epsg = 32643 if tr[1] > 0 else 32743\n",
    "    if 66 <= tr[0] <= 72:\n",
    "        epsg = 32642 if tr[1] > 0 else 32742\n",
    "    if 60 <= tr[0] <= 66:\n",
    "        epsg = 32641 if tr[1] > 0 else 32741\n",
    "    if 54 <= tr[0] <= 60:\n",
    "        epsg = 32640 if tr[1] > 0 else 32740\n",
    "    if 48 <= tr[0] <= 54:\n",
    "        epsg = 32639 if tr[1] > 0 else 32739\n",
    "    if 42 <= tr[0] <= 48:\n",
    "        epsg = 32638 if tr[1] > 0 else 32738\n",
    "    if 36 <= tr[0] <= 42:\n",
    "        epsg = 32637 if tr[1] > 0 else 32737\n",
    "    if 30 <= tr[0] <= 36:\n",
    "        epsg = 32636 if tr[1] > 0 else 32736\n",
    "    if 24 <= tr[0] <= 30:\n",
    "        epsg = 32635 if tr[1] > 0 else 32735\n",
    "    if 18 <= tr[0] <= 24:\n",
    "        epsg = 32634 if tr[1] > 0 else 32734\n",
    "    if 12 <= tr[0] <= 18:\n",
    "        epsg = 32633 if tr[1] > 0 else 32733\n",
    "    if 6 <= tr[0] <= 12:\n",
    "        epsg = 32632 if tr[1] > 0 else 32732\n",
    "    if 0 <= tr[0] <= 6:\n",
    "        epsg = 32631 if tr[1] > 0 else 32731\n",
    "    if -6 <= tr[0] <= 0:\n",
    "        epsg = 32630 if tr[1] > 0 else 32730\n",
    "    if -12 <= tr[0] <= -6:\n",
    "        epsg = 32629 if tr[1] > 0 else 32729\n",
    "    if -18 <= tr[0] <= -12:\n",
    "        epsg = 32628 if tr[1] > 0 else 32728\n",
    "    if -24 <= tr[0] <= -18:\n",
    "        epsg = 32627 if tr[1] > 0 else 32727\n",
    "    if -30 <= tr[0] <= -24:\n",
    "        epsg = 32626 if tr[1] > 0 else 32726\n",
    "    if -36 <= tr[0] <= -30:\n",
    "        epsg = 32625 if tr[1] > 0 else 32725\n",
    "    if -42 <= tr[0] <= -36:\n",
    "        epsg = 32624 if tr[1] > 0 else 32724\n",
    "    if -48 <= tr[0] <= -42:\n",
    "        epsg = 32623 if tr[1] > 0 else 32723\n",
    "    if -78 <= tr[0] <= -72:\n",
    "        epsg = 32618 if tr[1] > 0 else 32718\n",
    "    if -84 <= tr[0] <= -78:\n",
    "        epsg = 32617 if tr[1] > 0 else 32717\n",
    "    if -90 <= tr[0] <= -84:\n",
    "        epsg = 32616 if tr[1] > 0 else 32716\n",
    "    if -96 <= tr[0] <= -90:\n",
    "        epsg = 32615 if tr[1] > 0 else 32715\n",
    "\n",
    "    bl = convertCoords(bl, 4326, epsg)\n",
    "    tr = convertCoords(tr, 4326, epsg)\n",
    "    init = [b - a for a,b in zip(bl, tr)]\n",
    "    distance1 = tr[0] - bl[0]\n",
    "    distance2 = tr[1] - bl[1]\n",
    "    expansion1 = (expansion - distance1)/2\n",
    "    expansion2 = (expansion - distance2)/2\n",
    "    bl = [bl[0] - expansion1, bl[1] - expansion2]\n",
    "    tr = [tr[0] + expansion1, tr[1] + expansion2]\n",
    "\n",
    "    after = [b - a for a,b in zip(bl, tr)]   \n",
    "    print(after)\n",
    "    if max(init) > 130:\n",
    "        print(\"ERROR: Initial field greater than 130m\")\n",
    "    if min(init) < 120:\n",
    "        print(\"ERROR: Initial field less than 130m\")\n",
    "        \n",
    "    if min(after) < (expansion - 4.5):\n",
    "        print(\"ERROR\")\n",
    "    if max(after) > (expansion + 5):\n",
    "        print(\"ERROR\")\n",
    "    diffs = [b - a for b, a in zip(after, init)]\n",
    "\n",
    "    bl = convertCoords(bl, epsg, 4326)\n",
    "    tr = convertCoords(tr, epsg, 4326)\n",
    "    return bl, tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_clouds(bbox, epsg = EPSG, time = time):\n",
    "    try:\n",
    "        box = BBox(bbox, crs = epsg)\n",
    "        cloud_request = WmsRequest(\n",
    "            layer='CLOUD_DETECTION',\n",
    "            bbox=box,\n",
    "            time=time,\n",
    "            width=IMSIZE,\n",
    "            height=IMSIZE,\n",
    "            image_format = MimeType.TIFF_d32f,\n",
    "            maxcc=0.75,\n",
    "            instance_id=API_KEY,\n",
    "            custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "            time_difference=datetime.timedelta(hours=24),\n",
    "        )\n",
    "        \n",
    "        cloud_img = cloud_request.get_data()\n",
    "        cloud_probs = cloud_detector.get_cloud_probability_maps(np.array(cloud_img))\n",
    "        means = np.mean(cloud_probs, (1, 2))\n",
    "        clean_steps = [i for i, val in enumerate(means) if val < 0.20]\n",
    "        return clean_steps, means, cloud_probs\n",
    "    except Exception as e:\n",
    "        logging.fatal(e, exc_info=True)\n",
    "    \n",
    "    \n",
    "def download_dem(val, df, epsg = EPSG, ):\n",
    "    location = calc_bbox(val, df = df)\n",
    "    bbox = bounding_box(location, expansion = (IMSIZE+2)*10)\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    dem_request = WmsRequest(data_source=DataSource.DEM,\n",
    "                         layer='DEM',\n",
    "                         bbox=box,\n",
    "                         width=IMSIZE+2,\n",
    "                         height=IMSIZE+2,\n",
    "                         instance_id=API_KEY,\n",
    "                         image_format=MimeType.TIFF_d32f,\n",
    "                         custom_url_params={CustomUrlParam.SHOWLOGO: False})\n",
    "    dem_image = dem_request.get_data()[0]\n",
    "    dem_image = calcSlope(dem_image.reshape((1, IMSIZE+2, IMSIZE+2)),\n",
    "                  np.full((IMSIZE+2, IMSIZE+2), 10), np.full((IMSIZE+2, IMSIZE+2), 10), zScale = 1, minSlope = 0.02)\n",
    "    dem_image = dem_image.reshape((IMSIZE+2, IMSIZE+2, 1))\n",
    "    dem_image = dem_image[1:IMSIZE+1, 1:IMSIZE+1, :]\n",
    "    return dem_image\n",
    "\n",
    "def check_zenith(bbox, epsg = EPSG, time = time):\n",
    "    try:\n",
    "        box = BBox(bbox, crs = epsg)\n",
    "        zenith = WmsRequest(\n",
    "            layer='ZENITH',\n",
    "            bbox=box,\n",
    "            time=time,\n",
    "            width=IMSIZE,\n",
    "            height=IMSIZE,\n",
    "            image_format = MimeType.TIFF_d32f,\n",
    "            maxcc=0.75,\n",
    "            instance_id=API_KEY,\n",
    "            custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "            time_difference=datetime.timedelta(hours=24),\n",
    "        )\n",
    "        \n",
    "        zenith = zenith.get_data()\n",
    "        return zenith\n",
    "    except Exception as e:\n",
    "        logging.fatal(e, exc_info=True)\n",
    "        \n",
    "def download_layer(bbox, epsg = EPSG, time = time):\n",
    "    try:\n",
    "        box = BBox(bbox, crs = epsg)\n",
    "        image_request = WcsRequest(\n",
    "                layer='L2A20',\n",
    "                bbox=box,\n",
    "                time=time,\n",
    "                image_format = MimeType.TIFF_d32f,\n",
    "                maxcc=0.75,\n",
    "                resx='10m', resy='10m',\n",
    "                instance_id=API_KEY,\n",
    "                custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'NEAREST',\n",
    "                                    constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "                time_difference=datetime.timedelta(hours=24),\n",
    "            )\n",
    "        img_bands = image_request.get_data()\n",
    "        img_20 = np.stack(img_bands)\n",
    "        img_20 = resize(img_20, (img_20.shape[0], IMSIZE, IMSIZE, img_20.shape[-1]), order = 0)\n",
    "        \n",
    "        image_request = WcsRequest(\n",
    "                layer='L2A10',\n",
    "                bbox=box,\n",
    "                time=time,\n",
    "                image_format = MimeType.TIFF_d32f,\n",
    "                maxcc=0.75,\n",
    "                resx='10m', resy='10m',\n",
    "                instance_id=API_KEY,\n",
    "                custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'BICUBIC',\n",
    "                                    constants.CustomUrlParam.UPSAMPLING: 'BICUBIC'},\n",
    "                time_difference=datetime.timedelta(hours=24),\n",
    "        )\n",
    "        \n",
    "        img_bands = image_request.get_data()\n",
    "        img_10 = np.stack(img_bands)\n",
    "        print(\"Original size: {}\".format(img_10.shape))\n",
    "        img_10 = resize(img_10, (img_10.shape[0], IMSIZE, IMSIZE, img_10.shape[-1]), order = 0)\n",
    "        shadows = img_10[:, :, :, -1]\n",
    "        img_10 = img_10[:, :, :, :-1]\n",
    "        \n",
    "        shadows[np.where(shadows != 3)] = 0\n",
    "        shadows[np.where(shadows == 3)] = 1\n",
    "        shadows_sums = np.sum(shadows, axis = 0)\n",
    "        before = np.sum(shadows)\n",
    "        #shadows[np.where(shadows_sums > shadows.shape[0]/2)] = 0.\n",
    "        print(\"Difference: {}\".format(np.sum(shadows) - before))\n",
    "        print(\"Shadows: {}\".format(shadows.shape))\n",
    "        shadow_sum = np.sum(shadows, axis = (1, 2))\n",
    "        shadow_steps = np.argwhere(shadow_sum > (IMSIZE*IMSIZE) / 5)\n",
    "        \n",
    "        img = np.concatenate([img_10, img_20], axis = -1)\n",
    "        return img, image_request, shadows, shadow_steps\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.fatal(e, exc_info=True)\n",
    "        \n",
    "def download_sentinel_1(bbox, epsg = EPSG, time = time, year = 2019):\n",
    "    try:\n",
    "        box = BBox(bbox, crs = epsg)\n",
    "        image_request = WcsRequest(\n",
    "                layer='SENT',\n",
    "                bbox=box,\n",
    "                time=time,\n",
    "                image_format = MimeType.TIFF_d32f,\n",
    "                maxcc=1.0,\n",
    "                resx='5m', resy='5m',\n",
    "                instance_id=API_KEY,\n",
    "                custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'NEAREST',\n",
    "                                    constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "                time_difference=datetime.timedelta(hours=24),\n",
    "            )\n",
    "        img_bands = image_request.get_data()\n",
    "        s1 = np.stack(img_bands)\n",
    "        s1 = resize(s1, (s1.shape[0], IMSIZE*2, IMSIZE*2, s1.shape[-1]), order = 0)\n",
    "        s1 = np.reshape(s1, (s1.shape[0], s1.shape[1]//2, 2, s1.shape[2] // 2, 2, s1.shape[-1]))\n",
    "        s1 = np.mean(s1, (2, 4))\n",
    "        s1 = s1[:, 8:24, 8:24, :]\n",
    "        \n",
    "        image_dates = []\n",
    "        for date in image_request.get_dates():\n",
    "            if date.year == year - 1:\n",
    "                image_dates.append(-365 + starting_days[(date.month-1)] + date.day)\n",
    "            if date.year == year:\n",
    "                image_dates.append(starting_days[(date.month-1)] + date.day)\n",
    "            if date.year == year + 1:\n",
    "                image_dates.append(365 + starting_days[(date.month-1)]+date.day)\n",
    "        image_dates = np.array(image_dates)\n",
    "        to_remove = np.argwhere(np.max(s1, (1, 2, 3)) == 1.).flatten()\n",
    "        s1 = np.delete(s1, to_remove, 0)\n",
    "        #print(np.max(s1, (1, 2, 3)))\n",
    "        image_dates = np.delete(image_dates, to_remove)\n",
    "        return s1, image_dates\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.fatal(e, exc_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud and shadow removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_cloud_and_shadows(tiles, probs, shadows, image_dates, wsize = 5):\n",
    "    c_probs = np.copy(probs)\n",
    "    c_probs = c_probs - np.min(c_probs, axis = 0)\n",
    "    c_probs[np.where(c_probs > 0.33)] = 1.\n",
    "    c_probs[np.where(c_probs < 0.33)] = 0.\n",
    "    c_probs = np.reshape(c_probs, [c_probs.shape[0], int(IMSIZE/8), 8, int(IMSIZE/8), 8])\n",
    "    c_probs = np.sum(c_probs, (2, 4))\n",
    "    c_probs = resize(c_probs, (c_probs.shape[0], IMSIZE, IMSIZE), 0)\n",
    "    c_probs[np.where(c_probs < 12)] = 0.\n",
    "    c_probs[np.where(c_probs >= 12)] = 1.\n",
    "    c_probs += shadows\n",
    "    c_probs[np.where(c_probs >= 1.)] = 1.\n",
    "    n_interp = 0\n",
    "    for cval in range(0, IMSIZE - 4, 1):\n",
    "        for rval in range(0, IMSIZE - 4, 1):\n",
    "            subs = c_probs[:, cval:cval + wsize, rval:rval+wsize]\n",
    "            satisfactory = [x for x in range(c_probs.shape[0]) if np.sum(subs[x, :, :]) < 10]\n",
    "            satisfactory = np.array(satisfactory)\n",
    "            for date in range(0, tiles.shape[0]):\n",
    "                if np.sum(subs[date, :, :]) > 10:\n",
    "                    n_interp += 1\n",
    "                    before, after = calculate_proximal_steps_index(date, satisfactory)\n",
    "                    before = date + before\n",
    "                    after = date + after\n",
    "                    bef = tiles[before, cval:cval+wsize, rval:rval+wsize, : ]\n",
    "                    aft = tiles[after, cval:cval+wsize, rval:rval+wsize, : ]\n",
    "                    before = image_dates[before]\n",
    "                    after = image_dates[after]\n",
    "                    before_diff = abs(image_dates[date] - before)\n",
    "                    after_diff = abs(image_dates[date] - after)\n",
    "                    bef_wt = 1 - before_diff / (before_diff + after_diff)\n",
    "                    aft_wt = 1 - bef_wt\n",
    "                    candidate = bef_wt*bef + aft_wt*aft\n",
    "                    candidate = candidate*c_arr + tiles[date, cval:cval+wsize, rval:rval+wsize, : ]*o_arr\n",
    "                    tiles[date, cval:cval+wsize, rval:rval+wsize, : ] = candidate  \n",
    "    print(\"Interpolated {} px\".format(n_interp))\n",
    "    return tiles\n",
    "\n",
    "def remove_missed_clouds(img):\n",
    "    iqr = np.percentile(img[:, :, :, 3].flatten(), 75) - np.percentile(img[:, :, :, 3].flatten(), 25)\n",
    "    thresh_t = np.percentile(img[:, :, :, 3].flatten(), 75) + iqr*2\n",
    "    thresh_b = np.percentile(img[:, :, :, 3].flatten(), 25) - iqr*2\n",
    "    diffs_fw = np.diff(img, 1, axis = 0)\n",
    "    diffs_fw = np.mean(diffs_fw, axis = (1, 2, 3))\n",
    "    diffs_fw = np.array([0] + list(diffs_fw))\n",
    "    diffs_bw = np.diff(np.flip(img, 0), 1, axis = 0)\n",
    "    diffs_bw = np.flip(np.mean(diffs_bw, axis = (1, 2, 3)))\n",
    "    diffs_bw = np.array(list(diffs_bw) + [0])\n",
    "    diffs = abs(diffs_fw - diffs_bw) * 100 # 3, -3 -> 6, -3, 3 -> 6, -3, -3\n",
    "    #diffs = [int(x) for x in diffs]\n",
    "    outlier_percs = []\n",
    "    for step in range(img.shape[0]):\n",
    "        bottom = len(np.argwhere(img[step, :, :, 3].flatten() > thresh_t))\n",
    "        top = len(np.argwhere(img[step, :, :, 3].flatten() < thresh_b))\n",
    "        p = 100* ((bottom + top) / (IMSIZE*IMSIZE))\n",
    "        outlier_percs.append(p)\n",
    "    to_remove = np.argwhere(np.array(outlier_percs) > 15)\n",
    "    print([int(x) for x in outlier_percs])\n",
    "    return to_remove\n",
    "\n",
    "def threshold_shadows(arr):\n",
    "    arr = np.copy(arr)\n",
    "    iqr = np.percentile(arr.flatten(), 75) - np.percentile(arr.flatten(), 25)\n",
    "    low = np.percentile(arr.flatten(), 25)\n",
    "    #high = np.percentile(arr.flatten(), 75)\n",
    "    thresh_low = low - 1.5*iqr\n",
    "    #thresh_high = high + 2*iqr\n",
    "    #arr[np.where(arr > thresh_high)] = 1.\n",
    "    arr[np.where(arr < thresh_low)] = 1.\n",
    "    arr[np.where(arr < 1)] = 0.\n",
    "    arr = np.reshape(arr, (arr.shape[0], 6, 8, 6, 8))\n",
    "    arr = np.sum(arr, axis = (2, 4))\n",
    "    arr = resize(arr, (arr.shape[0], 48, 48), 0)\n",
    "    fake_shadows = np.zeros((arr.shape[0], arr.shape[1], arr.shape[2]))\n",
    "    for step in range(arr.shape[0]):\n",
    "        if step > 0:\n",
    "            for x in range(arr.shape[1]):\n",
    "                for y in range(arr.shape[2]):\n",
    "                    if arr[step, x, y] > 0:\n",
    "                        before = arr[step - 1, x, y]\n",
    "                        if abs(before - arr[step, x, y]) <= 20:\n",
    "                            fake_shadows[step, x, y] = 1\n",
    "                            \n",
    "    for step in range(arr.shape[0]):\n",
    "        if step < arr.shape[0] - 1:\n",
    "            for x in range(arr.shape[1]):\n",
    "                for y in range(arr.shape[2]):\n",
    "                    if arr[step, x, y] > 0:\n",
    "                        after = arr[step + 1, x, y]\n",
    "                        if abs(after - arr[step, x, y]) <= 20:\n",
    "                            fake_shadows[step, x, y] = 1\n",
    "    arr[np.where(arr > 5)] = 1.\n",
    "    arr[np.where(arr < 5)] = 0.\n",
    "    before = np.sum(arr)\n",
    "    arr[np.where(fake_shadows == 1)] = 0.\n",
    "    after = np.sum(arr)\n",
    "    \n",
    "    print(\"Removed {} fake shadows, leaving {}\".format(before - after, after))\n",
    "    print(\"The total percent shadow cover is: {}%\".format(100*(after/(arr.shape[0]*arr.shape[1]*arr.shape[2]))))\n",
    "    for step in range(arr.shape[0]):\n",
    "        for x in range(1, arr.shape[1] -1):\n",
    "            for y in range(1, arr.shape[2] - 1):\n",
    "                if np.sum(arr[step, x-1:x+2, y-1:y+2]) == 1:\n",
    "                    if arr[step, x, y] != 0:\n",
    "                        print(\"Removing: {} {} {} {}\".format(step, x, y, np.sum(arr[step, x-1:x+2, y-1:y+2])))\n",
    "                        arr[step, x, y] = 0\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_and_save_best_images(img_bands, image_dates):\n",
    "    # This function interpolates data to 5 day windows linearly\n",
    "\n",
    "    biweekly_dates = [day for day in range(0, 360, 5)] # ideal imagery dates are every 15 days\n",
    "    \n",
    "    # Clouds have been removed at this step, so all steps are satisfactory\n",
    "    satisfactory_ids = [x for x in range(0, img_bands.shape[0])]\n",
    "    satisfactory_dates = [value for idx, value in enumerate(image_dates) if idx in satisfactory_ids]\n",
    "    \n",
    "    \n",
    "    selected_images = {}\n",
    "    for i in biweekly_dates:\n",
    "        distances = [abs(date - i) for date in satisfactory_dates]\n",
    "        closest = np.min(distances)\n",
    "        closest_id = np.argmin(distances)\n",
    "        # If there is imagery within 5 days, select it\n",
    "        if closest < 8:\n",
    "            date = satisfactory_dates[closest_id]\n",
    "            image_idx = int(np.argwhere(np.array(image_dates) == date)[0])\n",
    "            selected_images[i] = {'image_date': [date], 'image_ratio': [1], 'image_idx': [image_idx]}\n",
    "        # If there is not imagery within 7 days, look for the closest above and below imagery\n",
    "        else:\n",
    "            distances = np.array([(date - i) for date in satisfactory_dates])\n",
    "            # Number of days above and below the selected date of the nearest clean imagery\n",
    "            above = distances[np.where(distances < 0, distances, -np.inf).argmax()]\n",
    "            below = distances[np.where(distances > 0, distances, np.inf).argmin()]\n",
    "            if abs(above) > 240: # If date is the last date, occassionally argmax would set above to - number\n",
    "                above = below\n",
    "            if abs(below) > 240:\n",
    "                below = above\n",
    "            if above != below:\n",
    "                below_ratio = above / (above - below)\n",
    "                above_ratio = 1 - below_ratio\n",
    "            else:\n",
    "                above_ratio = below_ratio = 0.5\n",
    "                \n",
    "            # Extract the image date and imagery index for the above and below values\n",
    "            above_date = i + above\n",
    "            above_image_idx = int(np.argwhere(np.array(image_dates) == above_date)[0])\n",
    "            \n",
    "            below_date = i + below\n",
    "            below_image_idx = int(np.argwhere(np.array(image_dates) == below_date)[0])\n",
    "            \n",
    "            selected_images[i] = {'image_date': [above_date, below_date], 'image_ratio': [above_ratio, below_ratio],\n",
    "                                 'image_idx': [above_image_idx, below_image_idx]}\n",
    "                               \n",
    "    max_distance = 0\n",
    "    \n",
    "    for i in selected_images.keys():\n",
    "        #print(i, selected_images[i])\n",
    "        if len(selected_images[i]['image_date']) == 2:\n",
    "            dist = selected_images[i]['image_date'][1] - selected_images[i]['image_date'][0]\n",
    "            if dist > max_distance:\n",
    "                max_distance = dist\n",
    "    \n",
    "    print(\"Maximum time distance: {}\".format(max_distance))\n",
    "        \n",
    "    # Compute the weighted average of the selected imagery for each time step\n",
    "    keep_steps = []\n",
    "    use_median = False\n",
    "    for i in selected_images.keys():\n",
    "        step1_additional = None\n",
    "        step2_additional = None\n",
    "        info = selected_images[i]\n",
    "        if len(info['image_idx']) == 1:\n",
    "            step = img_bands[info['image_idx'][0]]\n",
    "        if len(info['image_idx']) == 2:\n",
    "            step1 = img_bands[info['image_idx'][0]] # * info['image_ratio'][0]\n",
    "            step2 = img_bands[info['image_idx'][1]]\n",
    "            #if info['image_idx'][0] == 0:\n",
    "            #    step1_additional = img_bands[-1]\n",
    "            #    print(\"Using last step\")\n",
    "            #if info['image_idx'][1] == (img_bands.shape[0] - 1):\n",
    "            #    step2_additional = img_bands[0]\n",
    "            ##    print(\"Using first step\")\n",
    "            #if step1_additional is None and step2_additional is None:\n",
    "            step = step1 * 0.5 + step2 * 0.5\n",
    "            #if step1_additional is not None:\n",
    "            #    print(\"Echo\")\n",
    "            #    step = (step1 + step2 + step1_additional) * (1/3)\n",
    "            #if step2_additional is not None:\n",
    "             #   print(\"Echo\")\n",
    "            #    step = (step1 + step2 + step2_additional) * (1/3)\n",
    "        keep_steps.append(step)\n",
    "    '''\n",
    "    for i in selected_images.keys():\n",
    "        info = selected_images[i]\n",
    "        if len(info['image_idx']) == 1:\n",
    "            step = img_bands[info['image_idx'][0]]\n",
    "            use_median = False\n",
    "        if len(info['image_idx']) == 2:\n",
    "            difference = np.max([abs(info['image_date'][0] - int(i)),\n",
    "                                 abs(info['image_date'][1] - int(i))]) \n",
    "            step1 = img_bands[info['image_idx'][0]] # * info['image_ratio'][0]\n",
    "            step2_idx = info['image_idx'][0] - 1\n",
    "            if step2_idx < 0:\n",
    "                step2_idx = (img_bands.shape[0] - 1)\n",
    "            step2 = img_bands[step2_idx]\n",
    "            step3 = img_bands[info['image_idx'][1]]\n",
    "            step4_idx = info['image_idx'][1] + 1\n",
    "            if step4_idx > (img_bands.shape[0] - 1):\n",
    "                step4_idx = 0\n",
    "            step4 = img_bands[step4_idx]\n",
    "            #step2 = img_bands[info['image_idx'][1]] * 0.5 # info['image_ratio'][1]\n",
    "            if difference > 100 or use_median == True:\n",
    "                print(\"Median, {}\".format(difference))\n",
    "                use_median = True\n",
    "                stacked = np.stack([step1, step2, step3, step4])\n",
    "                step = np.median(stacked, axis = 0)\n",
    "            else:\n",
    "                use_median = False\n",
    "                step = step1 * 0.5 + step3 * 0.5\n",
    "        '''\n",
    "        #keep_steps.append(step)\n",
    "        \n",
    "    keep_steps = np.stack(keep_steps)\n",
    "    return keep_steps, max_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0  31  59  90 120 151 181 212 243 273 304 334]\n"
     ]
    }
   ],
   "source": [
    "days_per_month = [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30]\n",
    "starting_days = np.cumsum(days_per_month)\n",
    "print(starting_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "super_resolve = True\n",
    "year = 2019\n",
    "\n",
    "DATA_LOCATION = '../data/ghana-test.csv'\n",
    "OUTPUT_FOLDER = '../data/test-smooth-200/'\n",
    "\n",
    "\n",
    "def download_plots(data_location = DATA_LOCATION, output_folder = OUTPUT_FOLDER):\n",
    "    df = pd.read_csv(data_location)\n",
    "    df = df.drop('IMAGERY_TITLE', axis = 1)\n",
    "    df = df.dropna(axis = 0)\n",
    "    plot_ids = sorted(df['PLOT_ID'].unique())\n",
    "    existing = [int(x[:-4]) for x in os.listdir(output_folder) if \".DS\" not in x]\n",
    "    to_download = [x for x in plot_ids if x not in existing]\n",
    "    print(\"STARTING DOWNLOAD OF {} plots from {} to {}\".format(len(to_download), data_location, output_folder))\n",
    "    errors = []\n",
    "    for i, val in enumerate(to_download):\n",
    "        print(i, val)\n",
    "        print(\"Downloading {}/{}, {}\".format(i+1, len(to_download), val))\n",
    "        location = calc_bbox(val, df = df)\n",
    "        location = bounding_box(location, expansion = IMSIZE*10)\n",
    "        try:\n",
    "            # Identify cloud steps, download DEM, and download L2A series\n",
    "            clean_steps, means, probs = identify_clouds(location)\n",
    "            dem = download_dem(val, df = df)\n",
    "            s1, s1_dates = download_sentinel_1(location)\n",
    "            print(s1.shape)\n",
    "            img, image_request, shadows, shadow_steps = download_layer(location)\n",
    "\n",
    "            # Subset zenith < 70\n",
    "            zenith = check_zenith(location)\n",
    "            zenith = np.mean(np.stack(zenith), axis = (1, 2))\n",
    "            zenith_outliers = np.argwhere(zenith > 70)\n",
    "            if len(zenith_outliers) > 0:\n",
    "                print(\"Zenith outlier: {}\".format(zenith_outliers))\n",
    "\n",
    "            # Calculate imagery dates\n",
    "            image_dates = []\n",
    "            for date in image_request.get_dates():\n",
    "                if date.year == year - 1:\n",
    "                    image_dates.append(-365 + starting_days[(date.month-1)] + date.day)\n",
    "                if date.year == year:\n",
    "                    image_dates.append(starting_days[(date.month-1)] + date.day)\n",
    "                if date.year == year + 1:\n",
    "                    image_dates.append(365 + starting_days[(date.month-1)]+date.day)\n",
    "            image_dates = np.array(image_dates)\n",
    "            print(image_dates)\n",
    "\n",
    "            # Remove imagery where >4% is clouds, and where there is null data\n",
    "            args = np.array([len(np.argwhere(probs[x].flatten() > 0.3)) for x in range(probs.shape[0])])\n",
    "            dirty_steps = np.argwhere(args > (IMSIZE)*(IMSIZE) / 5)\n",
    "            missing_images = [np.argwhere(img[x, :, : :].flatten() == 0.0) for x in range(img.shape[0])]\n",
    "            missing_images = np.array([len(x) for x in missing_images])\n",
    "            missing_images_p = [np.argwhere(img[x, :, : :].flatten() >= 1) for x in range(img.shape[0])]\n",
    "            missing_images_p = np.array([len(x) for x in missing_images_p])\n",
    "            missing_images += missing_images_p\n",
    "            missing_images = list(np.argwhere(missing_images >= 25))\n",
    "            to_remove = np.unique(np.array(list(dirty_steps) + list(missing_images) + list(zenith_outliers) + list(shadow_steps)))\n",
    "\n",
    "            # Remove null steps\n",
    "            print(\"There are {}/{} dirty steps: {} cloud, {} missing, {} zenith, {} shadows\".format(len(to_remove),\n",
    "                                                                                        len(img), len(dirty_steps),\n",
    "                                                                                        len(missing_images),\n",
    "                                                                                        len(zenith_outliers),\n",
    "                                                                                        len(shadow_steps)))\n",
    "\n",
    "            img = np.delete(img, to_remove, 0)\n",
    "            probs = np.delete(probs, to_remove, 0)\n",
    "            shadows = np.delete(shadows, to_remove, 0)\n",
    "            image_dates = np.delete(image_dates, to_remove)\n",
    "\n",
    "            to_remove = remove_missed_clouds(img)\n",
    "            img = np.delete(img, to_remove, 0)\n",
    "            shadows = np.delete(shadows, to_remove, 0)\n",
    "            probs = np.delete(probs, to_remove, 0)\n",
    "            image_dates = np.delete(image_dates, to_remove)\n",
    "            print(\"Removing {} steps based on ratio\".format(len(to_remove)))\n",
    "\n",
    "\n",
    "            # Concatenate DEM\n",
    "            dem = np.tile(dem.reshape((1, IMSIZE, IMSIZE, 1)), (img.shape[0], 1, 1, 1))\n",
    "            tiles = np.concatenate([img, dem], axis = -1)\n",
    "            tiles[:, :, :, -1] /= 90\n",
    "\n",
    "            new_shadows = threshold_shadows(tiles[:, :, :, 3])\n",
    "            x = remove_cloud_and_shadows(tiles, probs, new_shadows, image_dates)\n",
    "            if super_resolve:\n",
    "                x = x[:, 8:40, 8:40, :]\n",
    "                print(\"Before super: {}\".format(x.shape))\n",
    "\n",
    "                d10 = x[:, :, :, 0:4]\n",
    "                d20 = x[:, :, :, 4:10]\n",
    "\n",
    "                d10 = np.swapaxes(d10, 1, -1)\n",
    "                d10 = np.swapaxes(d10, 2, 3)\n",
    "                d20 = np.swapaxes(d20, 1, -1)\n",
    "                d20 = np.swapaxes(d20, 2, 3)\n",
    "                superresolved = DSen2(d10, d20)\n",
    "                superresolved = np.swapaxes(superresolved, 1, -1)\n",
    "                superresolved = np.swapaxes(superresolved, 1, 2)\n",
    "                print(superresolved.shape)\n",
    "                print(x.shape)\n",
    "\n",
    "                # returns band IDXs 3, 4, 5, 7, 8, 9\n",
    "                x[:, :, :, 4:10] = superresolved\n",
    "                x = x[:, 8:24, 8:24, :]\n",
    "                print(\"After super shape: {}\".format(x.shape))\n",
    "            else:\n",
    "                bottom = int(IMSIZE/2 - 8)\n",
    "                top = int(IMSIZE/2 + 8)\n",
    "                x = x[:, bottom:top, bottom:top, :]\n",
    "\n",
    "            # Calculate indices\n",
    "            tiles, amin = evi(x, True)\n",
    "            tiles = bi(tiles, True)\n",
    "            tiles = msavi2(tiles, True)\n",
    "            x = si(tiles, True)\n",
    "\n",
    "            print(\"Clean: {}\".format(x.shape))\n",
    "            \n",
    "            missing_pixels = 0\n",
    "            for band in range(0, 15):\n",
    "                for time in range(0, x.shape[0]):\n",
    "                    x_i = x[time, :, :, band]\n",
    "                    missing_pixels += len(np.argwhere(np.isnan(x_i)))\n",
    "                    x_i[np.argwhere(np.isnan(x_i))] = np.mean(x_i)\n",
    "                    x[time, :, :, band] = x_i\n",
    "            print(\"There are {} missing pixels\".format(missing_pixels))\n",
    "\n",
    "            # Interpolate linearly to 5 day frequency\n",
    "            s1, max_distance_s1 = calculate_and_save_best_images(s1, s1_dates)\n",
    "            tiles, max_distance = calculate_and_save_best_images(x, image_dates) # 22, 16, 16, 10\n",
    "\n",
    "            # Smooth linear interpolation\n",
    "            for row in range(0, 16):\n",
    "                for column in range(0, 16):\n",
    "                    for band in [x for x in range(0, 15) if x != 10]:\n",
    "                        sm = smooth(tiles[:, row, column, band], 800, d = 2)\n",
    "                        tiles[:, row, column, band] = sm\n",
    "\n",
    "            # Retain only iamgery every 15 days\n",
    "            biweekly_dates = np.array([day for day in range(0, 360, 5)])\n",
    "            to_remove = np.argwhere(biweekly_dates % 15 != 0)\n",
    "            tiles = np.delete(tiles, to_remove, 0)\n",
    "            s1 = np.delete(s1, to_remove, 0)\n",
    "            print(tiles.shape)\n",
    "            print(s1.shape)\n",
    "            tiles = np.concatenate([tiles, s1], -1)\n",
    "\n",
    "            if max_distance <= 240:\n",
    "                np.save(output_folder + str(val), tiles)\n",
    "                print(\"\\n\")\n",
    "            else:\n",
    "                print(\"Skipping {} because there is a {} distance\".format(val, max_distance))\n",
    "                print(\"\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            logging.fatal(e, exc_info=True)\n",
    "            errors.append(img)\n",
    "            #continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING DOWNLOAD OF 36 plots from ../data/test-csv/ghana-test.csv to ../data/test-super-fuse/\n",
      "0 135702360\n",
      "Downloading 1/36, 135702360\n",
      "[480.0, 480.0]\n",
      "[500.0, 500.0]\n",
      "(50, 16, 16, 2)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-123-4b1f9dafa250>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/test-csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\".csv\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mdownload_plots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/test-csv/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../data/test-super-fuse/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-122-004b5a0d6d06>\u001b[0m in \u001b[0;36mdownload_plots\u001b[0;34m(data_location, output_folder)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms1_dates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_sentinel_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshadows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshadow_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;31m# Subset zenith < 70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-118-dca4d15e547f>\u001b[0m in \u001b[0;36mdownload_layer\u001b[0;34m(bbox, epsg, time)\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0mtime_difference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhours\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             )\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mimg_bands\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mimg_20\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_bands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mimg_20\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_20\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMSIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMSIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_20\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/sentinelhub/data_request.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, save_data, data_filter, redownload, max_threads, raise_download_errors)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \"\"\"\n\u001b[1;32m    108\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preprocess_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mdata_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_data_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_filter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mredownload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_threads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_download_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_saved_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_filter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_download_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/sentinelhub/data_request.py\u001b[0m in \u001b[0;36m_execute_data_download\u001b[0;34m(self, data_filter, redownload, max_threads, raise_download_errors)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mdata_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdownload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_download_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mredownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mredownload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_threads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_threads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0mdata_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSHConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_timeout_seconds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/sentinelhub/download.py\u001b[0m in \u001b[0;36mdownload_data\u001b[0;34m(request_list, redownload, max_threads)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mconcurrent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mThreadPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_threads\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecute_download_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrequest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrequest_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_threads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                 \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m     \u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in os.listdir(\"../data/test-csv\"):\n",
    "    if \".csv\" in i:\n",
    "        download_plots(\"../data/test-csv/\" + i, \"../data/test-super-fuse/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load(\"../data/test-super-fuse/135702360.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1543ddda0>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.scatterplot(y = x[:, 3, 3, -1], x = [x for x in range(24)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0],\n",
       "       [ 2],\n",
       "       [ 4],\n",
       "       [ 6],\n",
       "       [ 8],\n",
       "       [10],\n",
       "       [12],\n",
       "       [14],\n",
       "       [16],\n",
       "       [18],\n",
       "       [20],\n",
       "       [22]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argwhere(np.sum(x[:, :, :, -2:], (1, 2, 3)) > 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([512.      ,  17.028301, 512.      ,  15.295774, 512.      ,\n",
       "        18.434525, 512.      ,  24.792126, 512.      ,  26.578152,\n",
       "       512.      ,  44.0458  , 512.      ,  33.67505 , 512.      ,\n",
       "        40.811874, 512.      ,  32.44155 , 512.      ,  37.157825,\n",
       "       512.      ,  32.370224, 512.      ,  29.100075], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(x[:, :, :, -2:], (1, 2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "policy-toolkit",
   "language": "python",
   "name": "policy-toolkit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
