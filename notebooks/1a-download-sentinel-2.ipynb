{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and process sentinel 2 data\n",
    "\n",
    "## John Brandt\n",
    "## April 1, 2020\n",
    "\n",
    "## Package imports, API import, source scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/john.brandt/anaconda3/envs/remote_sensing/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/john.brandt/anaconda3/envs/remote_sensing/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import scipy.sparse as sparse\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "\n",
    "from collections import Counter\n",
    "from osgeo import ogr, osr\n",
    "from random import shuffle\n",
    "from scipy.sparse.linalg import splu\n",
    "from s2cloudless import S2PixelCloudDetector, CloudMaskRequest\n",
    "from sentinelhub import WmsRequest, WcsRequest, MimeType\n",
    "from sentinelhub import CRS, BBox, constants, DataSource, CustomUrlParam\n",
    "from skimage.transform import resize\n",
    "\n",
    "with open(\"../config.yaml\", 'r') as stream:\n",
    "        key = (yaml.safe_load(stream))\n",
    "        API_KEY = key['key'] \n",
    "        \n",
    "%matplotlib inline\n",
    "%run ../src/utils/slope.py\n",
    "%run ../src/utils/utils-bilinear.py\n",
    "%run ../src/dsen2/utils/DSen2Net.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "SUPER_RESOLVE = True\n",
    "YEAR = 2019\n",
    "TIME = ('{}-12-15'.format(str(YEAR - 1)), '{}-01-15'.format(str(YEAR + 1)))\n",
    "EPSG = CRS.WGS84\n",
    "IMSIZE = 48\n",
    "CLOUD_DETECTOR = S2PixelCloudDetector(threshold=0.4, average_over=4, dilation_size=2)\n",
    "DATA_LOCATION = '../data/ghana-test.csv'\n",
    "OUTPUT_FOLDER = '../data/test-smooth-200/'\n",
    "\n",
    "# For DSen2 superresolve\n",
    "MDL_PATH = \"../src/dsen2/models/\"\n",
    "INPUT_SHAPE = ((4, None, None), (6, None, None))\n",
    "MODEL = s2model(INPUT_SHAPE, num_layers=6, feature_size=128)\n",
    "PREDICT_FILE = MDL_PATH+'s2_032_lr_1e-04.hdf5'\n",
    "MODEL.load_weights(PREDICT_FILE)\n",
    "\n",
    "# Constants\n",
    "starting_days = np.cumsum([0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30])\n",
    "c_arr = np.array([[1, 1, 1, 1, 1,],\n",
    "                  [1, 2, 2, 2, 1,],\n",
    "                  [1, 2, 3, 2, 1,],\n",
    "                  [1, 2, 2, 2, 1,],\n",
    "                  [1, 1, 1, 1, 1,],])\n",
    "                  \n",
    "c_arr = c_arr / 3\n",
    "o_arr = 1 - c_arr\n",
    "c_arr = np.tile(c_arr[:, :, np.newaxis], (1, 1, 11))\n",
    "o_arr = np.tile(o_arr[:, :, np.newaxis], (1, 1, 11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_proximal_steps_index(date, satisfactory):\n",
    "    \"\"\"Returns proximal steps that are cloud and shadow free\n",
    "\n",
    "         Parameters:\n",
    "          date (int): current time step\n",
    "          satisfactory (list): time steps with no clouds or shadows\n",
    "\n",
    "         Returns:\n",
    "          arg_before (str): index of the prior clean image\n",
    "          arg_after (int): index of the next clean image\n",
    "    \"\"\"\n",
    "    arg_before, arg_after = None, None\n",
    "    if date > 0:\n",
    "        idx_before = satisfactory - date\n",
    "        arg_before = idx_before[np.where(idx_before < 0, idx_before, -np.inf).argmax()]\n",
    "    if date < np.max(satisfactory):\n",
    "        idx_after = satisfactory - date\n",
    "        arg_after = idx_after[np.where(idx_after > 0, idx_after, np.inf).argmin()]\n",
    "    if not arg_after and not arg_before:\n",
    "        arg_after = date\n",
    "        arg_before = date\n",
    "    if not arg_after:\n",
    "        arg_after = arg_before\n",
    "    if not arg_before:\n",
    "        arg_before = arg_after\n",
    "    return arg_before, arg_after\n",
    "\n",
    "def speyediff(N, d, format = 'csc'):\n",
    "    \"\"\"Calculates the d-th order sparse difference matrix based on \n",
    "       an initial N x N identity matrix\n",
    "\n",
    "         Parameters:\n",
    "          N (int): input length\n",
    "          d (int): smoothing order\n",
    "\n",
    "         Returns:\n",
    "          spmat (arr): sparse difference matrix\n",
    "    \"\"\"\n",
    "    shape = (N-d, N)\n",
    "    diagonals = np.zeros(2*d + 1)\n",
    "    diagonals[d] = 1.\n",
    "    for i in range(d):\n",
    "        diff = diagonals[:-1] - diagonals[1:]\n",
    "        diagonals = diff\n",
    "    offsets = np.arange(d+1)\n",
    "    spmat = sparse.diags(diagonals, offsets, shape, format = format)\n",
    "    return spmat\n",
    "\n",
    "def smooth(y, lmbd, d = 2):\n",
    "    \"\"\"Calculates the whittaker smoother on input array\n",
    "\n",
    "         Parameters:\n",
    "          y (arr): 1-dimensional input array\n",
    "          lmbd (int): degree of smoothing, higher is more\n",
    "\n",
    "         Returns:\n",
    "          z (arr): smoothed version of y\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    E = sparse.eye(m, format = 'csc')\n",
    "    D = speyediff(m, d, format = 'csc')\n",
    "    coefmat = E + lmbd * D.conj().T.dot(D)\n",
    "    z = splu(coefmat).solve(y)\n",
    "    return z\n",
    "\n",
    "def DSen2(d10, d20):\n",
    "    \"\"\"Super resolves 20 meter bans using the DSen2 convolutional\n",
    "       neural network, as specified in Lanaras et al. 2018\n",
    "       https://github.com/lanha/DSen2\n",
    "\n",
    "        Parameters:\n",
    "         d10 (arr): (4, X, Y) shape array with 10 meter resolution\n",
    "         d20 (arr): (6, X, Y) shape array with 20 meter resolution\n",
    "\n",
    "        Returns:\n",
    "         prediction (arr): (6, X, Y) shape array with 10 meter superresolved\n",
    "                          output of DSen2 on d20 array\n",
    "    \"\"\"\n",
    "    test = [d10, d20]\n",
    "    input_shape = ((4, None, None), (6, None, None))\n",
    "    prediction = _predict(test, input_shape, deep=False)\n",
    "    return prediction\n",
    "\n",
    "def _predict(test, input_shape, model = MODEL, deep=False, run_60=False):\n",
    "    \n",
    "    prediction = model.predict(test, verbose=1)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertCoords(xy, src='', targ=''):\n",
    "    \"\"\" Converts coords from one EPSG to another\n",
    "\n",
    "        Parameters:\n",
    "         xy (tuple): input longitiude, latitude tuple\n",
    "         src (str): EPSG code associated with xy\n",
    "         targ (str): EPSG code of target output\n",
    "    \n",
    "        Returns:\n",
    "         pt (tuple): (x, y) tuple of xy in targ EPSG\n",
    "    \"\"\"\n",
    "\n",
    "    srcproj = osr.SpatialReference()\n",
    "    srcproj.ImportFromEPSG(src)\n",
    "    targproj = osr.SpatialReference()\n",
    "    if isinstance(targ, str):\n",
    "        targproj.ImportFromProj4(targ)\n",
    "    else:\n",
    "        targproj.ImportFromEPSG(targ)\n",
    "    transform = osr.CoordinateTransformation(srcproj, targproj)\n",
    "\n",
    "    pt = ogr.Geometry(ogr.wkbPoint)\n",
    "    pt.AddPoint(xy[0], xy[1])\n",
    "    pt.Transform(transform)\n",
    "    return([pt.GetX(), pt.GetY()])\n",
    "\n",
    "def calculate_epsg(points):\n",
    "    \"\"\" Calculates the UTM EPSG of an input WGS 84 lon, lat\n",
    "\n",
    "        Parameters:\n",
    "         points (tuple): input longitiude, latitude tuple\n",
    "    \n",
    "        Returns:\n",
    "         epsg_code (int): integer form of associated UTM EPSG\n",
    "    \"\"\"\n",
    "    lon, lat = points[0], points[1]\n",
    "\n",
    "    utm_band = str((math.floor((lon + 180) / 6 ) % 60) + 1)\n",
    "    if len(utm_band) == 1:\n",
    "        utm_band = '0'+utm_band\n",
    "    if lat >= 0:\n",
    "        epsg_code = '326' + utm_band\n",
    "    else:\n",
    "        epsg_code = '327' + utm_band\n",
    "    return int(epsg_code)\n",
    "    \n",
    "\n",
    "def calc_bbox(plot_id, df):\n",
    "    \"\"\" Calculates the corners of a bounding box from an input\n",
    "        pandas dataframe as output by Collect Earth Online\n",
    "\n",
    "        Parameters:\n",
    "         plot_id (int): plot_id of associated plot\n",
    "         df (pandas.DataFrame): dataframe of associated CEO survey\n",
    "    \n",
    "        Returns:\n",
    "         bounding_box (list): [(min(x), min(y)),\n",
    "                              (max(x), max_y))]\n",
    "    \"\"\"\n",
    "    subs = df[df['PLOT_ID'] == plot_id]\n",
    "    # TOP, LEFT, BOTTOM, RIGHT\n",
    "    # (min x, min y), (max x, max y)\n",
    "    return [(min(subs['LON']), min(subs['LAT'])),\n",
    "            (max(subs['LON']), max(subs['LAT']))]\n",
    "\n",
    "def bounding_box(points, expansion = 160):\n",
    "    \"\"\" Calculates the corners of a bounding box with an\n",
    "        input expansion in meters from a given bounding_box\n",
    "        \n",
    "        Subcalls:\n",
    "         calculate_epsg, convertCoords\n",
    "\n",
    "        Parameters:\n",
    "         points (list): output of calc_bbox\n",
    "         expansion (float): number of meters to expand or shrink the\n",
    "                            points edges to be\n",
    "    \n",
    "        Returns:\n",
    "         bl (tuple): x, y of bottom left corner with edges of expansion meters\n",
    "         tr (tuple): x, y of top right corner with edges of expansion meters\n",
    "    \"\"\"\n",
    "    # This must be lon, lat\n",
    "    bl = list(points[0])\n",
    "    tr = list(points[1])\n",
    "    \n",
    "    epsg = calculate_epsg(bl)\n",
    "    bl = convertCoords(bl, 4326, epsg)\n",
    "    tr = convertCoords(tr, 4326, epsg)\n",
    "    init = [b - a for a,b in zip(bl, tr)]\n",
    "    distance1 = tr[0] - bl[0]\n",
    "    distance2 = tr[1] - bl[1]\n",
    "    expansion1 = (expansion - distance1)/2\n",
    "    expansion2 = (expansion - distance2)/2\n",
    "    bl = [bl[0] - expansion1, bl[1] - expansion2]\n",
    "    tr = [tr[0] + expansion1, tr[1] + expansion2]\n",
    "\n",
    "    after = [b - a for a,b in zip(bl, tr)]   \n",
    "    diffs = [b - a for b, a in zip(after, init)]\n",
    "\n",
    "    bl = convertCoords(bl, epsg, 4326)\n",
    "    tr = convertCoords(tr, epsg, 4326)\n",
    "    return bl, tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_array(arr):\n",
    "    order = arr.argsort()\n",
    "    ranks = order.argsort()\n",
    "    return ranks\n",
    "\n",
    "def mcm_shadow_mask(arr, c_probs):\n",
    "    \"\"\" Calculates the multitemporal shadow mask for Sentinel-2 using\n",
    "        the methods from Candra et al. 2020 on L1C images and matching\n",
    "        outputs to the s2cloudless cloud probabilities\n",
    "\n",
    "        Parameters:\n",
    "         arr (arr): (Time, X, Y, Band) array of L1C data scaled from [0, 1]\n",
    "         c_probs (arr): (Time, X, Y) array of S2cloudless cloud probabilities\n",
    "    \n",
    "        Returns:\n",
    "         shadows_new (arr): cloud mask after Candra et al. 2020 and cloud matching \n",
    "         shadows_original (arr): cloud mask after Candra et al. 2020\n",
    "    \"\"\"\n",
    "    mean_c_probs = np.mean(c_probs, axis = (1, 2))\n",
    "    cloudy_steps = np.argwhere(mean_c_probs > 0.25)\n",
    "    images_clean = np.delete(arr, cloudy_steps, 0)\n",
    "    cloud_ranks = rank_array(mean_c_probs)\n",
    "    diffs = abs(np.sum(arr - np.mean(images_clean, axis = 0), axis = (1, 2, 3)))\n",
    "    diff_ranks = rank_array(diffs)\n",
    "    overall_rank = diff_ranks + cloud_ranks\n",
    "    reference_idx = np.argmin(overall_rank)\n",
    "    ri = arr[reference_idx]\n",
    "    print(\"The shadow reference index is: {}\".format(reference_idx))\n",
    "    \n",
    "    shadows = np.zeros((arr.shape[0], 96, 96))    \n",
    "    # Candra et al. 2020\n",
    "    \n",
    "    for time in range(arr.shape[0]):\n",
    "        for x in range(arr.shape[1]):\n",
    "            for y in range(arr.shape[2]):\n",
    "                ti_slice = arr[time, x, y]\n",
    "                ri_slice = ri[x, y]\n",
    "                deltab2 = ti_slice[0] - ri_slice[0]\n",
    "                #deltab3 = ti_slice[2] - ri_slice[2]\n",
    "                #deltab4 = ti_slice[3] - ri_slice[3]\n",
    "                deltab8a = ti_slice[1] - ri_slice[1]\n",
    "                deltab11 = ti_slice[2] - ri_slice[2]\n",
    "\n",
    "                if deltab2 <0.1: #(1000/65535):\n",
    "                    #if deltab3 < (800/65535)\n",
    "                        #if deltab4 < (800/65535)\n",
    "                    if deltab8a < -0.04: # (-400/65535):\n",
    "                        if deltab11 < -0.04: ##(-400/65535):\n",
    "                            if ti_slice[0] < 0.095: #(950/65535):\n",
    "                                shadows[time, x, y] = 1.\n",
    "                                        \n",
    "    shadows_original = np.copy(shadows)\n",
    "    # Remove shadows if cannot coreference a cloud\n",
    "    print(shadows.shape)\n",
    "    shadow_large = np.reshape(shadows, (shadows.shape[0], 96//8, 8, 96//8, 8))\n",
    "    shadow_large = np.sum(shadow_large, axis = (2, 4))\n",
    "    \n",
    "    cloud_large = np.copy(c_probs)\n",
    "    cloud_large[np.where(c_probs > 0.33)] = 1.\n",
    "    cloud_large[np.where(c_probs < 0.33)] = 0.\n",
    "    cloud_large = np.reshape(cloud_large, (shadows.shape[0], 96//8, 8, 96//8, 8))\n",
    "    cloud_large = np.sum(cloud_large, axis = (2, 4))\n",
    "    for time in range(shadow_large.shape[0]):\n",
    "        for x in range(shadow_large.shape[1]):\n",
    "            x_low = np.max([x - 8, 0])\n",
    "            x_high = np.min([x + 8, shadow_large.shape[1] - 1])\n",
    "            for y in range(shadow_large.shape[2]):\n",
    "                y_low = np.max([y - 8, 0])\n",
    "                y_high = np.min([y + 8, shadow_large.shape[1] - 1])\n",
    "                if shadow_large[time, x, y] < 8:\n",
    "                    shadow_large[time, x, y] = 0.\n",
    "                if shadow_large[time, x, y] >= 8:\n",
    "                    shadow_large[time, x, y] = 1.\n",
    "                c_prob_window = cloud_large[time, x_low:x_high, y_low:y_high]\n",
    "                if np.max(c_prob_window) < 24:\n",
    "                    shadow_large[time, x, y] = 0.\n",
    "                    \n",
    "    \n",
    "    shadow_large = resize(shadow_large, (shadow_large.shape[0], 96, 96), order = 0)\n",
    "    shadows *= shadow_large\n",
    "    \n",
    "    # Go through and aggregate the shadow map to an 80m grid, and extend it one grid size around\n",
    "    # any positive ID\n",
    "    \n",
    "    \n",
    "    shadows = np.reshape(shadows, (shadows.shape[0], 96//8, 8, 96//8, 8))\n",
    "    shadows = np.sum(shadows, axis = (2, 4))\n",
    "    shadows[np.where(shadows < 16)] = 0.\n",
    "    shadows[np.where(shadows >= 16)] = 1.\n",
    "    shadows = resize(shadows, (shadows.shape[0], 96, 96), order = 0)\n",
    "    shadows = np.reshape(shadows, (shadows.shape[0], 96//4, 4, 96//4, 4))\n",
    "    shadows = np.max(shadows, (2, 4))\n",
    "    \n",
    "    shadows_new = np.zeros_like(shadows)\n",
    "    for time in range(shadows.shape[0]):\n",
    "        for x in range(shadows.shape[1]):\n",
    "            for y in range(shadows.shape[2]):\n",
    "                if shadows[time, x, y] == 1:\n",
    "                    min_x = np.max([x - 1, 0])\n",
    "                    max_x = np.min([x + 2, shadows.shape[1] - 1])\n",
    "                    min_y = np.max([y - 1, 0])\n",
    "                    max_y = np.min([y + 2, shadows.shape[1] - 1])\n",
    "                    for x_idx in range(min_x, max_x):\n",
    "                        for y_idx in range(min_y, max_y):\n",
    "                            shadows_new[time, x_idx, y_idx] = 1.\n",
    "    shadows_new = resize(shadows_new, (shadows.shape[0], 96, 96), order = 0)\n",
    "    print(\"The shadow probability is: {}\".format(100*np.sum(shadows_new)/(96*96*shadows_new.shape[0])))\n",
    "    return np.array(shadows_new)\n",
    "\n",
    "def identify_clouds(bbox, epsg = EPSG, time = TIME, cloud_detector = CLOUD_DETECTOR,\n",
    "                    image_format =  MimeType.TIFF_d16):\n",
    "    \"\"\" Calculates the cloud probabilities of L1C Sentinel-2 data with\n",
    "        the S2Cloudless light GBM\n",
    "\n",
    "        Parameters:\n",
    "         bbox (list): output of calc_bbox\n",
    "         epsg (float): number \n",
    "         time (float): number \n",
    "         cloud_detector (obj): \n",
    "    \n",
    "        Returns:\n",
    "         cloud_probs (arr)\n",
    "         shadows (arr)\n",
    "         shadow_original (arr)\n",
    "         cloud_probs (arr)\n",
    "    \"\"\"\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    cloud_request = WmsRequest(\n",
    "        layer='CLOUD_DETECTION',\n",
    "        bbox=box,\n",
    "        time=time,\n",
    "        width=96,\n",
    "        height=96,\n",
    "        image_format = image_format,\n",
    "        maxcc=0.75,\n",
    "        instance_id=API_KEY,\n",
    "        custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "        time_difference=datetime.timedelta(hours=48),\n",
    "    )\n",
    "\n",
    "    cloud_img = cloud_request.get_data()\n",
    "    print(\"Bef\", np.max(cloud_img))\n",
    "    if image_format == MimeType.TIFF_d16:\n",
    "        cloud_img = np.array(cloud_img) / 65535\n",
    "    print(\"Aft cl\", np.max(cloud_img))\n",
    "    cloud_probs = cloud_detector.get_cloud_probability_maps(np.array(cloud_img))\n",
    "    shadows, shadow_original = mcm_shadow_mask(np.array(cloud_img), cloud_probs)\n",
    "    print(shadows.shape)\n",
    "    print(type(shadows))\n",
    "    shadows = np.array(shadows)[:, 24:-24, 24:-24]\n",
    "    return cloud_probs[:, 24:-24, 24:-24], shadows, shadow_original, cloud_probs\n",
    "    #except Exception as e:\n",
    "    #    logging.fatal(e, exc_info=True)\n",
    "    \n",
    "def identify_clouds_new(bbox, epsg = EPSG, time = TIME):\n",
    "\n",
    "    for try_ in range(0, 5):\n",
    "        try:\n",
    "            box = BBox(bbox, crs = epsg)\n",
    "            cloud_request = WmsRequest(\n",
    "                layer='CLOUD_NEW',\n",
    "                bbox=box,\n",
    "                time=time,\n",
    "                width=96,\n",
    "                height=96,\n",
    "                image_format =  MimeType.TIFF_d32f,\n",
    "                maxcc=0.75,\n",
    "                instance_id=API_KEY,\n",
    "                custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "                time_difference=datetime.timedelta(hours=48))\n",
    "            \n",
    "            shadow_request = WmsRequest(\n",
    "                layer='SHADOW',\n",
    "                bbox=box,\n",
    "                time=time,\n",
    "                width=96,\n",
    "                height=96,\n",
    "                image_format =  MimeType.TIFF_d16,\n",
    "                maxcc=0.75,\n",
    "                instance_id=API_KEY,\n",
    "                custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "                time_difference=datetime.timedelta(hours=48))\n",
    "\n",
    "            cloud_img = cloud_request.get_data()\n",
    "            cloud_img = np.array(cloud_img)\n",
    "            cloud_probs = cloud_img / 255\n",
    "            \n",
    "            shadow_img = shadow_request.get_data()\n",
    "            shadow_img = np.array(shadow_img)\n",
    "            print(np.max(shadow_img))\n",
    "            shadow_img = shadow_img / 65535\n",
    "            print(np.max(shadow_img))\n",
    "            \n",
    "            shadows = mcm_shadow_mask(np.array(shadow_img), cloud_probs)\n",
    "            shadows = shadows[:, 24:-24, 24:-24]\n",
    "            return cloud_probs[:, 24:-24, 24:-24], shadows#, shadow_original, cloud_probs\n",
    "        except Exception as e:\n",
    "            logging.fatal(e, exc_info=True)\n",
    "    \n",
    "    \n",
    "def download_dem(plot_id, df, epsg = EPSG, image_format = MimeType.TIFF_d32f):\n",
    "    #! TODO: ensure that centroid vs. bbox is correctly distinguished\n",
    "    \"\"\" Downloads MapZen digital elevation model and return slope\n",
    "\n",
    "        Parameters:\n",
    "         plot_id (tuple): plot id from collect earth online (CEO)\n",
    "         df (pandas.DataFrame): data associated with plot_id from CEO\n",
    "         epsg (int): UTM EPSG associated with plot_id\n",
    "    \n",
    "        Returns:\n",
    "         slope (arr): (X, Y, 1) array of per-pixel slope from [0, 1]\n",
    "    \"\"\"\n",
    "    location = calc_bbox(plot_id, df = df)\n",
    "    bbox = bounding_box(location, expansion = (IMSIZE+2)*10)\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    dem_request = WmsRequest(data_source=DataSource.DEM,\n",
    "                         layer='DEM',\n",
    "                         bbox=box,\n",
    "                         width=IMSIZE+2,\n",
    "                         height=IMSIZE+2,\n",
    "                         instance_id=API_KEY,\n",
    "                         image_format= image_format,\n",
    "                         custom_url_params={CustomUrlParam.SHOWLOGO: False})\n",
    "    dem_image = dem_request.get_data()[0]\n",
    "    slope = calcSlope(dem_image.reshape((1, IMSIZE+2, IMSIZE+2)),\n",
    "                      np.full((IMSIZE+2, IMSIZE+2), 10),\n",
    "                      np.full((IMSIZE+2, IMSIZE+2), 10), \n",
    "                      zScale = 1, minSlope = 0.02)\n",
    "    slope = slope.reshape((IMSIZE+2, IMSIZE+2, 1))\n",
    "    slope = slope[1:IMSIZE+1, 1:IMSIZE+1, :]\n",
    "    print(slope.shape)\n",
    "    return slope\n",
    "\n",
    "def check_zenith(bbox, epsg = EPSG, time = TIME):\n",
    "    \"\"\" Downloads the zenith layer from sentinel-hub so that\n",
    "        if desired, the data can be subsetted by zenith angle\n",
    "        \n",
    "        Parameters:\n",
    "         bbox (list): output of calc_bbox\n",
    "         epsg (float): number \n",
    "         time (float): number \n",
    "    \n",
    "        Returns:\n",
    "         zenith (arr): (Time, X, Y) array of per-pixel zenith from [0, 1] \n",
    "    \"\"\"\n",
    "    try:\n",
    "        box = BBox(bbox, crs = epsg)\n",
    "        zenith = WmsRequest(\n",
    "            layer='ZENITH',\n",
    "            bbox=box,\n",
    "            time=time,\n",
    "            width=IMSIZE,\n",
    "            height=IMSIZE,\n",
    "            image_format = MimeType.TIFF_d16,\n",
    "            maxcc=0.75,\n",
    "            instance_id=API_KEY,\n",
    "            custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "            time_difference=datetime.timedelta(hours=48),\n",
    "        )\n",
    "        \n",
    "        zenith = zenith.get_data()\n",
    "        return zenith\n",
    "    except Exception as e:\n",
    "        logging.fatal(e, exc_info=True)\n",
    "        \n",
    "def download_layer(bbox, epsg = EPSG, time = TIME, image_format = MimeType.TIFF_d16):\n",
    "    \"\"\" Downloads the VV and VH gamma backscatter coefficient sentinel 1 data\n",
    "        \n",
    "        Parameters:\n",
    "         bbox (list): output of calc_bbox\n",
    "         epsg (float): UTM EPSG associated with bbox \n",
    "         time (tuple): YY-MM-DD - YY-MM-DD bounds for downloading \n",
    "    \n",
    "        Returns:\n",
    "         img (arr):\n",
    "         img_request (obj): \n",
    "    \"\"\"\n",
    "    try:\n",
    "        box = BBox(bbox, crs = epsg)\n",
    "        image_request = WcsRequest(\n",
    "                layer='L2A20',\n",
    "                bbox=box,\n",
    "                time=time,\n",
    "                image_format = image_format,\n",
    "                maxcc=0.75,\n",
    "                resx='10m', resy='10m',\n",
    "                instance_id=API_KEY,\n",
    "                custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'NEAREST',\n",
    "                                    constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "                time_difference=datetime.timedelta(hours=48),\n",
    "            )\n",
    "        img_bands = image_request.get_data()\n",
    "        img_20 = np.stack(img_bands)\n",
    "        img_20 = resize(img_20, (img_20.shape[0], IMSIZE, IMSIZE, img_20.shape[-1]), order = 0)\n",
    "        \n",
    "        image_request = WcsRequest(\n",
    "                layer='L2A10',\n",
    "                bbox=box,\n",
    "                time=time,\n",
    "                image_format = image_format,\n",
    "                maxcc=0.75,\n",
    "                resx='10m', resy='10m',\n",
    "                instance_id=API_KEY,\n",
    "                custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'BICUBIC',\n",
    "                                    constants.CustomUrlParam.UPSAMPLING: 'BICUBIC'},\n",
    "                time_difference=datetime.timedelta(hours=48),\n",
    "        )\n",
    "        \n",
    "        img_bands = image_request.get_data()\n",
    "        img_10 = np.stack(img_bands)\n",
    "        print(\"The original L2A image size is: {}\".format(img_10.shape))\n",
    "        img_10 = resize(img_10, (img_10.shape[0], IMSIZE, IMSIZE, img_10.shape[-1]), order = 0)\n",
    "        img = np.concatenate([img_10, img_20], axis = -1)\n",
    "        print(np.max(img))\n",
    "        #if image_format == MimeType.TIFF_d16:\n",
    "        #    img = np.array(img) / 65535\n",
    "        return img, image_request\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.fatal(e, exc_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud and shadow removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_cloud_and_shadows(tiles, probs, shadows, image_dates, wsize = 5):\n",
    "    \"\"\" Interpolates clouds and shadows for each time step with \n",
    "        linear combination of proximal clean time steps for each\n",
    "        region of specified window size\n",
    "        \n",
    "        Parameters:\n",
    "         tiles (arr):\n",
    "         probs (arr): \n",
    "         shadows (arr):\n",
    "         image_dates (list):\n",
    "         wsize (int): \n",
    "    \n",
    "        Returns:\n",
    "         tiles (arr): \n",
    "    \"\"\"\n",
    "    c_probs = np.copy(probs)\n",
    "    c_probs = c_probs - np.min(c_probs, axis = 0)\n",
    "    c_probs[np.where(c_probs > 0.33)] = 1.\n",
    "    c_probs[np.where(c_probs < 0.33)] = 0.\n",
    "    c_probs = np.reshape(c_probs, [c_probs.shape[0], int(IMSIZE/8), 8, int(IMSIZE/8), 8])\n",
    "    c_probs = np.sum(c_probs, (2, 4))\n",
    "    c_probs = resize(c_probs, (c_probs.shape[0], IMSIZE, IMSIZE), 0)\n",
    "    c_probs[np.where(c_probs < 12)] = 0.\n",
    "    c_probs[np.where(c_probs >= 12)] = 1.\n",
    "    c_probs += shadows\n",
    "    c_probs[np.where(c_probs >= 1.)] = 1.\n",
    "    n_interp = 0\n",
    "    for cval in range(0, IMSIZE - 5, 1):\n",
    "        for rval in range(0, IMSIZE - 5, 1):\n",
    "            subs = c_probs[:, cval:cval + wsize, rval:rval+wsize]\n",
    "            satisfactory = [x for x in range(c_probs.shape[0]) if np.sum(subs[x, :, :]) < 10]\n",
    "            satisfactory = np.array(satisfactory)\n",
    "            for date in range(0, tiles.shape[0]):\n",
    "                if np.sum(subs[date, :, :]) > 10:\n",
    "                    n_interp += 1\n",
    "                    before, after = calculate_proximal_steps_index(date, satisfactory)\n",
    "                    before = date + before\n",
    "                    after = date + after\n",
    "                    if after >= tiles.shape[0]:\n",
    "                        after = before\n",
    "                    if before < 0:\n",
    "                        before = after\n",
    "                    bef = tiles[before, cval:cval+wsize, rval:rval+wsize, : ]\n",
    "                    aft = tiles[after, cval:cval+wsize, rval:rval+wsize, : ]\n",
    "                    before = image_dates[before]\n",
    "                    after = image_dates[after]\n",
    "                    before_diff = abs(image_dates[date] - before)\n",
    "                    after_diff = abs(image_dates[date] - after)\n",
    "                    bef_wt = 1 - before_diff / (before_diff + after_diff)\n",
    "                    aft_wt = 1 - bef_wt\n",
    "                    candidate = bef_wt*bef + aft_wt*aft\n",
    "                    candidate = candidate*c_arr + tiles[date, cval:cval+wsize, rval:rval+wsize, : ]*o_arr\n",
    "                    tiles[date, cval:cval+wsize, rval:rval+wsize, : ] = candidate  \n",
    "    print(\"Interpolated {} px\".format(n_interp))\n",
    "    return tiles\n",
    "\n",
    "def remove_missed_clouds(img):\n",
    "    \"\"\" Removes steps that are likely to be missed cloud or shadows\n",
    "        based on two interquartile ranges for the near infrared band\n",
    "        \n",
    "        Parameters:\n",
    "         img (arr):\n",
    "\n",
    "        Returns:\n",
    "         to_remove (list): \n",
    "    \"\"\"\n",
    "    iqr = np.percentile(img[:, :, :, 3].flatten(), 75) - np.percentile(img[:, :, :, 3].flatten(), 25)\n",
    "    thresh_t = np.percentile(img[:, :, :, 3].flatten(), 75) + iqr*2\n",
    "    thresh_b = np.percentile(img[:, :, :, 3].flatten(), 25) - iqr*2\n",
    "    diffs_fw = np.diff(img, 1, axis = 0)\n",
    "    diffs_fw = np.mean(diffs_fw, axis = (1, 2, 3))\n",
    "    diffs_fw = np.array([0] + list(diffs_fw))\n",
    "    diffs_bw = np.diff(np.flip(img, 0), 1, axis = 0)\n",
    "    diffs_bw = np.flip(np.mean(diffs_bw, axis = (1, 2, 3)))\n",
    "    diffs_bw = np.array(list(diffs_bw) + [0])\n",
    "    diffs = abs(diffs_fw - diffs_bw) * 100 # 3, -3 -> 6, -3, 3 -> 6, -3, -3\n",
    "    #diffs = [int(x) for x in diffs]\n",
    "    outlier_percs = []\n",
    "    for step in range(img.shape[0]):\n",
    "        bottom = len(np.argwhere(img[step, :, :, 3].flatten() > thresh_t))\n",
    "        top = len(np.argwhere(img[step, :, :, 3].flatten() < thresh_b))\n",
    "        p = 100* ((bottom + top) / (IMSIZE*IMSIZE))\n",
    "        outlier_percs.append(p)\n",
    "    to_remove = np.argwhere(np.array(outlier_percs) > 15)\n",
    "    return to_remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_and_save_best_images(img_bands, image_dates):\n",
    "    \"\"\" Interpolate input data of (Time, X, Y, Band) to a constant\n",
    "        (72, X, Y, Band) shape with one time step every five days\n",
    "        \n",
    "        Parameters:\n",
    "         img_bands (arr):\n",
    "         image_dates (list):\n",
    "         \n",
    "        Returns:\n",
    "         keep_steps (arr):\n",
    "         max_distance (int)\n",
    "    \"\"\"\n",
    "    biweekly_dates = [day for day in range(0, 360, 5)] # ideal imagery dates are every 15 days\n",
    "    \n",
    "    # Clouds have been removed at this step, so all steps are satisfactory\n",
    "    satisfactory_ids = [x for x in range(0, img_bands.shape[0])]\n",
    "    satisfactory_dates = [value for idx, value in enumerate(image_dates) if idx in satisfactory_ids]\n",
    "    \n",
    "    \n",
    "    selected_images = {}\n",
    "    for i in biweekly_dates:\n",
    "        distances = [abs(date - i) for date in satisfactory_dates]\n",
    "        closest = np.min(distances)\n",
    "        closest_id = np.argmin(distances)\n",
    "        # If there is imagery within 5 days, select it\n",
    "        if closest < 8:\n",
    "            date = satisfactory_dates[closest_id]\n",
    "            image_idx = int(np.argwhere(np.array(image_dates) == date)[0])\n",
    "            selected_images[i] = {'image_date': [date], 'image_ratio': [1], 'image_idx': [image_idx]}\n",
    "        # If there is not imagery within 7 days, look for the closest above and below imagery\n",
    "        else:\n",
    "            distances = np.array([(date - i) for date in satisfactory_dates])\n",
    "            # Number of days above and below the selected date of the nearest clean imagery\n",
    "            above = distances[np.where(distances < 0, distances, -np.inf).argmax()]\n",
    "            below = distances[np.where(distances > 0, distances, np.inf).argmin()]\n",
    "            if abs(above) > 240: # If date is the last date, occassionally argmax would set above to - number\n",
    "                above = below\n",
    "            if abs(below) > 240:\n",
    "                below = above\n",
    "            if above != below:\n",
    "                below_ratio = above / (above - below)\n",
    "                above_ratio = 1 - below_ratio\n",
    "            else:\n",
    "                above_ratio = below_ratio = 0.5\n",
    "                \n",
    "            # Extract the image date and imagery index for the above and below values\n",
    "            above_date = i + above\n",
    "            above_image_idx = int(np.argwhere(np.array(image_dates) == above_date)[0])\n",
    "            \n",
    "            below_date = i + below\n",
    "            below_image_idx = int(np.argwhere(np.array(image_dates) == below_date)[0])\n",
    "            \n",
    "            selected_images[i] = {'image_date': [above_date, below_date], 'image_ratio': [above_ratio, below_ratio],\n",
    "                                 'image_idx': [above_image_idx, below_image_idx]}\n",
    "                               \n",
    "    max_distance = 0\n",
    "    \n",
    "    for i in selected_images.keys():\n",
    "        #print(i, selected_images[i])\n",
    "        if len(selected_images[i]['image_date']) == 2:\n",
    "            dist = selected_images[i]['image_date'][1] - selected_images[i]['image_date'][0]\n",
    "            if dist > max_distance:\n",
    "                max_distance = dist\n",
    "    \n",
    "    print(\"Maximum time distance: {}\".format(max_distance))\n",
    "        \n",
    "    # Compute the weighted average of the selected imagery for each time step\n",
    "    keep_steps = []\n",
    "    use_median = False\n",
    "    for i in selected_images.keys():\n",
    "        step1_additional = None\n",
    "        step2_additional = None\n",
    "        info = selected_images[i]\n",
    "        if len(info['image_idx']) == 1:\n",
    "            step = img_bands[info['image_idx'][0]]\n",
    "        if len(info['image_idx']) == 2:\n",
    "            step1 = img_bands[info['image_idx'][0]] # * info['image_ratio'][0]\n",
    "            step2 = img_bands[info['image_idx'][1]]\n",
    "            #if info['image_idx'][0] == 0:\n",
    "            #    step1_additional = img_bands[-1]\n",
    "            #    print(\"Using last step\")\n",
    "            #if info['image_idx'][1] == (img_bands.shape[0] - 1):\n",
    "            #    step2_additional = img_bands[0]\n",
    "            ##    print(\"Using first step\")\n",
    "            #if step1_additional is None and step2_additional is None:\n",
    "            step = step1 * 0.5 + step2 * 0.5\n",
    "            #if step1_additional is not None:\n",
    "            #    print(\"Echo\")\n",
    "            #    step = (step1 + step2 + step1_additional) * (1/3)\n",
    "            #if step2_additional is not None:\n",
    "             #   print(\"Echo\")\n",
    "            #    step = (step1 + step2 + step2_additional) * (1/3)\n",
    "        keep_steps.append(step)\n",
    "    '''\n",
    "    for i in selected_images.keys():\n",
    "        info = selected_images[i]\n",
    "        if len(info['image_idx']) == 1:\n",
    "            step = img_bands[info['image_idx'][0]]\n",
    "            use_median = False\n",
    "        if len(info['image_idx']) == 2:\n",
    "            difference = np.max([abs(info['image_date'][0] - int(i)),\n",
    "                                 abs(info['image_date'][1] - int(i))]) \n",
    "            step1 = img_bands[info['image_idx'][0]] # * info['image_ratio'][0]\n",
    "            step2_idx = info['image_idx'][0] - 1\n",
    "            if step2_idx < 0:\n",
    "                step2_idx = (img_bands.shape[0] - 1)\n",
    "            step2 = img_bands[step2_idx]\n",
    "            step3 = img_bands[info['image_idx'][1]]\n",
    "            step4_idx = info['image_idx'][1] + 1\n",
    "            if step4_idx > (img_bands.shape[0] - 1):\n",
    "                step4_idx = 0\n",
    "            step4 = img_bands[step4_idx]\n",
    "            #step2 = img_bands[info['image_idx'][1]] * 0.5 # info['image_ratio'][1]\n",
    "            if difference > 100 or use_median == True:\n",
    "                print(\"Median, {}\".format(difference))\n",
    "                use_median = True\n",
    "                stacked = np.stack([step1, step2, step3, step4])\n",
    "                step = np.median(stacked, axis = 0)\n",
    "            else:\n",
    "                use_median = False\n",
    "                step = step1 * 0.5 + step3 * 0.5\n",
    "        '''\n",
    "        #keep_steps.append(step)\n",
    "        \n",
    "    keep_steps = np.stack(keep_steps)\n",
    "    return keep_steps, max_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def download_plots(data_location = DATA_LOCATION, output_folder = OUTPUT_FOLDER, image_format = MimeType.TIFF_d16):\n",
    "    \"\"\" Downloads slope and sentinel-2 data for all plots associated\n",
    "        with an input CSV from a collect earth online survey\n",
    "        \n",
    "        Parameters:\n",
    "         data_location (os.path)\n",
    "         output_folder (os.path)\n",
    "        \n",
    "        Subcalls:\n",
    "         calc_bbox, bounding_box\n",
    "         identify_clouds, download_layer, check_zenith, download_dem\n",
    "         remove_clouds_and_shadows, remove_missed_clouds\n",
    "         DSen2\n",
    "         calculate_and_save_best_images\n",
    "         \n",
    "        Creates:\n",
    "         output_folder/{plot_id}.npy\n",
    "    \n",
    "        Returns:\n",
    "         None\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(data_location)\n",
    "    for column in ['IMAGERY_TITLE', 'STACKINGPROFILEDG', 'PL_PLOTID', 'IMAGERYYEARDG']:\n",
    "        if column in df.columns:\n",
    "            df = df.drop(column, axis = 1)\n",
    "    df = df.dropna(axis = 0)\n",
    "    plot_ids = sorted(df['PLOT_ID'].unique())\n",
    "    existing = [int(x[:-4]) for x in os.listdir(output_folder) if \".DS\" not in x]\n",
    "    to_download = [x for x in plot_ids if x not in existing]\n",
    "    print(\"STARTING DOWNLOAD OF {} plots from {} to {}\".format(len(to_download), data_location, output_folder))\n",
    "    errors = []\n",
    "    for i, val in enumerate(to_download):\n",
    "        print(\"Downloading {}/{}, {}\".format(i+1, len(to_download), val))\n",
    "        location = calc_bbox(val, df = df)\n",
    "        location = bounding_box(location, expansion = IMSIZE*10)\n",
    "        location_clouds = bounding_box(location, expansion = 96*10)\n",
    "        try:\n",
    "            # Identify cloud steps, download DEM, and download L2A series\n",
    "            probs, shadows = identify_clouds(location_clouds, image_format = image_format)\n",
    "            shadow_sums = np.sum(shadows, axis = (1, 2))\n",
    "            shadow_steps = np.argwhere(shadow_sums > (48*48/3))\n",
    "            dem = download_dem(val, df = df)\n",
    "            img, image_request = download_layer(location, image_format = image_format)\n",
    "            print(img[0, :, :, 0])\n",
    "            #np.save(\"../data/raw/train-raw/\" + str(val) + \".npy\", img)\n",
    "            #np.save(\"../data/raw/train-dates/\" + str(val) + \".npy\", image_request.get_dates())\n",
    "\n",
    "            # Calculate imagery dates\n",
    "            image_dates = []\n",
    "            for date in image_request.get_dates():\n",
    "                if date.year == YEAR - 1:\n",
    "                    image_dates.append(-365 + starting_days[(date.month-1)] + date.day)\n",
    "                if date.year == YEAR:\n",
    "                    image_dates.append(starting_days[(date.month-1)] + date.day)\n",
    "                if date.year == YEAR + 1:\n",
    "                    image_dates.append(365 + starting_days[(date.month-1)]+date.day)\n",
    "            image_dates = np.array(image_dates)\n",
    "\n",
    "            # Remove imagery where >4% is clouds, and where there is null data\n",
    "            args = np.array([len(np.argwhere(probs[x].flatten() > 0.3)) for x in range(probs.shape[0])])\n",
    "            dirty_steps = np.argwhere(args > (IMSIZE)*(IMSIZE) / 5)\n",
    "            missing_images = [np.argwhere(img[x, :, : :].flatten() == 0.0) for x in range(img.shape[0])]\n",
    "            missing_images = np.array([len(x) for x in missing_images])\n",
    "            missing_images_p = [np.argwhere(img[x, :, : :].flatten() >= 1) for x in range(img.shape[0])]\n",
    "            missing_images_p = np.array([len(x) for x in missing_images_p])\n",
    "            missing_images += missing_images_p\n",
    "            missing_images = list(np.argwhere(missing_images >= 25))\n",
    "            to_remove = np.unique(np.array(list(dirty_steps) + list(missing_images) + list(shadow_steps)))\n",
    "\n",
    "            # Remove null steps\n",
    "            print(\"There are {}/{} dirty steps: {}\"\n",
    "                  \" cloud, {} missing, {} shadow\".format(len(to_remove),\n",
    "                                                         len(img), len(dirty_steps),\n",
    "                                                         len(missing_images),\n",
    "                                                         #len(zenith_outliers),\n",
    "                                                         len(shadow_steps)))\n",
    "\n",
    "            img = np.delete(img, to_remove, 0)\n",
    "            probs = np.delete(probs, to_remove, 0)\n",
    "            image_dates = np.delete(image_dates, to_remove)\n",
    "            shadows = np.delete(shadows, to_remove, 0)\n",
    "\n",
    "            to_remove = remove_missed_clouds(img)\n",
    "            img = np.delete(img, to_remove, 0)\n",
    "            probs = np.delete(probs, to_remove, 0)\n",
    "            image_dates = np.delete(image_dates, to_remove)\n",
    "            shadows = np.delete(shadows, to_remove, 0)\n",
    "            print(\"Removing {} steps based on ratio\".format(len(to_remove)))\n",
    "\n",
    "\n",
    "            # Concatenate DEM\n",
    "            dem = np.tile(dem.reshape((1, IMSIZE, IMSIZE, 1)), (img.shape[0], 1, 1, 1))\n",
    "            tiles = np.concatenate([img, dem], axis = -1)\n",
    "            tiles[:, :, :, -1] /= 90\n",
    "\n",
    "            x = remove_cloud_and_shadows(tiles, probs, shadows, image_dates)\n",
    "            if SUPER_RESOLVE:\n",
    "                x = x[:, 8:40, 8:40, :]\n",
    "                print(\"Shape before super: {}\".format(x.shape))\n",
    "\n",
    "                d10 = x[:, :, :, 0:4]\n",
    "                d20 = x[:, :, :, 4:10]\n",
    "\n",
    "                d10 = np.swapaxes(d10, 1, -1)\n",
    "                d10 = np.swapaxes(d10, 2, 3)\n",
    "                d20 = np.swapaxes(d20, 1, -1)\n",
    "                d20 = np.swapaxes(d20, 2, 3)\n",
    "                superresolved = DSen2(d10, d20)\n",
    "                superresolved = np.swapaxes(superresolved, 1, -1)\n",
    "                superresolved = np.swapaxes(superresolved, 1, 2)\n",
    "                print(superresolved.shape)\n",
    "                print(x.shape)\n",
    "\n",
    "                # returns band IDXs 3, 4, 5, 7, 8, 9\n",
    "                x[:, :, :, 4:10] = superresolved\n",
    "                x = x[:, 8:24, 8:24, :]\n",
    "                print(\"Shape after super: {}\".format(x.shape))\n",
    "            else:\n",
    "                bottom = int(IMSIZE/2 - 8)\n",
    "                top = int(IMSIZE/2 + 8)\n",
    "                x = x[:, bottom:top, bottom:top, :]\n",
    "\n",
    "            # Calculate indices\n",
    "            tiles, amin = evi(x, True)\n",
    "            tiles = bi(tiles, True)\n",
    "            tiles = msavi2(tiles, True)\n",
    "            x = si(tiles, True)\n",
    "\n",
    "            print(\"Shape after vegetation indexes: {}\".format(x.shape))\n",
    "            \n",
    "            missing_pixels = 0\n",
    "            for band in range(0, 15):\n",
    "                for time in range(0, x.shape[0]):\n",
    "                    x_i = x[time, :, :, band]\n",
    "                    missing_pixels += len(np.argwhere(np.isnan(x_i)))\n",
    "                    x_i[np.argwhere(np.isnan(x_i))] = np.mean(x_i)\n",
    "                    x[time, :, :, band] = x_i\n",
    "            print(\"There are {} missing pixels\".format(missing_pixels))\n",
    "\n",
    "            # Interpolate linearly to 5 day frequency\n",
    "            tiles, max_distance = calculate_and_save_best_images(x, image_dates)\n",
    "\n",
    "            # Smooth linear interpolation\n",
    "            for row in range(0, 16):\n",
    "                for column in range(0, 16):\n",
    "                    for band in [x for x in range(0, 15) if x != 10]:\n",
    "                        sm = smooth(tiles[:, row, column, band], 800, d = 2)\n",
    "                        tiles[:, row, column, band] = sm\n",
    "\n",
    "            # Retain only iamgery every 15 days\n",
    "            biweekly_dates = np.array([day for day in range(0, 360, 5)])\n",
    "            to_remove = np.argwhere(biweekly_dates % 15 != 0)\n",
    "            tiles = np.delete(tiles, to_remove, 0)\n",
    "\n",
    "            if max_distance <= 240:\n",
    "                np.save(output_folder + str(val), tiles)\n",
    "                #return tiles\n",
    "                #np.save(\"../data/raw/train-clouds/\" + str(val) + \".npy\", probs)\n",
    "                #np.save(\"../data/raw/train-shadows/\" + str(val) + \".npy\", shadows)\n",
    "                print(\"Saved array of {} shape to {}\".format(tiles.shape, val))\n",
    "                print(\"\\n\")\n",
    "            else:\n",
    "                print(\"Skipping {} because there is a {} distance\".format(val, max_distance))\n",
    "                print(\"\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            logging.fatal(e, exc_info=True)\n",
    "            #errors.append(img)\n",
    "            #continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(shadows, axis = (1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1c3b6173c8>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(probs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Downloading 1/51, 135804022\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-176-cfc58c8155ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mlocation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_bbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mlocation_clouds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbounding_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpansion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m96\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshadows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midentify_clouds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation_clouds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-169-7c3547eaf62d>\u001b[0m in \u001b[0;36midentify_clouds\u001b[0;34m(bbox, epsg, time, cloud_detector, image_format)\u001b[0m\n\u001b[1;32m    139\u001b[0m     )\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0mcloud_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcloud_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Bef\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcloud_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mimage_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mMimeType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTIFF_d16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sentinelhub/data_request.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, save_data, data_filter, redownload, max_threads, raise_download_errors)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \"\"\"\n\u001b[1;32m    108\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preprocess_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mdata_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_data_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_filter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mredownload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_threads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_download_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_saved_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_filter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_download_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sentinelhub/data_request.py\u001b[0m in \u001b[0;36m_execute_data_download\u001b[0;34m(self, data_filter, redownload, max_threads, raise_download_errors)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mdata_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdownload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_download_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mredownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mredownload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_threads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_threads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0mdata_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSHConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_timeout_seconds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sentinelhub/download.py\u001b[0m in \u001b[0;36mdownload_data\u001b[0;34m(request_list, redownload, max_threads)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mconcurrent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mThreadPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_threads\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecute_download_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrequest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrequest_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/remote_sensing/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/remote_sensing/lib/python3.6/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_threads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                 \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m     \u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/remote_sensing/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/remote_sensing/lib/python3.6/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in os.listdir(\"../data/test-csv/\")[1:2]:\n",
    "    if \".csv\" in i:\n",
    "        df = pd.read_csv(\"../data/test-csv/{}\".format(i))\n",
    "        for column in ['IMAGERY_TITLE', 'STACKINGPROFILEDG', 'PL_PLOTID', 'IMAGERYYEARDG']:\n",
    "            if column in df.columns:\n",
    "                df = df.drop(column, axis = 1)\n",
    "        df = df.dropna(axis = 0)\n",
    "        plot_ids = sorted(df['PLOT_ID'].unique())\n",
    "        existing = []#[int(x[:-4]) for x in os.listdir(\"../data/raw/test-shadows/\") if \".DS\" not in x]\n",
    "        print(existing)\n",
    "        to_download = [x for x in plot_ids if x not in existing]\n",
    "        errors = []\n",
    "        for i, val in enumerate(to_download[:1]):\n",
    "            print(\"Downloading {}/{}, {}\".format(i+1, len(to_download), val))\n",
    "            location = calc_bbox(val, df = df)\n",
    "            location_clouds = bounding_box(location, expansion = 96*10)\n",
    "            probs, shadows, _, _ = identify_clouds_new(location_clouds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in os.listdir(\"../data/drylands/csv/\"):\n",
    "    if \"fao-test-2.csv\" in i:\n",
    "    #if \".csv\" in i:\n",
    "        #if any(x in i for x in [\"africa-west\", \"cameroon\", \"koure\", \"niger\"]):\n",
    "        tile = download_plots(\"../data/drylands/csv/\" + i, \"../data/drylands/s2/\", image_format = MimeType.TIFF_d16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_sensing",
   "language": "python",
   "name": "remote_sensing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
