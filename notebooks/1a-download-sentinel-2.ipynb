{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and process sentinel 2 data\n",
    "\n",
    "## John Brandt\n",
    "## April 1, 2020\n",
    "\n",
    "## Package imports, API import, source scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/john.brandt/anaconda3/envs/remote_sensing/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/john.brandt/anaconda3/envs/remote_sensing/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import scipy.sparse as sparse\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "\n",
    "from collections import Counter\n",
    "from osgeo import ogr, osr\n",
    "from random import shuffle\n",
    "from scipy.sparse.linalg import splu\n",
    "from s2cloudless import S2PixelCloudDetector, CloudMaskRequest\n",
    "from sentinelhub import WmsRequest, WcsRequest, MimeType\n",
    "from sentinelhub import CRS, BBox, constants, DataSource, CustomUrlParam\n",
    "from skimage.transform import resize\n",
    "\n",
    "with open(\"../config.yaml\", 'r') as stream:\n",
    "        key = (yaml.safe_load(stream))\n",
    "        API_KEY = key['key'] \n",
    "        \n",
    "%matplotlib inline\n",
    "%run ../src/utils/slope.py\n",
    "%run ../src/utils/utils-bilinear.py\n",
    "%run ../src/utils/download_utils.py\n",
    "%run ../src/dsen2/utils/DSen2Net.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "SUPER_RESOLVE = True\n",
    "YEAR = 2019\n",
    "TIME = ('{}-12-15'.format(str(YEAR - 1)), '{}-01-15'.format(str(YEAR + 1)))\n",
    "EPSG = CRS.WGS84\n",
    "IMSIZE = 48\n",
    "CLOUD_DETECTOR = S2PixelCloudDetector(threshold=0.4, average_over=4, dilation_size=2)\n",
    "DATA_LOCATION = '../data/ghana-test.csv'\n",
    "OUTPUT_FOLDER = '../data/test-smooth-200/'\n",
    "\n",
    "# For DSen2 superresolve\n",
    "MDL_PATH = \"../src/dsen2/models/\"\n",
    "INPUT_SHAPE = ((4, None, None), (6, None, None))\n",
    "MODEL = s2model(INPUT_SHAPE, num_layers=6, feature_size=128)\n",
    "PREDICT_FILE = MDL_PATH+'s2_032_lr_1e-04.hdf5'\n",
    "MODEL.load_weights(PREDICT_FILE)\n",
    "\n",
    "# Constants\n",
    "starting_days = np.cumsum([0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30])\n",
    "c_arr = np.array([[1, 1, 1, 1, 1,],\n",
    "                  [1, 2, 2, 2, 1,],\n",
    "                  [1, 2, 3, 2, 1,],\n",
    "                  [1, 2, 2, 2, 1,],\n",
    "                  [1, 1, 1, 1, 1,],])\n",
    "                  \n",
    "c_arr = c_arr / 3\n",
    "o_arr = 1 - c_arr\n",
    "c_arr = np.tile(c_arr[:, :, np.newaxis], (1, 1, 11))\n",
    "o_arr = np.tile(o_arr[:, :, np.newaxis], (1, 1, 11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_proximal_steps_index(date, satisfactory):\n",
    "    \"\"\"Returns proximal steps that are cloud and shadow free\n",
    "\n",
    "         Parameters:\n",
    "          date (int): current time step\n",
    "          satisfactory (list): time steps with no clouds or shadows\n",
    "\n",
    "         Returns:\n",
    "          arg_before (str): index of the prior clean image\n",
    "          arg_after (int): index of the next clean image\n",
    "    \"\"\"\n",
    "    arg_before, arg_after = None, None\n",
    "    if date > 0:\n",
    "        idx_before = satisfactory - date\n",
    "        arg_before = idx_before[np.where(idx_before < 0, idx_before, -np.inf).argmax()]\n",
    "    if date < np.max(satisfactory):\n",
    "        idx_after = satisfactory - date\n",
    "        arg_after = idx_after[np.where(idx_after > 0, idx_after, np.inf).argmin()]\n",
    "    if not arg_after and not arg_before:\n",
    "        arg_after = date\n",
    "        arg_before = date\n",
    "    if not arg_after:\n",
    "        arg_after = arg_before\n",
    "    if not arg_before:\n",
    "        arg_before = arg_after\n",
    "    return arg_before, arg_after\n",
    "\n",
    "def speyediff(N, d, format = 'csc'):\n",
    "    \"\"\"Calculates the d-th order sparse difference matrix based on \n",
    "       an initial N x N identity matrix\n",
    "\n",
    "         Parameters:\n",
    "          N (int): input length\n",
    "          d (int): smoothing order\n",
    "\n",
    "         Returns:\n",
    "          spmat (arr): sparse difference matrix\n",
    "    \"\"\"\n",
    "    shape = (N-d, N)\n",
    "    diagonals = np.zeros(2*d + 1)\n",
    "    diagonals[d] = 1.\n",
    "    for i in range(d):\n",
    "        diff = diagonals[:-1] - diagonals[1:]\n",
    "        diagonals = diff\n",
    "    offsets = np.arange(d+1)\n",
    "    spmat = sparse.diags(diagonals, offsets, shape, format = format)\n",
    "    return spmat\n",
    "\n",
    "def smooth(y, lmbd, d = 2):\n",
    "    \"\"\"Calculates the whittaker smoother on input array\n",
    "\n",
    "         Parameters:\n",
    "          y (arr): 1-dimensional input array\n",
    "          lmbd (int): degree of smoothing, higher is more\n",
    "\n",
    "         Returns:\n",
    "          z (arr): smoothed version of y\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    E = sparse.eye(m, format = 'csc')\n",
    "    D = speyediff(m, d, format = 'csc')\n",
    "    coefmat = E + lmbd * D.conj().T.dot(D)\n",
    "    z = splu(coefmat).solve(y)\n",
    "    return z\n",
    "\n",
    "def DSen2(d10, d20):\n",
    "    \"\"\"Super resolves 20 meter bans using the DSen2 convolutional\n",
    "       neural network, as specified in Lanaras et al. 2018\n",
    "       https://github.com/lanha/DSen2\n",
    "\n",
    "        Parameters:\n",
    "         d10 (arr): (4, X, Y) shape array with 10 meter resolution\n",
    "         d20 (arr): (6, X, Y) shape array with 20 meter resolution\n",
    "\n",
    "        Returns:\n",
    "         prediction (arr): (6, X, Y) shape array with 10 meter superresolved\n",
    "                          output of DSen2 on d20 array\n",
    "    \"\"\"\n",
    "    test = [d10, d20]\n",
    "    input_shape = ((4, None, None), (6, None, None))\n",
    "    prediction = _predict(test, input_shape, deep=False)\n",
    "    return prediction\n",
    "\n",
    "def _predict(test, input_shape, model = MODEL, deep=False, run_60=False):\n",
    "    \n",
    "    prediction = model.predict(test, verbose=1)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertCoords(xy, src='', targ=''):\n",
    "    \"\"\" Converts coords from one EPSG to another\n",
    "\n",
    "        Parameters:\n",
    "         xy (tuple): input longitiude, latitude tuple\n",
    "         src (str): EPSG code associated with xy\n",
    "         targ (str): EPSG code of target output\n",
    "    \n",
    "        Returns:\n",
    "         pt (tuple): (x, y) tuple of xy in targ EPSG\n",
    "    \"\"\"\n",
    "\n",
    "    srcproj = osr.SpatialReference()\n",
    "    srcproj.ImportFromEPSG(src)\n",
    "    targproj = osr.SpatialReference()\n",
    "    if isinstance(targ, str):\n",
    "        targproj.ImportFromProj4(targ)\n",
    "    else:\n",
    "        targproj.ImportFromEPSG(targ)\n",
    "    transform = osr.CoordinateTransformation(srcproj, targproj)\n",
    "\n",
    "    pt = ogr.Geometry(ogr.wkbPoint)\n",
    "    pt.AddPoint(xy[0], xy[1])\n",
    "    pt.Transform(transform)\n",
    "    return([pt.GetX(), pt.GetY()])\n",
    "\n",
    "def calculate_epsg(points):\n",
    "    \"\"\" Calculates the UTM EPSG of an input WGS 84 lon, lat\n",
    "\n",
    "        Parameters:\n",
    "         points (tuple): input longitiude, latitude tuple\n",
    "    \n",
    "        Returns:\n",
    "         epsg_code (int): integer form of associated UTM EPSG\n",
    "    \"\"\"\n",
    "    lon, lat = points[0], points[1]\n",
    "\n",
    "    utm_band = str((math.floor((lon + 180) / 6 ) % 60) + 1)\n",
    "    if len(utm_band) == 1:\n",
    "        utm_band = '0'+utm_band\n",
    "    if lat >= 0:\n",
    "        epsg_code = '326' + utm_band\n",
    "    else:\n",
    "        epsg_code = '327' + utm_band\n",
    "    return int(epsg_code)\n",
    "    \n",
    "\n",
    "def calc_bbox(plot_id, df):\n",
    "    \"\"\" Calculates the corners of a bounding box from an input\n",
    "        pandas dataframe as output by Collect Earth Online\n",
    "\n",
    "        Parameters:\n",
    "         plot_id (int): plot_id of associated plot\n",
    "         df (pandas.DataFrame): dataframe of associated CEO survey\n",
    "    \n",
    "        Returns:\n",
    "         bounding_box (list): [(min(x), min(y)),\n",
    "                              (max(x), max_y))]\n",
    "    \"\"\"\n",
    "    subs = df[df['PLOT_ID'] == plot_id]\n",
    "    # TOP, LEFT, BOTTOM, RIGHT\n",
    "    # (min x, min y), (max x, max y)\n",
    "    return [(min(subs['LON']), min(subs['LAT'])),\n",
    "            (max(subs['LON']), max(subs['LAT']))]\n",
    "\n",
    "def bounding_box(points, expansion = 160):\n",
    "    \"\"\" Calculates the corners of a bounding box with an\n",
    "        input expansion in meters from a given bounding_box\n",
    "        \n",
    "        Subcalls:\n",
    "         calculate_epsg, convertCoords\n",
    "\n",
    "        Parameters:\n",
    "         points (list): output of calc_bbox\n",
    "         expansion (float): number of meters to expand or shrink the\n",
    "                            points edges to be\n",
    "    \n",
    "        Returns:\n",
    "         bl (tuple): x, y of bottom left corner with edges of expansion meters\n",
    "         tr (tuple): x, y of top right corner with edges of expansion meters\n",
    "    \"\"\"\n",
    "    # This must be lon, lat\n",
    "    bl = list(points[0])\n",
    "    tr = list(points[1])\n",
    "    \n",
    "    epsg = calculate_epsg(bl)\n",
    "    bl = convertCoords(bl, 4326, epsg)\n",
    "    tr = convertCoords(tr, 4326, epsg)\n",
    "    init = [b - a for a,b in zip(bl, tr)]\n",
    "    distance1 = tr[0] - bl[0]\n",
    "    distance2 = tr[1] - bl[1]\n",
    "    expansion1 = (expansion - distance1)/2\n",
    "    expansion2 = (expansion - distance2)/2\n",
    "    bl = [bl[0] - expansion1, bl[1] - expansion2]\n",
    "    tr = [tr[0] + expansion1, tr[1] + expansion2]\n",
    "\n",
    "    after = [b - a for a,b in zip(bl, tr)]   \n",
    "    diffs = [b - a for b, a in zip(after, init)]\n",
    "\n",
    "    bl = convertCoords(bl, epsg, 4326)\n",
    "    tr = convertCoords(tr, epsg, 4326)\n",
    "    return bl, tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_array(arr):\n",
    "    order = arr.argsort()\n",
    "    ranks = order.argsort()\n",
    "    return ranks\n",
    "\n",
    "def mcm_shadow_mask(arr, c_probs):\n",
    "    \"\"\" Calculates the multitemporal shadow mask for Sentinel-2 using\n",
    "        the methods from Candra et al. 2020 on L1C images and matching\n",
    "        outputs to the s2cloudless cloud probabilities\n",
    "\n",
    "        Parameters:\n",
    "         arr (arr): (Time, X, Y, Band) array of L1C data scaled from [0, 1]\n",
    "         c_probs (arr): (Time, X, Y) array of S2cloudless cloud probabilities\n",
    "    \n",
    "        Returns:\n",
    "         shadows_new (arr): cloud mask after Candra et al. 2020 and cloud matching \n",
    "         shadows_original (arr): cloud mask after Candra et al. 2020\n",
    "    \"\"\"\n",
    "    mean_c_probs = np.mean(c_probs, axis = (1, 2))\n",
    "    cloudy_steps = np.argwhere(mean_c_probs > 0.25)\n",
    "    images_clean = np.delete(arr, cloudy_steps, 0)\n",
    "    cloud_ranks = rank_array(mean_c_probs)\n",
    "    diffs = abs(np.sum(arr - np.mean(images_clean, axis = 0), axis = (1, 2, 3)))\n",
    "    diff_ranks = rank_array(diffs)\n",
    "    overall_rank = diff_ranks + cloud_ranks\n",
    "    reference_idx = np.argmin(overall_rank)\n",
    "    ri = arr[reference_idx]\n",
    "    print(\"The shadow reference index is: {}\".format(reference_idx))\n",
    "    \n",
    "    shadows = np.zeros((arr.shape[0], 96, 96))    \n",
    "    # Candra et al. 2020\n",
    "    \n",
    "    for time in range(arr.shape[0]):\n",
    "        for x in range(arr.shape[1]):\n",
    "            for y in range(arr.shape[2]):\n",
    "                ti_slice = arr[time, x, y]\n",
    "                ri_slice = ri[x, y]\n",
    "                deltab2 = ti_slice[0] - ri_slice[0]\n",
    "                #deltab3 = ti_slice[2] - ri_slice[2]\n",
    "                #deltab4 = ti_slice[3] - ri_slice[3]\n",
    "                deltab8a = ti_slice[1] - ri_slice[1]\n",
    "                deltab11 = ti_slice[2] - ri_slice[2]\n",
    "\n",
    "                if deltab2 <0.1: #(1000/65535):\n",
    "                    #if deltab3 < (800/65535)\n",
    "                        #if deltab4 < (800/65535)\n",
    "                    if deltab8a < -0.04: # (-400/65535):\n",
    "                        if deltab11 < -0.04: ##(-400/65535):\n",
    "                            if ti_slice[0] < 0.095: #(950/65535):\n",
    "                                shadows[time, x, y] = 1.\n",
    "                                        \n",
    "    shadows_original = np.copy(shadows)\n",
    "    # Remove shadows if cannot coreference a cloud\n",
    "    print(shadows.shape)\n",
    "    shadow_large = np.reshape(shadows, (shadows.shape[0], 96//8, 8, 96//8, 8))\n",
    "    shadow_large = np.sum(shadow_large, axis = (2, 4))\n",
    "    \n",
    "    cloud_large = np.copy(c_probs)\n",
    "    cloud_large[np.where(c_probs > 0.33)] = 1.\n",
    "    cloud_large[np.where(c_probs < 0.33)] = 0.\n",
    "    cloud_large = np.reshape(cloud_large, (shadows.shape[0], 96//8, 8, 96//8, 8))\n",
    "    cloud_large = np.sum(cloud_large, axis = (2, 4))\n",
    "    for time in range(shadow_large.shape[0]):\n",
    "        for x in range(shadow_large.shape[1]):\n",
    "            x_low = np.max([x - 8, 0])\n",
    "            x_high = np.min([x + 8, shadow_large.shape[1] - 1])\n",
    "            for y in range(shadow_large.shape[2]):\n",
    "                y_low = np.max([y - 8, 0])\n",
    "                y_high = np.min([y + 8, shadow_large.shape[1] - 1])\n",
    "                if shadow_large[time, x, y] < 8:\n",
    "                    shadow_large[time, x, y] = 0.\n",
    "                if shadow_large[time, x, y] >= 8:\n",
    "                    shadow_large[time, x, y] = 1.\n",
    "                c_prob_window = cloud_large[time, x_low:x_high, y_low:y_high]\n",
    "                if np.max(c_prob_window) < 24:\n",
    "                    shadow_large[time, x, y] = 0.\n",
    "                    \n",
    "    \n",
    "    shadow_large = resize(shadow_large, (shadow_large.shape[0], 96, 96), order = 0)\n",
    "    shadows *= shadow_large\n",
    "    \n",
    "    # Go through and aggregate the shadow map to an 80m grid, and extend it one grid size around\n",
    "    # any positive ID\n",
    "    \n",
    "    \n",
    "    shadows = np.reshape(shadows, (shadows.shape[0], 96//8, 8, 96//8, 8))\n",
    "    shadows = np.sum(shadows, axis = (2, 4))\n",
    "    shadows[np.where(shadows < 16)] = 0.\n",
    "    shadows[np.where(shadows >= 16)] = 1.\n",
    "    shadows = resize(shadows, (shadows.shape[0], 96, 96), order = 0)\n",
    "    shadows = np.reshape(shadows, (shadows.shape[0], 96//4, 4, 96//4, 4))\n",
    "    shadows = np.max(shadows, (2, 4))\n",
    "    \n",
    "    shadows_new = np.zeros_like(shadows)\n",
    "    for time in range(shadows.shape[0]):\n",
    "        for x in range(shadows.shape[1]):\n",
    "            for y in range(shadows.shape[2]):\n",
    "                if shadows[time, x, y] == 1:\n",
    "                    min_x = np.max([x - 1, 0])\n",
    "                    max_x = np.min([x + 2, shadows.shape[1] - 1])\n",
    "                    min_y = np.max([y - 1, 0])\n",
    "                    max_y = np.min([y + 2, shadows.shape[1] - 1])\n",
    "                    for x_idx in range(min_x, max_x):\n",
    "                        for y_idx in range(min_y, max_y):\n",
    "                            shadows_new[time, x_idx, y_idx] = 1.\n",
    "    shadows_new = resize(shadows_new, (shadows.shape[0], 96, 96), order = 0)\n",
    "    print(\"The shadow probability is: {}\".format(100*np.sum(shadows_new)/(96*96*shadows_new.shape[0])))\n",
    "    return np.array(shadows_new)\n",
    "\n",
    "def identify_clouds(bbox, epsg = EPSG, time = TIME, cloud_detector = CLOUD_DETECTOR,\n",
    "                    image_format =  MimeType.TIFF_d16):\n",
    "    \"\"\" Calculates the cloud probabilities of L1C Sentinel-2 data with\n",
    "        the S2Cloudless light GBM\n",
    "\n",
    "        Parameters:\n",
    "         bbox (list): output of calc_bbox\n",
    "         epsg (float): number \n",
    "         time (float): number \n",
    "         cloud_detector (obj): \n",
    "    \n",
    "        Returns:\n",
    "         cloud_probs (arr)\n",
    "         shadows (arr)\n",
    "         shadow_original (arr)\n",
    "         cloud_probs (arr)\n",
    "    \"\"\"\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    cloud_request = WmsRequest(\n",
    "        layer='CLOUD_DETECTION',\n",
    "        bbox=box,\n",
    "        time=time,\n",
    "        width=96,\n",
    "        height=96,\n",
    "        image_format = image_format,\n",
    "        maxcc=0.75,\n",
    "        instance_id=API_KEY,\n",
    "        custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "        time_difference=datetime.timedelta(hours=48),\n",
    "    )\n",
    "\n",
    "    cloud_img = cloud_request.get_data()\n",
    "    print(\"Bef\", np.max(cloud_img))\n",
    "    if image_format == MimeType.TIFF_d16:\n",
    "        cloud_img = np.array(cloud_img) / 65535\n",
    "    print(\"Aft cl\", np.max(cloud_img))\n",
    "    cloud_probs = cloud_detector.get_cloud_probability_maps(np.array(cloud_img))\n",
    "    shadows, shadow_original = mcm_shadow_mask(np.array(cloud_img), cloud_probs)\n",
    "    print(shadows.shape)\n",
    "    print(type(shadows))\n",
    "    shadows = np.array(shadows)[:, 24:-24, 24:-24]\n",
    "    return cloud_probs[:, 24:-24, 24:-24], shadows, shadow_original, cloud_probs\n",
    "    #except Exception as e:\n",
    "    #    logging.fatal(e, exc_info=True)\n",
    "    \n",
    "def identify_clouds_new(bbox, epsg = EPSG, time = TIME):\n",
    "\n",
    "    for try_ in range(0, 5):\n",
    "        try:\n",
    "            box = BBox(bbox, crs = epsg)\n",
    "            cloud_request = WmsRequest(\n",
    "                layer='CLOUD_NEW',\n",
    "                bbox=box,\n",
    "                time=time,\n",
    "                width=96,\n",
    "                height=96,\n",
    "                image_format =  MimeType.TIFF_d32f,\n",
    "                maxcc=0.75,\n",
    "                instance_id=API_KEY,\n",
    "                custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "                time_difference=datetime.timedelta(hours=48))\n",
    "            \n",
    "            shadow_request = WmsRequest(\n",
    "                layer='SHADOW',\n",
    "                bbox=box,\n",
    "                time=time,\n",
    "                width=96,\n",
    "                height=96,\n",
    "                image_format =  MimeType.TIFF_d16,\n",
    "                maxcc=0.75,\n",
    "                instance_id=API_KEY,\n",
    "                custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "                time_difference=datetime.timedelta(hours=48))\n",
    "\n",
    "            cloud_img = cloud_request.get_data()\n",
    "            cloud_img = np.array(cloud_img)\n",
    "            cloud_probs = cloud_img / 255\n",
    "            \n",
    "            shadow_img = shadow_request.get_data()\n",
    "            shadow_img = np.array(shadow_img)\n",
    "            print(np.max(shadow_img))\n",
    "            shadow_img = shadow_img / 65535\n",
    "            print(np.max(shadow_img))\n",
    "            \n",
    "            shadows = mcm_shadow_mask(np.array(shadow_img), cloud_probs)\n",
    "            shadows = shadows[:, 24:-24, 24:-24]\n",
    "            return cloud_probs[:, 24:-24, 24:-24], shadows#, shadow_original, cloud_probs\n",
    "        except Exception as e:\n",
    "            logging.fatal(e, exc_info=True)\n",
    "    \n",
    "    \n",
    "def download_dem(plot_id, df, epsg = EPSG, image_format = MimeType.TIFF_d32f):\n",
    "    #! TODO: ensure that centroid vs. bbox is correctly distinguished\n",
    "    \"\"\" Downloads MapZen digital elevation model and return slope\n",
    "\n",
    "        Parameters:\n",
    "         plot_id (tuple): plot id from collect earth online (CEO)\n",
    "         df (pandas.DataFrame): data associated with plot_id from CEO\n",
    "         epsg (int): UTM EPSG associated with plot_id\n",
    "    \n",
    "        Returns:\n",
    "         slope (arr): (X, Y, 1) array of per-pixel slope from [0, 1]\n",
    "    \"\"\"\n",
    "    location = calc_bbox(plot_id, df = df)\n",
    "    bbox = bounding_box(location, expansion = (IMSIZE+2)*10)\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    dem_request = WmsRequest(data_source=DataSource.DEM,\n",
    "                         layer='DEM',\n",
    "                         bbox=box,\n",
    "                         width=IMSIZE+2,\n",
    "                         height=IMSIZE+2,\n",
    "                         instance_id=API_KEY,\n",
    "                         image_format= image_format,\n",
    "                         custom_url_params={CustomUrlParam.SHOWLOGO: False})\n",
    "    dem_image = dem_request.get_data()[0]\n",
    "    slope = calcSlope(dem_image.reshape((1, IMSIZE+2, IMSIZE+2)),\n",
    "                      np.full((IMSIZE+2, IMSIZE+2), 10),\n",
    "                      np.full((IMSIZE+2, IMSIZE+2), 10), \n",
    "                      zScale = 1, minSlope = 0.02)\n",
    "    slope = slope.reshape((IMSIZE+2, IMSIZE+2, 1))\n",
    "    slope = slope[1:IMSIZE+1, 1:IMSIZE+1, :]\n",
    "    print(slope.shape)\n",
    "    return slope\n",
    "\n",
    "def check_zenith(bbox, epsg = EPSG, time = TIME):\n",
    "    \"\"\" Downloads the zenith layer from sentinel-hub so that\n",
    "        if desired, the data can be subsetted by zenith angle\n",
    "        \n",
    "        Parameters:\n",
    "         bbox (list): output of calc_bbox\n",
    "         epsg (float): number \n",
    "         time (float): number \n",
    "    \n",
    "        Returns:\n",
    "         zenith (arr): (Time, X, Y) array of per-pixel zenith from [0, 1] \n",
    "    \"\"\"\n",
    "    try:\n",
    "        box = BBox(bbox, crs = epsg)\n",
    "        zenith = WmsRequest(\n",
    "            layer='ZENITH',\n",
    "            bbox=box,\n",
    "            time=time,\n",
    "            width=IMSIZE,\n",
    "            height=IMSIZE,\n",
    "            image_format = MimeType.TIFF_d16,\n",
    "            maxcc=0.75,\n",
    "            instance_id=API_KEY,\n",
    "            custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "            time_difference=datetime.timedelta(hours=48),\n",
    "        )\n",
    "        \n",
    "        zenith = zenith.get_data()\n",
    "        return zenith\n",
    "    except Exception as e:\n",
    "        logging.fatal(e, exc_info=True)\n",
    "        \n",
    "def download_layer(bbox, epsg = EPSG, time = TIME, image_format = MimeType.TIFF_d16):\n",
    "    \"\"\" Downloads the L2A sentinel layer with 10 and 20 meter bands\n",
    "        \n",
    "        Parameters:\n",
    "         bbox (list): output of calc_bbox\n",
    "         epsg (float): EPSG associated with bbox \n",
    "         time (tuple): YY-MM-DD - YY-MM-DD bounds for downloading \n",
    "    \n",
    "        Returns:\n",
    "         img (arr):\n",
    "         img_request (obj): \n",
    "    \"\"\"\n",
    "    try:\n",
    "        box = BBox(bbox, crs = epsg)\n",
    "        image_request = WcsRequest(\n",
    "                layer='L2A20',\n",
    "                bbox=box,\n",
    "                time=time,\n",
    "                image_format = image_format,\n",
    "                maxcc=0.75,\n",
    "                resx='10m', resy='10m',\n",
    "                instance_id=API_KEY,\n",
    "                custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'NEAREST',\n",
    "                                    constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "                time_difference=datetime.timedelta(hours=48),\n",
    "            )\n",
    "        img_bands = image_request.get_data()\n",
    "        img_20 = np.stack(img_bands)\n",
    "        img_20 = resize(img_20, (img_20.shape[0], IMSIZE, IMSIZE, img_20.shape[-1]), order = 0)\n",
    "        \n",
    "        image_request = WcsRequest(\n",
    "                layer='L2A10',\n",
    "                bbox=box,\n",
    "                time=time,\n",
    "                image_format = image_format,\n",
    "                maxcc=0.75,\n",
    "                resx='10m', resy='10m',\n",
    "                instance_id=API_KEY,\n",
    "                custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'BICUBIC',\n",
    "                                    constants.CustomUrlParam.UPSAMPLING: 'BICUBIC'},\n",
    "                time_difference=datetime.timedelta(hours=48),\n",
    "        )\n",
    "        \n",
    "        img_bands = image_request.get_data()\n",
    "        img_10 = np.stack(img_bands)\n",
    "        print(\"The original L2A image size is: {}\".format(img_10.shape))\n",
    "        img_10 = resize(img_10, (img_10.shape[0], IMSIZE, IMSIZE, img_10.shape[-1]), order = 0)\n",
    "        img = np.concatenate([img_10, img_20], axis = -1)\n",
    "        print(np.max(img))\n",
    "        #if image_format == MimeType.TIFF_d16:\n",
    "        #    img = np.array(img) / 65535\n",
    "        return img, image_request\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.fatal(e, exc_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud and shadow removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_cloud_and_shadows(tiles, probs, shadows, image_dates, wsize = 5):\n",
    "    \"\"\" Interpolates clouds and shadows for each time step with \n",
    "        linear combination of proximal clean time steps for each\n",
    "        region of specified window size\n",
    "        \n",
    "        Parameters:\n",
    "         tiles (arr):\n",
    "         probs (arr): \n",
    "         shadows (arr):\n",
    "         image_dates (list):\n",
    "         wsize (int): \n",
    "    \n",
    "        Returns:\n",
    "         tiles (arr): \n",
    "    \"\"\"\n",
    "    c_probs = np.copy(probs)\n",
    "    c_probs = c_probs - np.min(c_probs, axis = 0)\n",
    "    c_probs[np.where(c_probs > 0.33)] = 1.\n",
    "    c_probs[np.where(c_probs < 0.33)] = 0.\n",
    "    c_probs = np.reshape(c_probs, [c_probs.shape[0], int(IMSIZE/8), 8, int(IMSIZE/8), 8])\n",
    "    c_probs = np.sum(c_probs, (2, 4))\n",
    "    c_probs = resize(c_probs, (c_probs.shape[0], IMSIZE, IMSIZE), 0)\n",
    "    c_probs[np.where(c_probs < 12)] = 0.\n",
    "    c_probs[np.where(c_probs >= 12)] = 1.\n",
    "    c_probs += shadows\n",
    "    c_probs[np.where(c_probs >= 1.)] = 1.\n",
    "    n_interp = 0\n",
    "    for cval in range(0, IMSIZE - 5, 1):\n",
    "        for rval in range(0, IMSIZE - 5, 1):\n",
    "            subs = c_probs[:, cval:cval + wsize, rval:rval+wsize]\n",
    "            satisfactory = [x for x in range(c_probs.shape[0]) if np.sum(subs[x, :, :]) < 10]\n",
    "            satisfactory = np.array(satisfactory)\n",
    "            for date in range(0, tiles.shape[0]):\n",
    "                if np.sum(subs[date, :, :]) > 10:\n",
    "                    n_interp += 1\n",
    "                    before, after = calculate_proximal_steps_index(date, satisfactory)\n",
    "                    before = date + before\n",
    "                    after = date + after\n",
    "                    if after >= tiles.shape[0]:\n",
    "                        after = before\n",
    "                    if before < 0:\n",
    "                        before = after\n",
    "                    bef = tiles[before, cval:cval+wsize, rval:rval+wsize, : ]\n",
    "                    aft = tiles[after, cval:cval+wsize, rval:rval+wsize, : ]\n",
    "                    before = image_dates[before]\n",
    "                    after = image_dates[after]\n",
    "                    before_diff = abs(image_dates[date] - before)\n",
    "                    after_diff = abs(image_dates[date] - after)\n",
    "                    bef_wt = 1 - before_diff / (before_diff + after_diff)\n",
    "                    aft_wt = 1 - bef_wt\n",
    "                    candidate = bef_wt*bef + aft_wt*aft\n",
    "                    candidate = candidate*c_arr + tiles[date, cval:cval+wsize, rval:rval+wsize, : ]*o_arr\n",
    "                    tiles[date, cval:cval+wsize, rval:rval+wsize, : ] = candidate  \n",
    "    print(\"Interpolated {} px\".format(n_interp))\n",
    "    return tiles\n",
    "\n",
    "def remove_missed_clouds(img):\n",
    "    \"\"\" Removes steps that are likely to be missed cloud or shadows\n",
    "        based on two interquartile ranges for the near infrared band\n",
    "        \n",
    "        Parameters:\n",
    "         img (arr):\n",
    "\n",
    "        Returns:\n",
    "         to_remove (list): \n",
    "    \"\"\"\n",
    "    iqr = np.percentile(img[:, :, :, 3].flatten(), 75) - np.percentile(img[:, :, :, 3].flatten(), 25)\n",
    "    thresh_t = np.percentile(img[:, :, :, 3].flatten(), 75) + iqr*2\n",
    "    thresh_b = np.percentile(img[:, :, :, 3].flatten(), 25) - iqr*2\n",
    "    diffs_fw = np.diff(img, 1, axis = 0)\n",
    "    diffs_fw = np.mean(diffs_fw, axis = (1, 2, 3))\n",
    "    diffs_fw = np.array([0] + list(diffs_fw))\n",
    "    diffs_bw = np.diff(np.flip(img, 0), 1, axis = 0)\n",
    "    diffs_bw = np.flip(np.mean(diffs_bw, axis = (1, 2, 3)))\n",
    "    diffs_bw = np.array(list(diffs_bw) + [0])\n",
    "    diffs = abs(diffs_fw - diffs_bw) * 100 # 3, -3 -> 6, -3, 3 -> 6, -3, -3\n",
    "    #diffs = [int(x) for x in diffs]\n",
    "    outlier_percs = []\n",
    "    for step in range(img.shape[0]):\n",
    "        bottom = len(np.argwhere(img[step, :, :, 3].flatten() > thresh_t))\n",
    "        top = len(np.argwhere(img[step, :, :, 3].flatten() < thresh_b))\n",
    "        p = 100* ((bottom + top) / (IMSIZE*IMSIZE))\n",
    "        outlier_percs.append(p)\n",
    "    to_remove = np.argwhere(np.array(outlier_percs) > 15)\n",
    "    return to_remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def download_plots(data_location = DATA_LOCATION, output_folder = OUTPUT_FOLDER, image_format = MimeType.TIFF_d16):\n",
    "    \"\"\" Downloads slope and sentinel-2 data for all plots associated\n",
    "        with an input CSV from a collect earth online survey\n",
    "        \n",
    "        Parameters:\n",
    "         data_location (os.path)\n",
    "         output_folder (os.path)\n",
    "        \n",
    "        Subcalls:\n",
    "         calc_bbox, bounding_box\n",
    "         identify_clouds, download_layer, check_zenith, download_dem\n",
    "         remove_clouds_and_shadows, remove_missed_clouds\n",
    "         DSen2\n",
    "         calculate_and_save_best_images\n",
    "         \n",
    "        Creates:\n",
    "         output_folder/{plot_id}.npy\n",
    "    \n",
    "        Returns:\n",
    "         None\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(data_location)\n",
    "    for column in ['IMAGERY_TITLE', 'STACKINGPROFILEDG', 'PL_PLOTID', 'IMAGERYYEARDG']:\n",
    "        if column in df.columns:\n",
    "            df = df.drop(column, axis = 1)\n",
    "    df = df.dropna(axis = 0)\n",
    "    plot_ids = sorted(df['PLOT_ID'].unique())\n",
    "    existing = [int(x[:-4]) for x in os.listdir(output_folder) if \".DS\" not in x]\n",
    "    to_download = [x for x in plot_ids if x not in existing]\n",
    "    print(\"STARTING DOWNLOAD OF {} plots from {} to {}\".format(len(to_download), data_location, output_folder))\n",
    "    errors = []\n",
    "    for i, val in enumerate(to_download):\n",
    "        print(\"Downloading {}/{}, {}\".format(i+1, len(to_download), val))\n",
    "        location = calc_bbox(val, df = df)\n",
    "        location = bounding_box(location, expansion = IMSIZE*10)\n",
    "        location_clouds = bounding_box(location, expansion = 96*10)\n",
    "        try:\n",
    "            # Identify cloud steps, download DEM, and download L2A series\n",
    "            probs, shadows = identify_clouds(location_clouds, image_format = image_format)\n",
    "            shadow_sums = np.sum(shadows, axis = (1, 2))\n",
    "            shadow_steps = np.argwhere(shadow_sums > (48*48/3))\n",
    "            dem = download_dem(val, df = df)\n",
    "            img, image_request = download_layer(location, image_format = image_format)\n",
    "            print(img[0, :, :, 0])\n",
    "            #np.save(\"../data/raw/train-raw/\" + str(val) + \".npy\", img)\n",
    "            #np.save(\"../data/raw/train-dates/\" + str(val) + \".npy\", image_request.get_dates())\n",
    "\n",
    "            # Calculate imagery dates\n",
    "            image_dates = []\n",
    "            for date in image_request.get_dates():\n",
    "                if date.year == YEAR - 1:\n",
    "                    image_dates.append(-365 + starting_days[(date.month-1)] + date.day)\n",
    "                if date.year == YEAR:\n",
    "                    image_dates.append(starting_days[(date.month-1)] + date.day)\n",
    "                if date.year == YEAR + 1:\n",
    "                    image_dates.append(365 + starting_days[(date.month-1)]+date.day)\n",
    "            image_dates = np.array(image_dates)\n",
    "\n",
    "            # Remove imagery where >4% is clouds, and where there is null data\n",
    "            args = np.array([len(np.argwhere(probs[x].flatten() > 0.3)) for x in range(probs.shape[0])])\n",
    "            dirty_steps = np.argwhere(args > (IMSIZE)*(IMSIZE) / 5)\n",
    "            missing_images = [np.argwhere(img[x, :, : :].flatten() == 0.0) for x in range(img.shape[0])]\n",
    "            missing_images = np.array([len(x) for x in missing_images])\n",
    "            missing_images_p = [np.argwhere(img[x, :, : :].flatten() >= 1) for x in range(img.shape[0])]\n",
    "            missing_images_p = np.array([len(x) for x in missing_images_p])\n",
    "            missing_images += missing_images_p\n",
    "            missing_images = list(np.argwhere(missing_images >= 25))\n",
    "            to_remove = np.unique(np.array(list(dirty_steps) + list(missing_images) + list(shadow_steps)))\n",
    "\n",
    "            # Remove null steps\n",
    "            print(\"There are {}/{} dirty steps: {}\"\n",
    "                  \" cloud, {} missing, {} shadow\".format(len(to_remove),\n",
    "                                                         len(img), len(dirty_steps),\n",
    "                                                         len(missing_images),\n",
    "                                                         #len(zenith_outliers),\n",
    "                                                         len(shadow_steps)))\n",
    "\n",
    "            img = np.delete(img, to_remove, 0)\n",
    "            probs = np.delete(probs, to_remove, 0)\n",
    "            image_dates = np.delete(image_dates, to_remove)\n",
    "            shadows = np.delete(shadows, to_remove, 0)\n",
    "\n",
    "            to_remove = remove_missed_clouds(img)\n",
    "            img = np.delete(img, to_remove, 0)\n",
    "            probs = np.delete(probs, to_remove, 0)\n",
    "            image_dates = np.delete(image_dates, to_remove)\n",
    "            shadows = np.delete(shadows, to_remove, 0)\n",
    "            print(\"Removing {} steps based on ratio\".format(len(to_remove)))\n",
    "\n",
    "\n",
    "            # Concatenate DEM\n",
    "            dem = np.tile(dem.reshape((1, IMSIZE, IMSIZE, 1)), (img.shape[0], 1, 1, 1))\n",
    "            tiles = np.concatenate([img, dem], axis = -1)\n",
    "            tiles[:, :, :, -1] /= 90\n",
    "\n",
    "            x = remove_cloud_and_shadows(tiles, probs, shadows, image_dates)\n",
    "            if SUPER_RESOLVE:\n",
    "                x = x[:, 8:40, 8:40, :]\n",
    "                print(\"Shape before super: {}\".format(x.shape))\n",
    "\n",
    "                d10 = x[:, :, :, 0:4]\n",
    "                d20 = x[:, :, :, 4:10]\n",
    "\n",
    "                d10 = np.swapaxes(d10, 1, -1)\n",
    "                d10 = np.swapaxes(d10, 2, 3)\n",
    "                d20 = np.swapaxes(d20, 1, -1)\n",
    "                d20 = np.swapaxes(d20, 2, 3)\n",
    "                superresolved = DSen2(d10, d20)\n",
    "                superresolved = np.swapaxes(superresolved, 1, -1)\n",
    "                superresolved = np.swapaxes(superresolved, 1, 2)\n",
    "                print(superresolved.shape)\n",
    "                print(x.shape)\n",
    "\n",
    "                # returns band IDXs 3, 4, 5, 7, 8, 9\n",
    "                x[:, :, :, 4:10] = superresolved\n",
    "                x = x[:, 8:24, 8:24, :]\n",
    "                print(\"Shape after super: {}\".format(x.shape))\n",
    "            else:\n",
    "                bottom = int(IMSIZE/2 - 8)\n",
    "                top = int(IMSIZE/2 + 8)\n",
    "                x = x[:, bottom:top, bottom:top, :]\n",
    "\n",
    "            # Calculate indices\n",
    "            tiles, amin = evi(x, True)\n",
    "            tiles = bi(tiles, True)\n",
    "            tiles = msavi2(tiles, True)\n",
    "            x = si(tiles, True)\n",
    "\n",
    "            print(\"Shape after vegetation indexes: {}\".format(x.shape))\n",
    "            \n",
    "            missing_pixels = 0\n",
    "            for band in range(0, 15):\n",
    "                for time in range(0, x.shape[0]):\n",
    "                    x_i = x[time, :, :, band]\n",
    "                    missing_pixels += len(np.argwhere(np.isnan(x_i)))\n",
    "                    x_i[np.argwhere(np.isnan(x_i))] = np.mean(x_i)\n",
    "                    x[time, :, :, band] = x_i\n",
    "            print(\"There are {} missing pixels\".format(missing_pixels))\n",
    "\n",
    "            # Interpolate linearly to 5 day frequency\n",
    "            tiles, max_distance = calculate_and_save_best_images(x, image_dates)\n",
    "\n",
    "            # Smooth linear interpolation\n",
    "            for row in range(0, 16):\n",
    "                for column in range(0, 16):\n",
    "                    for band in [x for x in range(0, 15) if x != 10]:\n",
    "                        sm = smooth(tiles[:, row, column, band], 800, d = 2)\n",
    "                        tiles[:, row, column, band] = sm\n",
    "\n",
    "            # Retain only iamgery every 15 days\n",
    "            biweekly_dates = np.array([day for day in range(0, 360, 5)])\n",
    "            to_remove = np.argwhere(biweekly_dates % 15 != 0)\n",
    "            tiles = np.delete(tiles, to_remove, 0)\n",
    "\n",
    "            if max_distance <= 240:\n",
    "                np.save(output_folder + str(val), tiles)\n",
    "                #return tiles\n",
    "                #np.save(\"../data/raw/train-clouds/\" + str(val) + \".npy\", probs)\n",
    "                #np.save(\"../data/raw/train-shadows/\" + str(val) + \".npy\", shadows)\n",
    "                print(\"Saved array of {} shape to {}\".format(tiles.shape, val))\n",
    "                print(\"\\n\")\n",
    "            else:\n",
    "                print(\"Skipping {} because there is a {} distance\".format(val, max_distance))\n",
    "                print(\"\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            logging.fatal(e, exc_info=True)\n",
    "            #errors.append(img)\n",
    "            #continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in os.listdir(\"../data/drylands/csv/\"):\n",
    "    if \"fao-test-2.csv\" in i:\n",
    "    #if \".csv\" in i:\n",
    "        #if any(x in i for x in [\"africa-west\", \"cameroon\", \"koure\", \"niger\"]):\n",
    "        tile = download_plots(\"../data/drylands/csv/\" + i, \"../data/drylands/s2/\", image_format = MimeType.TIFF_d16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_sensing",
   "language": "python",
   "name": "remote_sensing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
