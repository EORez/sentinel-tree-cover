{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and process sentinel 2 data\n",
    "\n",
    "## John Brandt\n",
    "## April 1, 2020\n",
    "\n",
    "## Package imports, API import, source scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import scipy.sparse as sparse\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "\n",
    "from collections import Counter\n",
    "from osgeo import ogr, osr\n",
    "from random import shuffle\n",
    "from scipy.sparse.linalg import splu\n",
    "from sentinelhub import WmsRequest, WcsRequest, MimeType\n",
    "from sentinelhub import CRS, BBox, constants, DataSource, CustomUrlParam\n",
    "from skimage.transform import resize\n",
    "\n",
    "with open(\"../config.yaml\", 'r') as stream:\n",
    "        key = (yaml.safe_load(stream))\n",
    "        API_KEY = key['key'] \n",
    "        \n",
    "%matplotlib inline\n",
    "%run ../src/preprocessing/slope.py\n",
    "%run ../src/preprocessing/indices.py\n",
    "%run ../src/downloading/utils.py\n",
    "%run ../src/preprocessing/cloud_removal.py\n",
    "%run ../src/preprocessing/whittaker_smoother.py\n",
    "%run ../src/dsen2/utils/DSen2Net.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "SUPER_RESOLVE = True\n",
    "YEAR = 2019\n",
    "TIME = ('{}-12-15'.format(str(YEAR - 1)), '{}-01-15'.format(str(YEAR + 1)))\n",
    "EPSG = CRS.WGS84\n",
    "IMSIZE = 48\n",
    "DATA_LOCATION = '../data/ghana-test.csv'\n",
    "OUTPUT_FOLDER = '../data/test-smooth-200/'\n",
    "\n",
    "# For DSen2 superresolve\n",
    "MDL_PATH = \"../src/dsen2/models/\"\n",
    "INPUT_SHAPE = ((4, None, None), (6, None, None))\n",
    "MODEL = s2model(INPUT_SHAPE, num_layers=6, feature_size=128)\n",
    "PREDICT_FILE = MDL_PATH+'s2_032_lr_1e-04.hdf5'\n",
    "MODEL.load_weights(PREDICT_FILE)\n",
    "\n",
    "# Constants\n",
    "starting_days = np.cumsum([0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30])\n",
    "c_arr = np.array([[1, 1, 1, 1, 1,],\n",
    "                  [1, 2, 2, 2, 1,],\n",
    "                  [1, 2, 3, 2, 1,],\n",
    "                  [1, 2, 2, 2, 1,],\n",
    "                  [1, 1, 1, 1, 1,],])\n",
    "                  \n",
    "c_arr = c_arr / 3\n",
    "o_arr = 1 - c_arr\n",
    "c_arr = np.tile(c_arr[:, :, np.newaxis], (1, 1, 11))\n",
    "o_arr = np.tile(o_arr[:, :, np.newaxis], (1, 1, 11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DSen2(d10, d20):\n",
    "    \"\"\"Super resolves 20 meter bans using the DSen2 convolutional\n",
    "       neural network, as specified in Lanaras et al. 2018\n",
    "       https://github.com/lanha/DSen2\n",
    "\n",
    "        Parameters:\n",
    "         d10 (arr): (4, X, Y) shape array with 10 meter resolution\n",
    "         d20 (arr): (6, X, Y) shape array with 20 meter resolution\n",
    "\n",
    "        Returns:\n",
    "         prediction (arr): (6, X, Y) shape array with 10 meter superresolved\n",
    "                          output of DSen2 on d20 array\n",
    "    \"\"\"\n",
    "    test = [d10, d20]\n",
    "    input_shape = ((4, None, None), (6, None, None))\n",
    "    prediction = _predict(test, input_shape, deep=False)\n",
    "    return prediction\n",
    "\n",
    "def _predict(test, input_shape, model = MODEL, deep=False, run_60=False):\n",
    "    \n",
    "    prediction = model.predict(test, verbose=1)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bbox(plot_id, df):\n",
    "    \"\"\" Calculates the corners of a bounding box from an input\n",
    "        pandas dataframe as output by Collect Earth Online\n",
    "\n",
    "        Parameters:\n",
    "         plot_id (int): plot_id of associated plot\n",
    "         df (pandas.DataFrame): dataframe of associated CEO survey\n",
    "    \n",
    "        Returns:\n",
    "         bounding_box (list): [(min(x), min(y)),\n",
    "                              (max(x), max_y))]\n",
    "    \"\"\"\n",
    "    subs = df[df['PLOT_ID'] == plot_id]\n",
    "    # TOP, LEFT, BOTTOM, RIGHT\n",
    "    # (min x, min y), (max x, max y)\n",
    "    return [(min(subs['LON']), min(subs['LAT'])),\n",
    "            (max(subs['LON']), max(subs['LAT']))]\n",
    "\n",
    "def bounding_box(points, expansion = 160):\n",
    "    \"\"\" Calculates the corners of a bounding box with an\n",
    "        input expansion in meters from a given bounding_box\n",
    "        \n",
    "        Subcalls:\n",
    "         calculate_epsg, convertCoords\n",
    "\n",
    "        Parameters:\n",
    "         points (list): output of calc_bbox\n",
    "         expansion (float): number of meters to expand or shrink the\n",
    "                            points edges to be\n",
    "    \n",
    "        Returns:\n",
    "         bl (tuple): x, y of bottom left corner with edges of expansion meters\n",
    "         tr (tuple): x, y of top right corner with edges of expansion meters\n",
    "    \"\"\"\n",
    "    bl = list(points[0])\n",
    "    tr = list(points[1])\n",
    "    inproj = Proj('epsg:4326')\n",
    "    outproj_code = calculate_epsg(bl)\n",
    "    outproj = Proj('epsg:' + str(outproj_code))\n",
    "    \n",
    "    bl_utm =  transform(inproj, outproj, bl[1], bl[0])\n",
    "    tr_utm =  transform(inproj, outproj, tr[1], tr[0])\n",
    "    #print(\"Before the expansion\")\n",
    "    #print((bl_utm, tr_utm))\n",
    "    \n",
    "    distance1 = tr_utm[0] - bl_utm[0]\n",
    "    distance2 = tr_utm[1] - bl_utm[1]\n",
    "    expansion1 = (expansion - distance1)/2\n",
    "    expansion2 = (expansion - distance2)/2\n",
    "        \n",
    "    bl_utm = [bl_utm[0] - expansion1, bl_utm[1] - expansion2]\n",
    "    tr_utm = [tr_utm[0] + expansion1, tr_utm[1] + expansion2]\n",
    "\n",
    "    assert (tr_utm[0] - bl_utm[0]) == expansion\n",
    "    assert (tr_utm[1] - bl_utm[1]) == expansion\n",
    "\n",
    "    \n",
    "    zone = str(outproj_code)[3:]\n",
    "    zone = zone[1:] if zone[0] == \"0\" else zone\n",
    "    direction = 'N' if tr[1] >= 0 else 'S'\n",
    "    utm_epsg = \"UTM_\" + zone + direction\n",
    "    return (bl_utm, tr_utm), CRS[utm_epsg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_clouds(bbox, epsg, time = TIME):\n",
    "\n",
    "    for try_ in range(0, 5):\n",
    "        try:\n",
    "            box = BBox(bbox, crs = epsg)\n",
    "            cloud_request = WmsRequest(\n",
    "                layer='CLOUD_NEW',\n",
    "                bbox=box,\n",
    "                time=time,\n",
    "                width=96,\n",
    "                height=96,\n",
    "                image_format =  MimeType.TIFF_d8,\n",
    "                maxcc=0.75,\n",
    "                instance_id=API_KEY,\n",
    "                custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "                time_difference=datetime.timedelta(hours=48))\n",
    "            \n",
    "            shadow_request = WmsRequest(\n",
    "                layer='SHADOW',\n",
    "                bbox=box,\n",
    "                time=time,\n",
    "                width=96,\n",
    "                height=96,\n",
    "                image_format =  MimeType.TIFF_d16,\n",
    "                maxcc=0.75,\n",
    "                instance_id=API_KEY,\n",
    "                custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "                time_difference=datetime.timedelta(hours=48))\n",
    "\n",
    "            cloud_img = cloud_request.get_data()\n",
    "            cloud_img = np.array(cloud_img)\n",
    "\n",
    "            if np.max(cloud_img > 10):\n",
    "                cloud_img = cloud_img / 255\n",
    "            assert np.max(cloud_img) <= 1.\n",
    "            print(\"Cloud_probs shape: {}\".format(cloud_img.shape))\n",
    "            \n",
    "            shadow_img = shadow_request.get_data()\n",
    "            shadow_img = np.array(shadow_img)\n",
    "            print(\"Shadows_shape: {}\".format(shadow_img.shape))\n",
    "\n",
    "            if np.max(shadow_img > 10):\n",
    "                shadow_img = shadow_img / 65535\n",
    "            print(np.max(shadow_img))\n",
    "            \n",
    "            shadows = mcm_shadow_mask(np.array(shadow_img), cloud_img)\n",
    "            shadows = shadows[:, 24:-24, 24:-24]\n",
    "            return cloud_img[:, 24:-24, 24:-24], shadows\n",
    "        except Exception as e:\n",
    "            logging.fatal(e, exc_info=True)\n",
    "    \n",
    "    \n",
    "def download_dem(plot_id, df, epsg, image_format = MimeType.TIFF_d32f):\n",
    "    #! TODO: ensure that centroid vs. bbox is correctly distinguished\n",
    "    \"\"\" Downloads MapZen digital elevation model and return slope\n",
    "\n",
    "        Parameters:\n",
    "         plot_id (tuple): plot id from collect earth online (CEO)\n",
    "         df (pandas.DataFrame): data associated with plot_id from CEO\n",
    "         epsg (int): UTM EPSG associated with plot_id\n",
    "    \n",
    "        Returns:\n",
    "         slope (arr): (X, Y, 1) array of per-pixel slope from [0, 1]\n",
    "    \"\"\"\n",
    "    location = calc_bbox(plot_id, df = df)\n",
    "    bbox, epsg = bounding_box(location, expansion = (IMSIZE+2)*10)\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    dem_request = WmsRequest(data_source=DataSource.DEM,\n",
    "                         layer='DEM',\n",
    "                         bbox=box,\n",
    "                         width=IMSIZE+2,\n",
    "                         height=IMSIZE+2,\n",
    "                         instance_id=API_KEY,\n",
    "                         image_format= image_format,\n",
    "                         custom_url_params={CustomUrlParam.SHOWLOGO: False})\n",
    "    dem_image = dem_request.get_data()[0]\n",
    "    slope = calcSlope(dem_image.reshape((1, IMSIZE+2, IMSIZE+2)),\n",
    "                      np.full((IMSIZE+2, IMSIZE+2), 10),\n",
    "                      np.full((IMSIZE+2, IMSIZE+2), 10), \n",
    "                      zScale = 1, minSlope = 0.02)\n",
    "    slope = slope.reshape((IMSIZE+2, IMSIZE+2, 1))\n",
    "    slope = slope[1:IMSIZE+1, 1:IMSIZE+1, :]\n",
    "    return slope\n",
    "\n",
    "        \n",
    "def download_layer(bbox, epsg, time = TIME, image_format = MimeType.TIFF_d16):\n",
    "    \"\"\" Downloads the L2A sentinel layer with 10 and 20 meter bands\n",
    "        \n",
    "        Parameters:\n",
    "         bbox (list): output of calc_bbox\n",
    "         epsg (float): EPSG associated with bbox \n",
    "         time (tuple): YY-MM-DD - YY-MM-DD bounds for downloading \n",
    "    \n",
    "        Returns:\n",
    "         img (arr):\n",
    "         img_request (obj): \n",
    "    \"\"\"\n",
    "    try:\n",
    "        box = BBox(bbox, crs = epsg)\n",
    "        image_request = WcsRequest(\n",
    "                layer='L2A20',\n",
    "                bbox=box,\n",
    "                time=time,\n",
    "                image_format = image_format,\n",
    "                maxcc=0.75,\n",
    "                resx='20m', resy='20m',\n",
    "                instance_id=API_KEY,\n",
    "                custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'NEAREST',\n",
    "                                    constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "                time_difference=datetime.timedelta(hours=48),\n",
    "            )\n",
    "        img_bands = image_request.get_data()\n",
    "        img_20 = np.stack(img_bands)\n",
    "\n",
    "        if np.max(img_20) >= 10:\n",
    "            img_20 = img_20 / 65535\n",
    "        assert np.max(img_20) <= 2.\n",
    "\n",
    "        s2_20_usage = (img_20.shape[1]*img_20.shape[2])/(512*512) * (6/3) * img_20.shape[0]\n",
    "        print(\"Original 20 meter bands size: {}, using {} PU\".format(img_20.shape, s2_20_usage))\n",
    "        img_20 = resize(img_20, (img_20.shape[0], IMSIZE, IMSIZE, img_20.shape[-1]), order = 0)\n",
    "        \n",
    "        image_request = WcsRequest(\n",
    "                layer='L2A10',\n",
    "                bbox=box,\n",
    "                time=time,\n",
    "                image_format = image_format,\n",
    "                maxcc=0.75,\n",
    "                resx='10m', resy='10m',\n",
    "                instance_id=API_KEY,\n",
    "                custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'BICUBIC',\n",
    "                                    constants.CustomUrlParam.UPSAMPLING: 'BICUBIC'},\n",
    "                time_difference=datetime.timedelta(hours=48),\n",
    "        )\n",
    "        \n",
    "        img_bands = image_request.get_data()\n",
    "        img_10 = np.stack(img_bands)\n",
    "        print(\"The original L2A image size is: {}\".format(img_10.shape))\n",
    "        img_10 = resize(img_10, (img_10.shape[0], IMSIZE, IMSIZE, img_10.shape[-1]), order = 0)\n",
    "        img = np.concatenate([img_10, img_20], axis = -1)\n",
    "\n",
    "        if np.max(img_10) >= 10:\n",
    "            img_10 = img_10 / 65535\n",
    "        assert np.max(img_10) <= 2.\n",
    "        return img, image_request\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.fatal(e, exc_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud and shadow removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_missed_clouds(img):\n",
    "    \"\"\" Removes steps that are likely to be missed cloud or shadows\n",
    "        based on two interquartile ranges for the near infrared band\n",
    "        \n",
    "        Parameters:\n",
    "         img (arr):\n",
    "\n",
    "        Returns:\n",
    "         to_remove (list): \n",
    "    \"\"\"\n",
    "    iqr = np.percentile(img[:, :, :, 3].flatten(), 75) - np.percentile(img[:, :, :, 3].flatten(), 25)\n",
    "    thresh_t = np.percentile(img[:, :, :, 3].flatten(), 75) + iqr*2\n",
    "    thresh_b = np.percentile(img[:, :, :, 3].flatten(), 25) - iqr*2\n",
    "    diffs_fw = np.diff(img, 1, axis = 0)\n",
    "    diffs_fw = np.mean(diffs_fw, axis = (1, 2, 3))\n",
    "    diffs_fw = np.array([0] + list(diffs_fw))\n",
    "    diffs_bw = np.diff(np.flip(img, 0), 1, axis = 0)\n",
    "    diffs_bw = np.flip(np.mean(diffs_bw, axis = (1, 2, 3)))\n",
    "    diffs_bw = np.array(list(diffs_bw) + [0])\n",
    "    diffs = abs(diffs_fw - diffs_bw) * 100 # 3, -3 -> 6, -3, 3 -> 6, -3, -3\n",
    "    #diffs = [int(x) for x in diffs]\n",
    "    outlier_percs = []\n",
    "    for step in range(img.shape[0]):\n",
    "        bottom = len(np.argwhere(img[step, :, :, 3].flatten() > thresh_t))\n",
    "        top = len(np.argwhere(img[step, :, :, 3].flatten() < thresh_b))\n",
    "        p = 100* ((bottom + top) / (IMSIZE*IMSIZE))\n",
    "        outlier_percs.append(p)\n",
    "    to_remove = np.argwhere(np.array(outlier_percs) > 15)\n",
    "    return to_remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def download_plots(data_location = DATA_LOCATION, output_folder = OUTPUT_FOLDER, image_format = MimeType.TIFF_d16):\n",
    "    \"\"\" Downloads slope and sentinel-2 data for all plots associated\n",
    "        with an input CSV from a collect earth online survey\n",
    "        \n",
    "        Parameters:\n",
    "         data_location (os.path)\n",
    "         output_folder (os.path)\n",
    "        \n",
    "        Subcalls:\n",
    "         calc_bbox, bounding_box\n",
    "         identify_clouds, download_layer, check_zenith, download_dem\n",
    "         remove_clouds_and_shadows, remove_missed_clouds\n",
    "         DSen2\n",
    "         calculate_and_save_best_images\n",
    "         \n",
    "        Creates:\n",
    "         output_folder/{plot_id}.npy\n",
    "    \n",
    "        Returns:\n",
    "         None\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(data_location)\n",
    "    for column in ['IMAGERY_TITLE', 'STACKINGPROFILEDG', 'PL_PLOTID', 'IMAGERYYEARDG']:\n",
    "        if column in df.columns:\n",
    "            df = df.drop(column, axis = 1)\n",
    "    df = df.dropna(axis = 0)\n",
    "    plot_ids = sorted(df['PLOT_ID'].unique())\n",
    "    existing = [int(x[:-4]) for x in os.listdir(output_folder) if \".DS\" not in x]\n",
    "    to_download = [x for x in plot_ids if x not in existing]\n",
    "    print(\"STARTING DOWNLOAD OF {} plots from {} to {}\".format(len(to_download), data_location, output_folder))\n",
    "    errors = []\n",
    "    for i, val in enumerate(to_download):\n",
    "        print(\"Downloading {}/{}, {}\".format(i+1, len(to_download), val))\n",
    "        initial_bbx = calc_bbox(val, df = df)\n",
    "        sentinel2_bbx, epsg = bounding_box(initial_bbx, expansion = IMSIZE*10)\n",
    "        cloud_bbx, _ = bounding_box(initial_bbx, expansion = 96*10)\n",
    "        try:\n",
    "            # Identify cloud steps, download DEM, and download L2A series\n",
    "            probs, shadows = identify_clouds(cloud_bbx, epsg = epsg)\n",
    "            shadow_sums = np.sum(shadows, axis = (1, 2))\n",
    "            shadow_steps = np.argwhere(shadow_sums > (48*48/3))\n",
    "            dem = download_dem(val, epsg = epsg, df = df)\n",
    "            img, image_request = download_layer(sentinel2_bbx, epsg = epsg, image_format = image_format)\n",
    "\n",
    "            #np.save(\"../data/raw/train-raw/\" + str(val) + \".npy\", img)\n",
    "            #np.save(\"../data/raw/train-dates/\" + str(val) + \".npy\", image_request.get_dates())\n",
    "\n",
    "            # Calculate imagery dates\n",
    "            image_dates = []\n",
    "            for date in image_request.get_dates():\n",
    "                if date.year == YEAR - 1:\n",
    "                    image_dates.append(-365 + starting_days[(date.month-1)] + date.day)\n",
    "                if date.year == YEAR:\n",
    "                    image_dates.append(starting_days[(date.month-1)] + date.day)\n",
    "                if date.year == YEAR + 1:\n",
    "                    image_dates.append(365 + starting_days[(date.month-1)]+date.day)\n",
    "            image_dates = np.array(image_dates)\n",
    "\n",
    "            # Remove imagery where >4% is clouds, and where there is null data\n",
    "            args = np.array([len(np.argwhere(probs[x].flatten() > 0.3)) for x in range(probs.shape[0])])\n",
    "            dirty_steps = np.argwhere(args > (IMSIZE)*(IMSIZE) / 5)\n",
    "            missing_images = [np.argwhere(img[x, :, : :].flatten() == 0.0) for x in range(img.shape[0])]\n",
    "            missing_images = np.array([len(x) for x in missing_images])\n",
    "            missing_images_p = [np.argwhere(img[x, :, : :].flatten() >= 1) for x in range(img.shape[0])]\n",
    "            missing_images_p = np.array([len(x) for x in missing_images_p])\n",
    "            missing_images += missing_images_p\n",
    "            missing_images = list(np.argwhere(missing_images >= 25))\n",
    "            to_remove = np.unique(np.array(list(dirty_steps) + list(missing_images) + list(shadow_steps)))\n",
    "\n",
    "            # Remove null steps\n",
    "            print(\"There are {}/{} dirty steps: {}\"\n",
    "                  \" cloud, {} missing, {} shadow\".format(len(to_remove),\n",
    "                                                         len(img), len(dirty_steps),\n",
    "                                                         len(missing_images),\n",
    "                                                         #len(zenith_outliers),\n",
    "                                                         len(shadow_steps)))\n",
    "\n",
    "            img = np.delete(img, to_remove, 0)\n",
    "            probs = np.delete(probs, to_remove, 0)\n",
    "            image_dates = np.delete(image_dates, to_remove)\n",
    "            shadows = np.delete(shadows, to_remove, 0)\n",
    "\n",
    "            to_remove = remove_missed_clouds(img)\n",
    "            img = np.delete(img, to_remove, 0)\n",
    "            probs = np.delete(probs, to_remove, 0)\n",
    "            image_dates = np.delete(image_dates, to_remove)\n",
    "            shadows = np.delete(shadows, to_remove, 0)\n",
    "            print(\"Removing {} steps based on ratio\".format(len(to_remove)))\n",
    "\n",
    "\n",
    "            # Concatenate DEM\n",
    "            dem = np.tile(dem.reshape((1, IMSIZE, IMSIZE, 1)), (img.shape[0], 1, 1, 1))\n",
    "            tiles = np.concatenate([img, dem], axis = -1)\n",
    "            tiles[:, :, :, -1] /= 90\n",
    "\n",
    "            x = remove_cloud_and_shadows(tiles, probs, shadows, image_dates)\n",
    "            if SUPER_RESOLVE:\n",
    "                x = x[:, 8:40, 8:40, :]\n",
    "                print(\"Shape before super: {}\".format(x.shape))\n",
    "\n",
    "                d10 = x[:, :, :, 0:4]\n",
    "                d20 = x[:, :, :, 4:10]\n",
    "\n",
    "                d10 = np.swapaxes(d10, 1, -1)\n",
    "                d10 = np.swapaxes(d10, 2, 3)\n",
    "                d20 = np.swapaxes(d20, 1, -1)\n",
    "                d20 = np.swapaxes(d20, 2, 3)\n",
    "                superresolved = DSen2(d10, d20)\n",
    "                superresolved = np.swapaxes(superresolved, 1, -1)\n",
    "                superresolved = np.swapaxes(superresolved, 1, 2)\n",
    "                print(superresolved.shape)\n",
    "                print(x.shape)\n",
    "\n",
    "                # returns band IDXs 3, 4, 5, 7, 8, 9\n",
    "                x[:, :, :, 4:10] = superresolved\n",
    "                x = x[:, 8:24, 8:24, :]\n",
    "                print(\"Shape after super: {}\".format(x.shape))\n",
    "            else:\n",
    "                bottom = int(IMSIZE/2 - 8)\n",
    "                top = int(IMSIZE/2 + 8)\n",
    "                x = x[:, bottom:top, bottom:top, :]\n",
    "\n",
    "            # Calculate indices\n",
    "            tiles = evi(x, True)\n",
    "            tiles = bi(tiles, True)\n",
    "            tiles = msavi2(tiles, True)\n",
    "            x = si(tiles, True)\n",
    "\n",
    "            print(\"Shape after vegetation indexes: {}\".format(x.shape))\n",
    "            \n",
    "            missing_pixels = 0\n",
    "            for band in range(0, 15):\n",
    "                for time in range(0, x.shape[0]):\n",
    "                    x_i = x[time, :, :, band]\n",
    "                    missing_pixels += len(np.argwhere(np.isnan(x_i)))\n",
    "                    x_i[np.argwhere(np.isnan(x_i))] = np.mean(x_i)\n",
    "                    x[time, :, :, band] = x_i\n",
    "            print(\"There are {} missing pixels\".format(missing_pixels))\n",
    "\n",
    "            # Interpolate linearly to 5 day frequency\n",
    "            tiles, max_distance = calculate_and_save_best_images(x, image_dates)\n",
    "\n",
    "            # Smooth linear interpolation\n",
    "            coefmat = intialize_smoother()\n",
    "            tiles = interpolate_array(tiles, dim = tiles.shape[1])\n",
    "            \n",
    "            if max_distance <= 240:\n",
    "                np.save(output_folder + str(val), tiles)\n",
    "                #np.save(\"../data/raw/train-clouds/\" + str(val) + \".npy\", probs)\n",
    "                #np.save(\"../data/raw/train-shadows/\" + str(val) + \".npy\", shadows)\n",
    "                print(\"Saved array of {} shape to {}\".format(tiles.shape, val))\n",
    "                print(\"\\n\")\n",
    "            else:\n",
    "                print(\"Skipping {} because there is a {} distance\".format(val, max_distance))\n",
    "                print(\"\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            logging.fatal(e, exc_info=True)\n",
    "            #errors.append(img)\n",
    "            #continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in os.listdir(\"../data/test-csv/\"):\n",
    "    if \".csv\" in i:\n",
    "        if \"americas\" not in i:\n",
    "            if \"global\" not in i:\n",
    "            #if \".csv\" in i:\n",
    "                #if any(x in i for x in [\"africa-west\", \"cameroon\", \"koure\", \"niger\"]):\n",
    "                tile = download_plots(\"../data/test-csv/\" + i, \"../data/test-s2/\", image_format = MimeType.TIFF_d16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_sensing",
   "language": "python",
   "name": "remote_sensing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
