{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This Jupyter notebook downloads and preprocesses Sentinel 1 and 2 tiles for large areas (at least 40 sq km). The workflow entails generating the tile coordinates, downloading the raw data, and processing (cloud and shadow removal, gap interpolation, indices, and superresolution).\n",
    "\n",
    "The notebook is broken down into the following sections:\n",
    "\n",
    "   * **Parameter definition**:\n",
    "   * **Projection functions**\n",
    "   * **Data download functions**\n",
    "   * **Cloud and shadow removal functions**\n",
    "   * **Superresoluttion functions**\n",
    "   * **Tile and folder management functions**\n",
    "   * **Function execution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are planning to download new Sentinel data, you need to have an API key to use the data provider [Sentinel Hub](https://www.sentinel-hub.com). If you do not have an API key but have access to sentinel imagery, the input data for this notebook is an entire year of:\n",
    "  * Cloud masks\n",
    "  * L1C bands 2, 8A, 11\n",
    "  * 10- and 20m L2A bands\n",
    "  * VV-VH Sentinel 1 bands\n",
    "  * Digital elevation model\n",
    "  \n",
    "  \n",
    "The data are tiled into 6300m x 6300m windows. An example of the raw data can be downloaded by running the following cell. This data can be preprocessed (cloud interpolation, super resolution, smoothing, etcetera) by running the rest of the notebook. It can then also be predicted by running `4b-predict-large-area`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from osgeo import ogr, osr\n",
    "from sentinelhub import WmsRequest, WcsRequest, MimeType, CRS, BBox, constants\n",
    "import logging\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import os\n",
    "import yaml\n",
    "from sentinelhub import DataSource\n",
    "import scipy.sparse as sparse\n",
    "from scipy.sparse.linalg import splu\n",
    "from skimage.transform import resize\n",
    "from sentinelhub import CustomUrlParam\n",
    "from time import time as timer\n",
    "from time import sleep as sleep\n",
    "import multiprocessing\n",
    "import math\n",
    "import reverse_geocoder as rg\n",
    "import pycountry\n",
    "import pycountry_convert as pc\n",
    "import hickle as hkl\n",
    "from shapely.geometry import Point, Polygon\n",
    "import geopandas\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "import math\n",
    "import boto3\n",
    "from pyproj import Proj, transform\n",
    "from timeit import default_timer as timer\n",
    "from typing import Tuple, List\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"../config.yaml\"):\n",
    "    with open(\"../config.yaml\", 'r') as stream:\n",
    "        key = (yaml.safe_load(stream))\n",
    "        API_KEY = key['key']\n",
    "        AWSKEY = key['awskey']\n",
    "        AWSSECRET = key['awssecret']\n",
    "else:\n",
    "    API_KEY = \"none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../src/preprocessing/slope.py\n",
    "%run ../src/preprocessing/indices.py\n",
    "%run ../src/downloading/utils.py\n",
    "%run ../src/preprocessing/cloud_removal.py\n",
    "%run ../src/preprocessing/whittaker_smoother.py\n",
    "%run ../src/io/upload.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Constants and Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the only years that can be downloaded from Sentinel Hub are 2018 and 2019. 2017 has an ETA of Summer 2020.\n",
    "\n",
    "The `landscapes` dictionary has a key, value convention of the landscape name, and a `(lat, long)` tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2019\n",
    "landscape = 'malawi-rumphi'\n",
    "\n",
    "if year > 2017:\n",
    "    dates = (f'{str(year - 1)}-12-01' , f'{str(year + 1)}-02-01')\n",
    "else: \n",
    "    dates = (f'{str(year)}-01-01' , f'{str(year + 1)}-02-01')\n",
    "    \n",
    "dates_sentinel_1 = (f'{str(year)}-01-01' , f'{str(year)}-12-31')\n",
    "SIZE = 9*5\n",
    "IMSIZE = (7*2) + (SIZE * 14)+2 # process 6320 x 6320 m blocks\n",
    "\n",
    "days_per_month = [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30]\n",
    "starting_days = np.cumsum(days_per_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33.241888, -11.134766) ../project-monitoring/zambia/eastern/chama/2019/\n"
     ]
    }
   ],
   "source": [
    "database = pd.read_csv(\"../project-monitoring/database.csv\")\n",
    "coords = database[database['landscape'] == landscape]\n",
    "path = coords['path'].tolist()[0]\n",
    "coords = (float(coords['longitude']), float(coords['latitude']))\n",
    "\n",
    "IO_PARAMS = {'prefix': '../',\n",
    "             'bucket': 'restoration-monitoring',\n",
    "             'coords': coords,\n",
    "             'bucket-prefix': '',\n",
    "             'path': path}\n",
    "\n",
    "OUTPUT_FOLDER = IO_PARAMS['prefix'] + IO_PARAMS['path'] + str(year) + '/'\n",
    "print(coords, OUTPUT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading formatted geocoded file...\n"
     ]
    }
   ],
   "source": [
    "to_append = pd.DataFrame({'landscape': ['malawi-rumphi'],\n",
    "                             'latitude': ['-11.134766'],\n",
    "                             'longitude': ['33.241888'],\n",
    "                             'path': [get_folder_prefix((-11.134766,  33.241888),\n",
    "                                                        params = {'bucket-prefix': 'project-monitoring'})]})\n",
    "database = database.append([to_append])\n",
    "#database.to_csv(\"../project-monitoring/database.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Data download functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using Sentinel hub, identify the following layers:\n",
    "  * CLOUD: return [CLP / 255]\n",
    "  * SHADOW: return [B02, B8A, B11]\n",
    "  * DEM: return [DEM]\n",
    "  * SENT: return [VV, VH]\n",
    "  * L2A10: return [B02,B03,B04, B08]\n",
    "  * L2A20: return [B05,B06,B07, B8A,B11,B12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dates(date_dict: dict, year: int) -> List:\n",
    "    \"\"\" Transforms a SentinelHub date dictionary to a\n",
    "         list of integer calendar dates\n",
    "    \"\"\"\n",
    "    dates = []\n",
    "    days_per_month = [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30]\n",
    "    starting_days = np.cumsum(days_per_month)\n",
    "    for date in date_dict:\n",
    "        if date.year == year - 1:\n",
    "            dates.append(-365 + starting_days[(date.month-1)] + date.day)\n",
    "        if date.year == year:\n",
    "            dates.append(starting_days[(date.month-1)] + date.day)\n",
    "        if date.year == year + 1:\n",
    "            dates.append(365 + starting_days[(date.month-1)]+date.day)\n",
    "    return dates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud and cloud shadow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_clouds(bbox: List[Tuple[float, float]],\n",
    "                epsg: 'CRS', dates: dict = dates) -> (np.ndarray, np.ndarray, np.ndarray):\n",
    "    \"\"\" Downloads and calculates cloud cover and shadow\n",
    "        \n",
    "        Parameters:\n",
    "         bbox (list): output of calc_bbox\n",
    "         epsg (float): EPSG associated with bbox \n",
    "         dates (tuple): YY-MM-DD - YY-MM-DD bounds for downloading \n",
    "    \n",
    "        Returns:\n",
    "         cloud_img (np.array):\n",
    "         shadows (np.array): \n",
    "         clean_steps (np.array):\n",
    "    \"\"\"\n",
    "    # Download 160 x 160 meter cloud masks, 0 - 255\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    cloud_request = WcsRequest(\n",
    "        layer='CLOUD_NEW',\n",
    "        bbox=box, time=dates,\n",
    "        resx='160m',resy='160m',\n",
    "        image_format = MimeType.TIFF_d8,\n",
    "        maxcc=0.75, instance_id=API_KEY,\n",
    "        custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "        time_difference=datetime.timedelta(hours=72),\n",
    "    )\n",
    "\n",
    "    # Download 20 x 20 meter bands for shadow masking, 0 - 65535\n",
    "    shadow_request = WcsRequest(\n",
    "        layer='SHADOW',\n",
    "        bbox=box, time=dates,\n",
    "        resx='20m', resy='20m',\n",
    "        image_format = MimeType.TIFF_d16,\n",
    "        maxcc=0.75, instance_id=API_KEY,\n",
    "        custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "        time_difference=datetime.timedelta(hours=72))\n",
    "\n",
    "    cloud_img = np.array(cloud_request.get_data())\n",
    "    if not isinstance(cloud_img.flat[0], np.floating):\n",
    "        assert np.max(cloud_img) > 1\n",
    "        print(f\"Original cloud image max is {np.max(cloud_img)}, {cloud_img.shape}\")\n",
    "        cloud_img = cloud_img / 255\n",
    "    assert np.max(cloud_img) <= 1., f'The max cloud probability is {np.max(cloud_img)}'\n",
    "    \n",
    "    c_probs_pus = ((40*40)/(512*512)) *(1/3)*cloud_img.shape[0]\n",
    "    print(f\"Cloud_probs used {round(c_probs_pus, 1)} processing units\")\n",
    "    \n",
    "    cloud_img = resize(cloud_img, (cloud_img.shape[0], IMSIZE, IMSIZE), order = 0)\n",
    "    n_cloud_px = np.sum(cloud_img > 0.33, axis = (1, 2))\n",
    "    cloud_steps = np.argwhere(n_cloud_px > (IMSIZE**2 * 0.15))\n",
    "    clean_steps = [x for x in range(cloud_img.shape[0]) if x not in cloud_steps]\n",
    "    \n",
    "    # Extract cloud, shadow dates\n",
    "    cloud_dates_dict = [x for x in cloud_request.get_dates()]\n",
    "    cloud_dates = extract_dates(cloud_dates_dict, year)\n",
    "    cloud_dates = [val for idx, val in enumerate(cloud_dates) if idx in clean_steps]\n",
    "    shadow_dates_dict = [x for x in shadow_request.get_dates()]\n",
    "    shadow_dates = extract_dates(shadow_dates_dict, year)\n",
    "    shadow_steps = [idx for idx, val in enumerate(shadow_dates) if val in cloud_dates]\n",
    "    \n",
    "    shadow_img = np.array(shadow_request.get_data(data_filter = shadow_steps))\n",
    "    shadow_pus = (shadow_img.shape[1]*shadow_img.shape[2])/(512*512) * shadow_img.shape[0]\n",
    "    shadow_img = resize(shadow_img, (shadow_img.shape[0], IMSIZE, IMSIZE, shadow_img.shape[-1]), order = 0)\n",
    "    print(f\"There are unique {len(np.unique(shadow_img))} shadow L1C values\")\n",
    "    \n",
    "    print(f\"The max shadows is {np.max(shadow_img)}\")\n",
    "    if not isinstance(shadow_img.flat[0], np.floating):\n",
    "        assert np.max(shadow_img) > 1\n",
    "        shadow_img = shadow_img / 65535\n",
    "    assert np.max(shadow_img) <= 1\n",
    " \n",
    "    cloud_img = np.delete(cloud_img, cloud_steps, 0)\n",
    "    assert shadow_img.shape[0] == cloud_img.shape[0], (shadow_img.shape, cloud_img.shape)\n",
    "    shadows = mcm_shadow_mask(np.array(shadow_img), cloud_img) # Make usre this makes sense??\n",
    "    print(f\"Shadows ({shadows.shape}) used {round(shadow_pus, 1)} processing units\")\n",
    "    n_always_shadow = np.min(shadows, axis = (1, 2))\n",
    "    print(f\"There are {np.sum(n_always_shadow)} pixels that always have shadows\")\n",
    "    \n",
    "    return cloud_img, shadows, clean_steps, np.array(cloud_dates)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digital elevation model, slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dem(bbox: List[Tuple[float, float]], epsg: 'CRS') -> np.ndarray:\n",
    "    \"\"\" Downloads the DEM layer from Sentinel hub\n",
    "        \n",
    "        Parameters:\n",
    "         bbox (list): output of calc_bbox\n",
    "         epsg (float): EPSG associated with bbox \n",
    "    \n",
    "        Returns:\n",
    "         dem_image (arr):\n",
    "    \"\"\"\n",
    "\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    dem_size = 650\n",
    "    dem_request = WmsRequest(data_source=DataSource.DEM,\n",
    "                         layer='DEM', bbox=box,\n",
    "                         width=dem_size, height=dem_size,\n",
    "                         instance_id=API_KEY,\n",
    "                         image_format=MimeType.TIFF_d32f,\n",
    "                         custom_url_params={CustomUrlParam.SHOWLOGO: False})\n",
    "    dem_image = dem_request.get_data()[0]\n",
    "    dem_image = median_filter(dem_image, size = 5)\n",
    "    dem_image = calcSlope(dem_image.reshape((1, dem_size, dem_size)),\n",
    "                          np.full((dem_size, dem_size), 10), \n",
    "                          np.full((dem_size, dem_size), 10), zScale = 1, minSlope = 0.02)\n",
    "    dem_image = dem_image.reshape((dem_size,dem_size, 1))\n",
    "    dem_image = dem_image[1:dem_size-1, 1:dem_size-1, :]\n",
    "    print(f\"DEM used {round(((IMSIZE*IMSIZE)/(512*512))*2, 1)} processing units\")\n",
    "    print(f\"There are {len(np.unique(dem_image))} unique DEM values\")\n",
    "    return dem_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Sentinel 2 L2A, 10 and 20 meter bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_layer(bbox: List[Tuple[float, float]],\n",
    "                   clean_steps: np.ndarray, epsg: 'CRS',\n",
    "                   dates: dict = dates, year: int = year) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\" Downloads the L2A sentinel layer with 10 and 20 meter bands\n",
    "        \n",
    "        Parameters:\n",
    "         bbox (list): output of calc_bbox\n",
    "         clean_steps (list): list of steps to filter download request\n",
    "         epsg (float): EPSG associated with bbox \n",
    "         time (tuple): YY-MM-DD - YY-MM-DD bounds for downloading \n",
    "    \n",
    "        Returns:\n",
    "         img (arr):\n",
    "         img_request (obj): \n",
    "    \"\"\"\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    image_request = WcsRequest(\n",
    "            layer='L2A20',\n",
    "            bbox=box, time=dates,\n",
    "            image_format = MimeType.TIFF_d16,\n",
    "            maxcc=0.75, resx='20m', resy='20m',\n",
    "            instance_id=API_KEY,\n",
    "            custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'NEAREST',\n",
    "                                constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "            time_difference=datetime.timedelta(hours=72),\n",
    "        )\n",
    "    ## There needs to be a code block here aligning the dates with cloud probs\n",
    "    image_dates_dict = [x for x in image_request.get_dates()]\n",
    "    image_dates = extract_dates(image_dates_dict, year)\n",
    "    steps_to_download = [i for i, val in enumerate(image_dates) if val in clean_steps]\n",
    "    dates_to_download = [val for i, val in enumerate(image_dates) if val in clean_steps]\n",
    "    print(f\"The cloud-free image dates are {dates_to_download}\")\n",
    "    \n",
    "    img_bands = image_request.get_data(data_filter = steps_to_download)\n",
    "    img_20 = np.stack(img_bands)\n",
    "    if not isinstance(img_20.flat[0], np.floating):\n",
    "        assert np.max(img_20) > 2\n",
    "        print(f\"Converting S2, 20m to float32, with {np.max(img_20)} max and\"\n",
    "              f\" {len(np.unique(img_20))} unique values\")\n",
    "        img_20 = img_20.astype(np.float32)\n",
    "        img_20 = img_20 / 65535.\n",
    "        assert np.max(img_20) <= 2.\n",
    "    \n",
    "    s2_20_usage = (img_20.shape[1]*img_20.shape[2])/(512*512) * (6/3) * img_20.shape[0]\n",
    "    print(f\"Original 20 meter bands size: {img_20.shape}, using {round(s2_20_usage, 1)} PU\")\n",
    "    if img_20.shape[2]*img_20.shape[2] != 323*323:\n",
    "        print(f\"Reshaping: {img_20.shape}\")\n",
    "        img_20 = resize(img_20, (img_20.shape[0], 323, 323, img_20.shape[-1]), order = 0)\n",
    "\n",
    "    image_request = WcsRequest(\n",
    "            layer='L2A10',\n",
    "            bbox=box, time=dates,\n",
    "            image_format = MimeType.TIFF_d16,\n",
    "            maxcc=0.75, resx='10m', resy='10m',\n",
    "            instance_id=API_KEY,\n",
    "            custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'BICUBIC',\n",
    "                                constants.CustomUrlParam.UPSAMPLING: 'BICUBIC'},\n",
    "            time_difference=datetime.timedelta(hours=72),\n",
    "    )\n",
    "    \n",
    "    img_bands = image_request.get_data(data_filter = steps_to_download)\n",
    "    img_10 = np.stack(img_bands)#.astype(np.float32)\n",
    "    if not isinstance(img_10.flat[0], np.floating):\n",
    "        assert np.max(img_10) > 2\n",
    "        print(f\"Converting S2, 10m to float32, with {np.max(img_10)} max and\"\n",
    "                  f\" {len(np.unique(img_10))} unique values\")\n",
    "        img_10 = img_10.astype(np.float32)\n",
    "        img_10 = img_10 / 65535.\n",
    "    assert np.max(img_10) <= 2.\n",
    "    \n",
    "    s2_10_usage = (img_10.shape[1]*img_10.shape[2])/(512*512) * (4/3) * img_10.shape[0]\n",
    "    if img_10.shape[2]*img_10.shape[1] != IMSIZE*IMSIZE:\n",
    "        print(f\"Reshaping: {img_10.shape}\")\n",
    "        img_10 = resize(img_10, (img_10.shape[0], IMSIZE, IMSIZE, img_10.shape[-1]), order = 0)\n",
    "    \n",
    "    \n",
    "    img_10 = np.clip(img_10, 0, 1)\n",
    "    img_20 = np.clip(img_20, 0, 1)\n",
    "    return img_10, img_20, np.array(dates_to_download)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentinel 1 IW bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_sentinel_1(bbox: List[Tuple[float, float]],\n",
    "                        epsg: 'CRS', imsize: int = IMSIZE, \n",
    "                        dates: dict = dates_sentinel_1, layer: str = \"SENT\",\n",
    "                        year: int = year) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\" Downloads the GRD Sentinel 1 VV-VH layer from Sentinel Hub\n",
    "        \n",
    "        Parameters:\n",
    "         bbox (list): output of calc_bbox\n",
    "         epsg (float): EPSG associated with bbox \n",
    "         imsize (int):\n",
    "         dates (tuple): YY-MM-DD - YY-MM-DD bounds for downloading \n",
    "         layer (str):\n",
    "         year (int): \n",
    "    \n",
    "        Returns:\n",
    "         s1 (arr):\n",
    "         image_dates (arr): \n",
    "    \"\"\"\n",
    "    source = DataSource.SENTINEL1_IW_DES if layer == \"SENT_DESC\" else DataSource.SENTINEL1_IW_ASC\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    image_request = WcsRequest(\n",
    "            layer=layer, bbox=box,\n",
    "            time=dates,\n",
    "            image_format = MimeType.TIFF_d16,\n",
    "            data_source=source, maxcc=1.0,\n",
    "            resx='10m', resy='10m',\n",
    "            instance_id=API_KEY,\n",
    "            custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'NEAREST',\n",
    "                                constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "            time_difference=datetime.timedelta(hours=72),\n",
    "        )\n",
    "    \n",
    "    data_filter = [x for x in range(len(image_request.download_list))]\n",
    "    if len(image_request.download_list) > 40:\n",
    "        data_filter = [x for x in range(len(image_request.download_list)) if x % 2 == 0]\n",
    "        \n",
    "    if len(image_request.download_list) > 0:\n",
    "        img_bands = image_request.get_data(data_filter = data_filter)\n",
    "        s1 = np.stack(img_bands)#.astype(np.float32)\n",
    "        if not isinstance(s1.flat[0], np.floating):\n",
    "            assert np.max(s1) > 2\n",
    "            print(f\"Converting s1 to float32, with {np.max(s1)} max and\"\n",
    "                  f\" {len(np.unique(s1))} unique values\")\n",
    "            s1 = s1.astype(np.float32)\n",
    "            s1 = s1 / 65535.\n",
    "\n",
    "        s1_usage = (2/3) * s1.shape[0] * ((s1.shape[1]*s1.shape[2]) / (512*512))\n",
    "        print(f\"Sentinel 1 used {round(s1_usage, 1)} PU for \\\n",
    "              {s1.shape[0]} out of {len(image_request.download_list)} images\")\n",
    "\n",
    "        image_dates_dict = [x for x in image_request.get_dates()]\n",
    "        image_dates = extract_dates(image_dates_dict, year)\n",
    "        image_dates = [val for idx, val in enumerate(image_dates) if idx in data_filter]\n",
    "        image_dates = np.array(image_dates)\n",
    "\n",
    "        s1c = np.copy(s1)\n",
    "        s1c[np.where(s1c < 1.)] = 0\n",
    "        s1c[np.where(s1c >= 1.)] = 1.\n",
    "        n_pix_oob = np.sum(s1c, axis = (1, 2, 3))\n",
    "        print(n_pix_oob / (imsize*2*imsize*2))\n",
    "        to_remove = np.argwhere(n_pix_oob > (imsize*2*imsize*2)/50)\n",
    "        s1 = np.delete(s1, to_remove, 0)\n",
    "        image_dates = np.delete(image_dates, to_remove)\n",
    "        s1 = np.clip(s1, 0, 1)\n",
    "        return s1, image_dates\n",
    "    else: \n",
    "        return np.empty((0,)), np.empty((0,))\n",
    "\n",
    "\n",
    "def identify_s1_layer(coords: Tuple[float, float]) -> str:\n",
    "    \"\"\" Identifies whether to download ascending or descending \n",
    "        sentinel 1 orbit based upon predetermined geographic coverage\n",
    "        \n",
    "        Reference: https://sentinel.esa.int/web/sentinel/missions/\n",
    "                   sentinel-1/satellite-description/geographical-coverage\n",
    "        \n",
    "        Parameters:\n",
    "         coords (tuple): \n",
    "    \n",
    "        Returns:\n",
    "         layer (str): either of SENT, SENT_DESC \n",
    "    \"\"\"\n",
    "    results = rg.search(coords)\n",
    "    country = results[-1]['cc']\n",
    "    continent_name = pc.country_alpha2_to_continent_code(country)\n",
    "    if continent_name in ['AF', 'OC']:\n",
    "        layer = \"SENT\"\n",
    "    if continent_name in ['SA']:\n",
    "        if coords[0] > -7.11:\n",
    "            layer = \"SENT\"\n",
    "        else:\n",
    "            layer = \"SENT_DESC\"\n",
    "    if continent_name in ['AS']:\n",
    "        if coords[0] > 23.3:\n",
    "            layer = \"SENT\"\n",
    "        else:\n",
    "            layer = \"SENT_DESC\"\n",
    "    if continent_name in ['NA']:\n",
    "        layer = \"SENT_DESC\"\n",
    "    print(f\"The continent is: {continent_name}, and the sentinel 1 orbit is {layer}\")\n",
    "    return layer\n",
    "    \n",
    "    \n",
    "def process_sentinel_1_tile(sentinel1: np.ndarray, dates: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Converts a (?, X, Y, 2) Sentinel 1 array to (24, X, Y, 2)\n",
    "\n",
    "        Parameters:\n",
    "         sentinel1 (np.array):\n",
    "         dates (np.array):\n",
    "\n",
    "        Returns:\n",
    "         s1 (np.array)\n",
    "    \"\"\"\n",
    "    s1, _ = calculate_and_save_best_images(sentinel1, dates)\n",
    "    biweekly_dates = np.array([day for day in range(0, 360, 5)])\n",
    "    to_remove = np.argwhere(biweekly_dates % 15 != 0)\n",
    "    s1 = np.delete(s1, to_remove, 0)\n",
    "    return s1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Superresolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "from keras import backend as K\n",
    "K.set_session(sess)\n",
    "\n",
    "MDL_PATH = \"../models/supres/\"\n",
    "\n",
    "model = tf.train.import_meta_graph(MDL_PATH + 'model.meta')\n",
    "model.restore(sess, tf.train.latest_checkpoint(MDL_PATH))\n",
    "\n",
    "logits = tf.get_default_graph().get_tensor_by_name(\"Add_6:0\")\n",
    "inp = tf.get_default_graph().get_tensor_by_name(\"Placeholder:0\")\n",
    "inp_bilinear = tf.get_default_graph().get_tensor_by_name(\"Placeholder_1:0\")\n",
    "\n",
    "def superresolve(input_data, bilinear_upsample):\n",
    "    \"\"\" Worker function to run predictions on input data\n",
    "    \"\"\"\n",
    "    x = sess.run([logits], \n",
    "                 feed_dict={inp: input_data,\n",
    "                            inp_bilinear: bilinear_upsample})\n",
    "    return x[0]\n",
    "\n",
    "\n",
    "def superresolve_tile(arr: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Superresolves each 56x56 subtile in a 646x646 input tile\n",
    "       by padding the subtiles to 64x64 and removing the pad after prediction,\n",
    "       eliminating boundary artifacts\n",
    "\n",
    "        Parameters:\n",
    "         arr (arr): (?, 646, 646, 10) array\n",
    "\n",
    "        Returns:\n",
    "         superresolved (arr): (?, 646, 646, 10) array\n",
    "    \"\"\"\n",
    "    print(f\"The input array to superresolve is {arr.shape}\")\n",
    "    tiles = tile_window(646, 646, 60, 60)\n",
    "    for i in tnrange(len(tiles)):\n",
    "        subtile = tiles[i]\n",
    "        pad_l = 0 if subtile[0] >= 2 else 2\n",
    "        pad_r = 0 if subtile[0] < (644 - 60) else 2\n",
    "        pad_u = 0 if subtile[1] >= 2 else 2\n",
    "        pad_d = 0 if subtile[1] < (644 - 60) else 2\n",
    "        to_resolve = arr[:, np.max([subtile[0]-2, 0]):subtile[0]+62,\n",
    "                            np.max([subtile[1]-2, 0]):subtile[1]+62, :]\n",
    "        to_resolve = np.pad(to_resolve, ((0, 0), (pad_l, pad_r), (pad_u, pad_d), (0, 0)), 'reflect')\n",
    "        \n",
    "        bilinear = to_resolve[..., 4:]\n",
    "        \n",
    "        resolved = superresolve(\n",
    "            to_resolve, bilinear)\n",
    "        resolved = resolved[:, 2:-2, 2:-2, :]\n",
    "        arr[:, subtile[0]:subtile[0]+60, subtile[1]:subtile[1]+60, 4:] = resolved\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Tiling and folder management functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# move to src/utils/pathing.py\n",
    "def make_output_and_temp_folders(idx: str, output_folder: str = OUTPUT_FOLDER) -> None:\n",
    "    \"\"\"Makes necessary folder structures for IO of raw and processed data\n",
    "\n",
    "        Parameters:\n",
    "         idx (str)\n",
    "         output_folder (path)\n",
    "\n",
    "        Returns:\n",
    "         None\n",
    "    \"\"\"\n",
    "    def _find_and_make_dirs(dirs):\n",
    "        if not os.path.exists(os.path.realpath(dirs)):\n",
    "            os.makedirs(os.path.realpath(dirs))\n",
    "            \n",
    "    _find_and_make_dirs(output_folder + \"raw/\")\n",
    "    _find_and_make_dirs(output_folder + \"raw/clouds/\")\n",
    "    _find_and_make_dirs(output_folder + \"raw/s1/\")\n",
    "    _find_and_make_dirs(output_folder + \"raw/s2_10/\")\n",
    "    _find_and_make_dirs(output_folder + \"raw/s2_20/\")\n",
    "    _find_and_make_dirs(output_folder + \"raw/misc/\")\n",
    "    _find_and_make_dirs(output_folder + \"processed/\")\n",
    "    _find_and_make_dirs(output_folder + \"interim/\")\n",
    "    _find_and_make_dirs(output_folder + \"raw/s2/\")\n",
    "\n",
    "# move to src/utils/typing.py\n",
    "def to_int16(array: np.array) -> np.array:\n",
    "    '''Converts a float32 array to int16, reducing storage costs by three-fold'''\n",
    "    assert np.min(array) > -0.01, np.min(array)\n",
    "    assert np.max(array) < 1.01, np.max(array)\n",
    "    \n",
    "    array = np.clip(array, 0, 1)\n",
    "    array = np.trunc(array * 65535)\n",
    "    assert np.min(array >= 0)\n",
    "    assert np.max(array <= 65535)\n",
    "    \n",
    "    return array.astype(np.uint16)\n",
    "\n",
    "# move to src/utils/typing.py\n",
    "def to_float32(array: np.array) -> np.array:\n",
    "    '''Converts an int16 array to float32'''\n",
    "    divide = 1. if isinstance(array.flat[0], np.floating) else 65535\n",
    "    return np.float32(array) / divide\n",
    "\n",
    "def id_missing_px(sentinel2, thresh = 11):\n",
    "    missing_images = np.sum(sentinel2[..., :10] == 0.0, axis = (1, 2, 3))\n",
    "    missing_images_p = np.sum(sentinel2[..., :10] >= 1., axis = (1, 2, 3))\n",
    "    missing_images = missing_images + missing_images_p\n",
    "    print(missing_images)\n",
    "    missing_images = np.argwhere(missing_images >= (sentinel2.shape[1]**2) / thresh)\n",
    "    missing_images = missing_images.flatten()\n",
    "    return missing_images\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download worker fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_large_tile(coord: tuple,\n",
    "                        step_x: int,\n",
    "                        step_y: int,\n",
    "                        folder: str = OUTPUT_FOLDER, \n",
    "                        year: int = year,\n",
    "                        s1_layer: str = \"SENT\") -> None:\n",
    "    \"\"\"Wrapper function to download cloud probs, Sentinel 2, Sentinel 1, and DEM\n",
    "\n",
    "        Parameters:\n",
    "         coord (tuple):\n",
    "         step_x (int):\n",
    "         step_y (int):\n",
    "         folder (path):\n",
    "         year (int):\n",
    "         s1_layer (str):\n",
    "\n",
    "        Returns:\n",
    "         None\n",
    "    \"\"\"\n",
    "    bbx, epsg = calculate_bbx_pyproj(coord, step_x, step_y, expansion = 80)\n",
    "    dem_bbx, _ = calculate_bbx_pyproj(coord, step_x, step_y, expansion = 90)\n",
    "    idx = str(step_y) + \"_\" + str(step_x)\n",
    "    idx = str(idx)\n",
    "    make_output_and_temp_folders(idx)\n",
    "    \n",
    "    output_path = f\"{folder}output/{str(step_y*5)}/{str(step_x*5)}.npy\"\n",
    "    process_path = f\"{folder}processed/{str(step_y*5)}/{str(step_x*5)}.npy\"\n",
    "    if not (os.path.exists(output_path) or os.path.exists(process_path)):\n",
    "        clouds_file = f'{folder}raw/clouds/clouds_{idx}.hkl'\n",
    "        shadows_file = f'{folder}raw/clouds/shadows_{idx}.hkl'\n",
    "        s1_file = f'{folder}raw/s1/{idx}.hkl'\n",
    "        s1_dates_file = f'{folder}raw/misc/s1_dates_{idx}.hkl'\n",
    "        s2_10_file = f'{folder}raw/s2_10/{idx}.hkl'\n",
    "        s2_20_file = f'{folder}raw/s2_20/{idx}.hkl'\n",
    "        s2_dates_file = f'{folder}raw/misc/s2_dates_{idx}.hkl'\n",
    "        s2_file = f'{folder}raw/s2/{idx}.hkl'\n",
    "        clean_steps_file = f'{folder}raw/clouds/clean_steps_{idx}.hkl'\n",
    "\n",
    "        if not os.path.exists(clouds_file):\n",
    "            # All this needs to be int16, copied to cloud with io.save_file\n",
    "            print(f\"Downloading clouds because {clouds_file} does not exist\")\n",
    "            cloud_probs, shadows, _, image_dates = identify_clouds(bbx, epsg = epsg)\n",
    "            to_remove, _ = calculate_cloud_steps(cloud_probs, image_dates)\n",
    "            print(to_remove)\n",
    "            clean_dates = np.delete(image_dates, to_remove)\n",
    "            cloud_probs = np.delete(cloud_probs, to_remove, 0)\n",
    "            shadows = np.delete(shadows, to_remove, 0)\n",
    "            hkl.dump(cloud_probs, clouds_file, mode='w', compression='gzip')\n",
    "            hkl.dump(shadows, shadows_file, mode='w', compression='gzip')\n",
    "            hkl.dump(clean_dates, clean_steps_file, mode='w', compression='gzip')\n",
    "\n",
    "        if not os.path.exists(s1_file):\n",
    "            print(f\"Downloading S1 because {s1_file} does not exist\")\n",
    "            s1_layer = identify_s1_layer((coord[1], coord[0]))\n",
    "            s1, s1_dates = download_sentinel_1(bbx, layer = s1_layer, epsg = epsg)\n",
    "            if s1.shape[0] == 0:\n",
    "                s1_layer = \"SENT_DESC\" if s1_layer == \"SENT\" else \"SENT\"\n",
    "                print(f'Switching to {s1_layer}')\n",
    "                s1, s1_dates = download_sentinel_1(bbx, layer = s1_layer, epsg = epsg)\n",
    "            s1 = process_sentinel_1_tile(s1, s1_dates)\n",
    "            hkl.dump(to_int16(s1), s1_file, mode='w', compression='gzip')\n",
    "            hkl.dump(s1_dates, s1_dates_file, mode='w', compression='gzip')\n",
    "\n",
    "        if not os.path.exists(s2_10_file):\n",
    "            # All this needs to be int16, copied to cloud with io.save_file\n",
    "            print(f\"Downloading S2 because {s2_10_file} does not exist\")\n",
    "            clean_steps = list(hkl.load(clean_steps_file))\n",
    "            cloud_probs = hkl.load(clouds_file)\n",
    "            shadows = hkl.load(shadows_file)    \n",
    "            s2_10, s2_20, s2_dates = download_layer(bbx, clean_steps = clean_steps, epsg = epsg)\n",
    "\n",
    "            # Steps to ensure that L2A, L1C derived products have exact matching dates\n",
    "            print(f\"Shadows {shadows.shape}, clouds {cloud_probs.shape}, S2, {s2_10.shape}, S2d, {s2_dates.shape}\")\n",
    "            to_remove_clouds = [i for i, val in enumerate(clean_steps) if val not in s2_dates]\n",
    "            to_remove_dates = [val for i, val in enumerate(clean_steps) if val not in s2_dates]\n",
    "            if len(to_remove_clouds) >= 1:\n",
    "                print(f\"Removing {to_remove_dates} from clouds because not in S2\")\n",
    "                cloud_probs = np.delete(cloud_probs, to_remove_clouds, 0)\n",
    "                shadows = np.delete(shadows, to_remove_clouds, 0)\n",
    "                print(f\"Shadows {shadows.shape}, clouds {cloud_probs.shape}\"\n",
    "                      f\" S2, {s2_10.shape}, S2d, {s2_dates.shape}\")\n",
    "                hkl.dump(cloud_probs, clouds_file, mode='w', compression='gzip')\n",
    "                hkl.dump(shadows, shadows_file, mode='w', compression='gzip')\n",
    "\n",
    "            assert cloud_probs.shape[0] == s2_10.shape[0], \"There is a date mismatch\"\n",
    "            hkl.dump(to_int16(s2_10), s2_10_file, mode='w', compression='gzip')\n",
    "            hkl.dump(to_int16(s2_20), s2_20_file, mode='w', compression='gzip')\n",
    "            hkl.dump(s2_dates, s2_dates_file, mode='w', compression='gzip')\n",
    "\n",
    "        if not os.path.exists(folder + \"raw/misc/dem_{}.hkl\".format(idx)):\n",
    "            dem = download_dem(dem_bbx, epsg = epsg)\n",
    "            hkl.dump(dem, folder + \"raw/misc/dem_{}.hkl\".format(idx), mode='w', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move to src/utils/pathing.py\n",
    "def make_folder_names(step_x: int, step_y: int) -> (list, list):\n",
    "    '''Given an input tile location (step_x, step_y), identify the folder and file\n",
    "       names for each 5x5 subtile\n",
    "       \n",
    "       Parameters:\n",
    "         step_x (int):\n",
    "         step_y (int):\n",
    "\n",
    "        Returns:\n",
    "         x_vals (list)\n",
    "         y_vals (list)\n",
    "    '''\n",
    "    x_vals = []\n",
    "    y_vals = []\n",
    "    for i in range(25):\n",
    "        y_val = (24 - i) // 5\n",
    "        x_val = 5 - ((25 - i) % 5)\n",
    "        x_val = 0 if x_val == 5 else x_val\n",
    "        x_vals.append(x_val)\n",
    "        y_vals.append(y_val)\n",
    "    y_vals = [i + (5*step_y) for i in y_vals]\n",
    "    x_vals = [i + (5*step_x) for i in x_vals]\n",
    "    return x_vals, y_vals\n",
    "\n",
    "\n",
    "def process_large_tile(coord: tuple,\n",
    "                       step_x: int,\n",
    "                       step_y: int,\n",
    "                       folder: str = OUTPUT_FOLDER,\n",
    "                       model: 'model' = model) -> None:\n",
    "    '''Wrapper function to interpolate clouds and temporal gaps, superresolve tiles,\n",
    "       calculate relevant indices, and save analysis-ready data to the output folder\n",
    "       \n",
    "       Parameters:\n",
    "        coord (tuple)\n",
    "        step_x (int):\n",
    "        step_y (int):\n",
    "        foldre (str):\n",
    "\n",
    "       Returns:\n",
    "        None\n",
    "    '''\n",
    "    idx = str(step_y) + \"_\" + str(step_x)\n",
    "    x_vals, y_vals = make_folder_names(step_x, step_y)\n",
    "\n",
    "    processed = True\n",
    "    for x, y in zip(x_vals, y_vals):\n",
    "        folder_path = f\"{str(y)}/{str(x)}\"\n",
    "        processed_exists = os.path.exists(folder + \"processed/\" + folder_path + \".hkl\")\n",
    "        output_exists = os.path.exists(folder + \"output/\" + folder_path + \".npy\")\n",
    "        if not (processed_exists or output_exists):\n",
    "            processed = False\n",
    "    if not processed:\n",
    "        print(f\"Processing because folder {folder_path}.npy does not exist\")\n",
    "\n",
    "        clouds = hkl.load(f'{folder}raw/clouds/clouds_{idx}.hkl')\n",
    "        sentinel1 = to_float32(hkl.load(f'{folder}raw/s1/{idx}.hkl'))\n",
    "        sentinel2_10 = to_float32(hkl.load(f'{folder}raw/s2_10/{idx}.hkl'))\n",
    "        sentinel2_20 = to_float32(hkl.load(f'{folder}raw/s2_20/{idx}.hkl'))\n",
    "        dem = hkl.load(f'{folder}raw/misc/dem_{idx}.hkl')\n",
    "        image_dates = hkl.load(f'{folder}raw/misc/s2_dates_{idx}.hkl')\n",
    "        shadows = hkl.load(f'{folder}raw/clouds/shadows_{idx}.hkl')  \n",
    "        \n",
    "        sentinel2 = np.empty((sentinel2_10.shape[0], 646, 646, 10))\n",
    "        sentinel2[..., :4] = sentinel2_10\n",
    "        for band in range(6):\n",
    "            for time in range(sentinel2.shape[0]):\n",
    "                sentinel2[time, ..., band + 4] = resize(sentinel2_20[time,..., band], (646, 646), 2)\n",
    "    \n",
    "        to_remove, _ = calculate_cloud_steps(clouds, image_dates)\n",
    "        print(sentinel2.shape, clouds.shape, shadows.shape, image_dates.shape)\n",
    "        if len(to_remove) > 0:\n",
    "            sentinel2 = np.delete(sentinel2, to_remove, axis = 0)\n",
    "            clouds = np.delete(clouds, to_remove, axis = 0)\n",
    "            shadows = np.delete(shadows, to_remove, axis = 0)\n",
    "            image_dates = np.delete(image_dates, to_remove)\n",
    "        print(f\"{to_remove} cloudy and missing images removed\")\n",
    "\n",
    "        missing_px = id_missing_px(sentinel2, 3)\n",
    "        if len(missing_px) > 0:\n",
    "            print(f\"Removing {missing_px} dates due to missing data\")\n",
    "            clouds = np.delete(clouds, missing_px, axis = 0)\n",
    "            shadows = np.delete(shadows, missing_px, axis = 0)\n",
    "            image_dates = np.delete(image_dates, missing_px)\n",
    "            sentinel2 = np.delete(sentinel2, missing_px, axis = 0)\n",
    "                    \n",
    "        x, interp = remove_cloud_and_shadows(sentinel2, clouds, shadows, image_dates) \n",
    "        to_remove = np.argwhere(np.mean(interp, axis = (1, 2)) > 0.5)\n",
    "        print(f\"{len(to_remove)} steps removed because of >50% interpolation rate\")\n",
    "        x = np.delete(x, to_remove, axis = 0)\n",
    "        clouds = np.delete(clouds, to_remove, axis = 0)\n",
    "        shadows = np.delete(shadows, to_remove, axis = 0)\n",
    "        image_dates = np.delete(image_dates, to_remove)\n",
    "        interp = np.delete(interp, to_remove, 0)\n",
    "                \n",
    "        x = superresolve_tile(np.float32(x))\n",
    "        dem_i = np.tile(dem[np.newaxis, 1:-1, 1:-1, :], (x.shape[0], 1, 1, 1))\n",
    "        dem_i = dem_i / 90\n",
    "        dem_i[dem_i > 0.25] = 0.25\n",
    "        x = np.concatenate([x, dem_i], axis = -1)\n",
    "        x = np.clip(x, 0, 1)\n",
    "        return x, image_dates, interp\n",
    "    else:\n",
    "        return None, None, None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FOLDER = \"/\".join(OUTPUT_FOLDER.split(\"/\")[:-2]) + \"/\"\n",
    "\n",
    "def interpolate_na_vals(s2):\n",
    "    '''Interpolates NA values with closest time steps, to deal with\n",
    "       the small potential for NA values in calculating indices'''\n",
    "    for x_loc in range(s2.shape[1]):\n",
    "        for y_loc in range(s2.shape[2]):\n",
    "            n_na = np.sum(np.isnan(s2[:, x_loc, y_loc, :]), axis = 1)\n",
    "            for date in range(s2.shape[0]):\n",
    "                if n_na.flatten()[date] > 0:\n",
    "                    before, after = calculate_proximal_steps(date, np.argwhere(n_na == 0))\n",
    "                    s2[date, x_loc, y_loc, :] = ((s2[date + before, x_loc, y_loc] + \n",
    "                                                 s2[date + after, x_loc, y_loc]) / 2)\n",
    "    numb_na = np.sum(np.isnan(s2), axis = (1, 2, 3))\n",
    "    if np.sum(numb_na) > 0:\n",
    "        print(f\"There are {numb_na} NA values\")\n",
    "    return s2\n",
    "\n",
    "def process_subtiles(coord: tuple,\n",
    "                       step_x: int,\n",
    "                       step_y: int,\n",
    "                       year = 2019,\n",
    "                       path: str = INPUT_FOLDER,\n",
    "                       s2: np.ndarray = None, \n",
    "                       dates: np.ndarray = None,\n",
    "                       interp: np.ndarray = None) -> None:\n",
    "    '''Wrapper function to interpolate clouds and temporal gaps, superresolve tiles,\n",
    "       calculate relevant indices, and save analysis-ready data to the output folder\n",
    "       \n",
    "       Parameters:\n",
    "        coord (tuple)\n",
    "        step_x (int):\n",
    "        step_y (int):\n",
    "        folder (str):\n",
    "\n",
    "       Returns:\n",
    "        None\n",
    "    '''\n",
    "    idx = str(step_y) + \"_\" + str(step_x)\n",
    "    x_vals, y_vals = make_folder_names(step_x, step_y)\n",
    "    s1 = hkl.load(f\"{path}/{year}/raw/s1/{idx}.hkl\")\n",
    "\n",
    "    s2 = evi(s2, verbose = True)\n",
    "    s2 = bi(s2, verbose = True)\n",
    "    s2 = msavi2(s2, verbose = True)\n",
    "    s2 = si(s2, verbose = True)\n",
    "    s2 = interpolate_na_vals(s2)\n",
    "\n",
    "    index = 0\n",
    "    tiles = tile_window(IMSIZE, IMSIZE, window_size = 142)\n",
    "    for t in tiles:\n",
    "        start_x, start_y = t[0], t[1]\n",
    "        end_x = start_x + t[2]\n",
    "        end_y = start_y + t[3]\n",
    "        subset = s2[:, start_x:end_x, start_y:end_y, :]\n",
    "        interp_tile = interp[:, start_x:end_x, start_y:end_y]\n",
    "        interp_tile = np.sum(interp_tile, axis = (1, 2))\n",
    "        print(f\"Interpolated amounts in this tile: {interp_tile}\")\n",
    "\n",
    "        dates_tile = np.copy(dates)\n",
    "        to_remove = np.argwhere(interp_tile > ((142*142) / 10)).flatten()\n",
    "        if len(to_remove) > 0:\n",
    "            print(f\"Removing {to_remove} dates due to interpolation\")\n",
    "            dates_tile = np.delete(dates_tile, to_remove)\n",
    "            subset = np.delete(subset, to_remove, 0)\n",
    "\n",
    "        missing_px = id_missing_px(subset)\n",
    "        if len(missing_px) > 0:\n",
    "            print(f\"Removing {missing_px} dates due to missing data\")\n",
    "            dates_tile = np.delete(dates_tile, missing_px)\n",
    "            subset = np.delete(subset, missing_px, 0)\n",
    "\n",
    "        to_remove = remove_missed_clouds(subset)\n",
    "        if len(to_remove) > 0:\n",
    "            subset = np.delete(subset, to_remove, axis = 0)\n",
    "            dates_tile = np.delete(dates_tile, to_remove)\n",
    "        print(f\"{len(to_remove)} missed cloudy images were removed: {to_remove}\")\n",
    "\n",
    "        subtile, _ = calculate_and_save_best_images(subset, dates_tile)\n",
    "        output = f\"{path}/{year}/processed/{y_vals[index]}/{x_vals[index]}.hkl\"\n",
    "\n",
    "        index += 1\n",
    "        sm = Smoother(lmbd = 800, size = subtile.shape[0], nbands = 14, dim = subtile.shape[1])\n",
    "        subtile = sm.interpolate_array(subtile)\n",
    "        subtile = np.concatenate([subtile, s1[:, start_x:end_x, start_y:end_y, :]], axis = -1)\n",
    "\n",
    "        output_folder = \"/\".join(output.split(\"/\")[:-1])\n",
    "        if not os.path.exists(os.path.realpath(output_folder)):\n",
    "            os.makedirs(os.path.realpath(output_folder))\n",
    "        subtile = np.float32(subtile)\n",
    "        subtile = np.reshape(subtile, (12, 2, 142, 142, subtile.shape[-1]))\n",
    "        subtile = np.mean(subtile, axis = (1))\n",
    "        print(f\"{index}: Writing {output}, {subtile.shape} shape\")\n",
    "        assert subtile.shape[1] == 142, f\"subtile shape is {subtile.shape}\"\n",
    "\n",
    "        hkl.dump(subtile, output, mode='w', compression='gzip')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 Function execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 2019 for malawi-rumphi\n",
      "0 0 False 0\n",
      "0 1 False 0\n",
      "0 2 False 0\n",
      "0 3 True 0\n",
      "Download 0/2500; X: 3 Y:0\n",
      "0 4 False 1\n",
      "0 5 False 1\n",
      "1 0 False 1\n",
      "1 1 True 1\n",
      "Download 1/2500; X: 1 Y:1\n",
      "1 2 True 2\n",
      "Download 2/2500; X: 2 Y:1\n",
      "1 3 True 3\n",
      "Download 3/2500; X: 3 Y:1\n",
      "1 4 True 4\n",
      "Download 4/2500; X: 4 Y:1\n",
      "1 5 True 5\n",
      "Download 5/2500; X: 5 Y:1\n",
      "2 0 False 6\n",
      "2 1 True 6\n",
      "Download 6/2500; X: 1 Y:2\n",
      "2 2 True 7\n",
      "Download 7/2500; X: 2 Y:2\n",
      "2 3 True 8\n",
      "Download 8/2500; X: 3 Y:2\n",
      "2 4 True 9\n",
      "Download 9/2500; X: 4 Y:2\n",
      "2 5 True 10\n",
      "Download 10/2500; X: 5 Y:2\n",
      "3 0 True 11\n",
      "Download 11/2500; X: 0 Y:3\n",
      "3 1 True 12\n",
      "Download 12/2500; X: 1 Y:3\n",
      "3 2 True 13\n",
      "Download 13/2500; X: 2 Y:3\n",
      "3 3 True 14\n",
      "Download 14/2500; X: 3 Y:3\n",
      "3 4 True 15\n",
      "Download 15/2500; X: 4 Y:3\n",
      "3 5 True 16\n",
      "Download 16/2500; X: 5 Y:3\n",
      "4 0 False 17\n",
      "4 1 True 17\n",
      "Download 17/2500; X: 1 Y:4\n",
      "4 2 True 18\n",
      "Download 18/2500; X: 2 Y:4\n",
      "4 3 True 19\n",
      "Download 19/2500; X: 3 Y:4\n",
      "4 4 True 20\n",
      "Download 20/2500; X: 4 Y:4\n",
      "4 5 True 21\n",
      "Download 21/2500; X: 5 Y:4\n",
      "Downloading clouds because ../project-monitoring/zambia/eastern/chama/2019/raw/clouds/clouds_4_5.hkl does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/john.brandt/anaconda3/envs/remote_sensing/lib/python3.6/site-packages/sentinelhub/data_request.py:47: SHDeprecationWarning: Parameter 'instance_id' is deprecated and will soon removed. Use parameter 'config' instead\n",
      "  category=SHDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original cloud image max is 255, (69, 40, 40)\n",
      "Cloud_probs used 0.1 processing units\n",
      "There are unique 8997 shadow L1C values\n",
      "The max shadows is 1.0\n",
      "Calculating shadows the new way took 0.4168720245361328\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e5a872bc1134623b07c96d1d329970f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=36), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/john.brandt/.local/lib/python3.6/site-packages/skimage/util/dtype.py:135: UserWarning: Possible precision loss when converting from int64 to float64\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shadows ((36, 646, 646)) used 14.3 processing units\n",
      "There are 0.0 pixels that always have shadows\n",
      "0, Dates: [-23], Min dist: 70, thresh: 0.06\n",
      "1, Dates: [47], Min dist: 45, thresh: 0.03\n",
      "2, Dates: [87], Min dist: 40, thresh: 0.15\n",
      "3, Dates: [ 92 107], Min dist: 30, thresh: 0.03\n",
      "4, Dates: [137 142], Min dist: 30, thresh: 0.03\n",
      "5, Dates: [162 172], Min dist: 20, thresh: 0.01\n",
      "6, Dates: [182 187 192 202], Min dist: 20, thresh: 0.01\n",
      "7, Dates: [212 217 232 237 242], Min dist: 30, thresh: 0.03\n",
      "8, Dates: [247 257 262 272], Min dist: 20, thresh: 0.01\n",
      "9, Dates: [277 287 302], Min dist: 25, thresh: 0.01\n",
      "10, Dates: [307 312], Min dist: 5, thresh: 0.01\n",
      "11, Dates: [347], Min dist: 35, thresh: 0.03\n",
      "[ 1  9 11 18 23 26 29 31]\n",
      "[0.06437807 0.09421158 0.01691045 0.07668769 0.0068629  0.\n",
      " 0.02741568 0.02630381]\n",
      "[ 1  9 11 18 23 26 29 31]\n",
      "Downloading S1 because ../project-monitoring/zambia/eastern/chama/2019/raw/s1/4_5.hkl does not exist\n",
      "The continent is: AF, and the sentinel 1 orbit is SENT\n",
      "Converting s1 to float32, with 65535 max and 64237 unique values\n",
      "Sentinel 1 used 31.8 PU for               30 out of 30 images\n",
      "[0.00545929 0.00402872 0.00336556 0.00386997 0.00210991 0.00107772\n",
      " 0.00253765 0.00222194 0.00065718 0.00059008 0.00077639 0.00052119\n",
      " 0.00033847 0.00027857 0.00023064 0.00028096 0.0002564  0.00025161\n",
      " 0.00026119 0.00029534 0.00029893 0.00036244 0.00034806 0.00034626\n",
      " 0.00035704 0.00036124 0.00036483 0.00137426 0.00350933 0.00417489]\n",
      "Maximum time distance: 24\n",
      "Downloading S2 because ../project-monitoring/zambia/eastern/chama/2019/raw/s2_10/4_5.hkl does not exist\n",
      "The cloud-free image dates are [-23, 47, 87, 92, 107, 137, 142, 162, 172, 182, 187, 192, 202, 212, 217, 232, 237, 242, 247, 257, 262, 272, 277, 287, 302, 307, 312, 347]\n",
      "Converting S2, 20m to float32, with 65535 max and 9937 unique values\n",
      "Original 20 meter bands size: (28, 323, 323, 6), using 22.3 PU\n",
      "Converting S2, 10m to float32, with 65535 max and 10001 unique values\n",
      "Shadows (28, 646, 646), clouds (28, 646, 646), S2, (28, 646, 646, 4), S2d, (28,)\n",
      "DEM used 3.2 processing units\n",
      "There are 176 unique DEM values\n",
      "Processing because folder 20/29.npy does not exist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/john.brandt/.local/lib/python3.6/site-packages/skimage/transform/_warps.py:833: UserWarning: Bi-quadratic interpolation behavior has changed due to a bug in the implementation of scikit-image. The new version now serves as a wrapper around SciPy's interpolation functions, which itself is not verified to be a correct implementation. Until skimage's implementation is fixed, we recommend to use bi-linear or bi-cubic interpolation instead.\n",
      "  warn(\"Bi-quadratic interpolation behavior has changed due \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, Dates: [-23], Min dist: 70, thresh: 0.06\n",
      "1, Dates: [47], Min dist: 70, thresh: 0.06\n",
      "2, Dates: [87], Min dist: 40, thresh: 0.15\n",
      "3, Dates: [ 92 107], Min dist: 30, thresh: 0.03\n",
      "4, Dates: [137 142], Min dist: 30, thresh: 0.03\n",
      "5, Dates: [162 172], Min dist: 20, thresh: 0.01\n",
      "6, Dates: [182 187 192 202], Min dist: 20, thresh: 0.01\n",
      "7, Dates: [212 217 232 237 242], Min dist: 30, thresh: 0.03\n",
      "8, Dates: [247 257 262 272], Min dist: 25, thresh: 0.01\n",
      "9, Dates: [277 287 302], Min dist: 25, thresh: 0.01\n",
      "10, Dates: [307 312], Min dist: 5, thresh: 0.01\n",
      "11, Dates: [347], Min dist: 35, thresh: 0.03\n",
      "[]\n",
      "(28, 646, 646, 10) (28, 646, 646) (28, 646, 646) (28,)\n",
      "[] cloudy and missing images removed\n",
      "[    0     0   713     0     0   675     0     0     0     0     0     1\n",
      "     0     0     0    33     0     0     0   647     0     3 16638     0\n",
      "     0    17     0     0]\n",
      "Interpolated 107914 px\n",
      "0 steps removed because of >50% interpolation rate\n",
      "The input array to superresolve is (28, 646, 646, 10)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a1bad8458674c03be1bdb9473b743cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=121), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Interpolated amounts in this tile: [    0.     0.  9757. 17406.     0. 10133.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.   926.     0.     0. 16342.     0.     0.]\n",
      "Removing [ 2  3  5 25] dates due to interpolation\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "0 missed cloudy images were removed: []\n",
      "Maximum time distance: 70\n",
      "(72, 142, 142, 14)\n",
      "1: Writing ../project-monitoring/zambia/eastern/chama//2019/processed/24/25.hkl, (12, 142, 142, 17) shape\n",
      "Interpolated amounts in this tile: [    0.     0.  8412. 16471.     0. 15919.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.  1166.     0.     0. 10201.     0.     0.]\n",
      "Removing [ 2  3  5 25] dates due to interpolation\n",
      "[0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "0 missed cloudy images were removed: []\n",
      "Maximum time distance: 70\n",
      "(72, 142, 142, 14)\n",
      "2: Writing ../project-monitoring/zambia/eastern/chama//2019/processed/24/26.hkl, (12, 142, 142, 17) shape\n",
      "Interpolated amounts in this tile: [    0.     0.     0. 11110.     0. 12502.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.  2578.     0.     0. 10102.     0.     0.]\n",
      "Removing [ 3  5 25] dates due to interpolation\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0 202   0   0   0   0]\n",
      "0 missed cloudy images were removed: []\n",
      "Maximum time distance: 70\n",
      "(72, 142, 142, 14)\n",
      "3: Writing ../project-monitoring/zambia/eastern/chama//2019/processed/24/27.hkl, (12, 142, 142, 17) shape\n",
      "Interpolated amounts in this tile: [   0.    0.    0. 1883.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0. 7643.    0.\n",
      "    0. 7696.    0.    0.]\n",
      "Removing [22 25] dates due to interpolation\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "0 missed cloudy images were removed: []\n",
      "Maximum time distance: 70\n",
      "(72, 142, 142, 14)\n",
      "4: Writing ../project-monitoring/zambia/eastern/chama//2019/processed/24/28.hkl, (12, 142, 142, 17) shape\n",
      "Interpolated amounts in this tile: [   0.    0. 9819.    0.    0. 1726.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0. 7105.    0. 2717.]\n",
      "Removing [ 2 25] dates due to interpolation\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   2 220]\n",
      "0 missed cloudy images were removed: []\n",
      "Maximum time distance: 70\n",
      "(72, 142, 142, 14)\n",
      "5: Writing ../project-monitoring/zambia/eastern/chama//2019/processed/24/29.hkl, (12, 142, 142, 17) shape\n",
      "Interpolated amounts in this tile: [    0.     0.   434. 13293.     0. 12678.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0. 17574.     0.     0.    59.     0.     0.]\n",
      "Removing [ 3  5 22] dates due to interpolation\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0]\n",
      "0 missed cloudy images were removed: []\n",
      "Maximum time distance: 70\n",
      "(72, 142, 142, 14)\n",
      "6: Writing ../project-monitoring/zambia/eastern/chama//2019/processed/23/25.hkl, (12, 142, 142, 17) shape\n",
      "Interpolated amounts in this tile: [    0.     0.  6261. 15610.     0.  6395.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0. 10976.     0.\n",
      "     0.     0. 18496.     0.     0.     0.     0.     0.]\n",
      "Removing [ 2  3  5 18 22] dates due to interpolation\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "0 missed cloudy images were removed: []\n",
      "Maximum time distance: 70\n",
      "(72, 142, 142, 14)\n",
      "7: Writing ../project-monitoring/zambia/eastern/chama//2019/processed/23/26.hkl, (12, 142, 142, 17) shape\n",
      "Interpolated amounts in this tile: [    0.     0. 13568.   616.     0. 11167.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.  6334.     0.\n",
      "     0.     0.  5896.     0.     0.     0.     0.     0.]\n",
      "Removing [ 2  5 18 22] dates due to interpolation\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "0 missed cloudy images were removed: []\n",
      "Maximum time distance: 70\n",
      "(72, 142, 142, 14)\n",
      "8: Writing ../project-monitoring/zambia/eastern/chama//2019/processed/23/27.hkl, (12, 142, 142, 17) shape\n",
      "Interpolated amounts in this tile: [   0.    0. 9690.    0.    0.    0.    0.    0.  280.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.  512.    0.\n",
      "    0.    0.    0. 2278.]\n",
      "Removing [2] dates due to interpolation\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   1   0 194]\n",
      "0 missed cloudy images were removed: []\n",
      "Maximum time distance: 70\n",
      "(72, 142, 142, 14)\n",
      "9: Writing ../project-monitoring/zambia/eastern/chama//2019/processed/23/28.hkl, (12, 142, 142, 17) shape\n",
      "Interpolated amounts in this tile: [   0.    0. 4631.    0.    0. 7183.    0.    0. 9158.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   34.    0.\n",
      "    0. 1095.    0.  202.]\n",
      "Removing [2 5 8] dates due to interpolation\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n",
      "0 missed cloudy images were removed: []\n",
      "Maximum time distance: 70\n",
      "(72, 142, 142, 14)\n",
      "10: Writing ../project-monitoring/zambia/eastern/chama//2019/processed/23/29.hkl, (12, 142, 142, 17) shape\n",
      "Interpolated amounts in this tile: [    0.     0.     0.  3457.     0.  9688.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.  5746.\n",
      "     0.     0. 18683.     0.     0.     0.     0.     0.]\n",
      "Removing [ 5 19 22] dates due to interpolation\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "0 missed cloudy images were removed: []\n",
      "Maximum time distance: 70\n",
      "(72, 142, 142, 14)\n",
      "11: Writing ../project-monitoring/zambia/eastern/chama//2019/processed/22/25.hkl, (12, 142, 142, 17) shape\n",
      "Interpolated amounts in this tile: [    0.     0.  1732.   301.     0.  9293.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.  1760.  2596.\n",
      "     0.     0. 15922.     0.     0.     0.     0.     0.]\n",
      "Removing [ 5 22] dates due to interpolation\n",
      "[ 0  0  4  0  0  0  0  0  0  0  0  0  0  0  0  0  0 12  0  0  0  0  0  0\n",
      "  0  0]\n",
      "0 missed cloudy images were removed: []\n",
      "Maximum time distance: 70\n",
      "(72, 142, 142, 14)\n",
      "12: Writing ../project-monitoring/zambia/eastern/chama//2019/processed/22/26.hkl, (12, 142, 142, 17) shape\n",
      "Interpolated amounts in this tile: [0.0000e+00 0.0000e+00 1.2024e+04 1.0000e+00 0.0000e+00 3.7310e+03\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 2.1970e+03 1.1600e+02 0.0000e+00 0.0000e+00 9.3570e+03 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      "Removing [ 2 22] dates due to interpolation\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "0 missed cloudy images were removed: []\n",
      "Maximum time distance: 70\n",
      "(72, 142, 142, 14)\n",
      "13: Writing ../project-monitoring/zambia/eastern/chama//2019/processed/22/27.hkl, (12, 142, 142, 17) shape\n",
      "Interpolated amounts in this tile: [0.0000e+00 0.0000e+00 1.8650e+04 5.7290e+03 0.0000e+00 4.8000e+01\n",
      " 0.0000e+00 0.0000e+00 1.8000e+01 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 1.0600e+02 0.0000e+00 0.0000e+00 0.0000e+00 1.7258e+04 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 1.0315e+04]\n",
      "Removing [ 2  3 22 27] dates due to interpolation\n",
      "[0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "0 missed cloudy images were removed: []\n",
      "Maximum time distance: 70\n",
      "(72, 142, 142, 14)\n",
      "14: Writing ../project-monitoring/zambia/eastern/chama//2019/processed/22/28.hkl, (12, 142, 142, 17) shape\n",
      "Interpolated amounts in this tile: [   0.    0. 1313. 9915.    0. 8679.    0.    0. 1915.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0. 7841.    0.    0.    0. 8616.    0.\n",
      "    0.  220.    0. 1161.]\n",
      "Removing [ 3  5 18 22] dates due to interpolation\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0  5  0  0 13  0 22]\n",
      "0 missed cloudy images were removed: []\n",
      "Maximum time distance: 70\n",
      "(72, 142, 142, 14)\n",
      "15: Writing ../project-monitoring/zambia/eastern/chama//2019/processed/22/29.hkl, (12, 142, 142, 17) shape\n",
      "Interpolated amounts in this tile: [    0.     0.     0.     0.     0. 15618.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.  5928.\n",
      "     0.     0.  8269.     0.     0.  4664.     0.     0.]\n",
      "Removing [ 5 19 22 25] dates due to interpolation\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "0 missed cloudy images were removed: []\n",
      "Maximum time distance: 70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72, 142, 142, 14)\n",
      "16: Writing ../project-monitoring/zambia/eastern/chama//2019/processed/21/25.hkl, (12, 142, 142, 17) shape\n",
      "Interpolated amounts in this tile: [    0.     0.  5664.     0.     0. 12249.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0. 10268.\n",
      "     0.     0.   526.     0.     0.  2190.     0.     0.]\n",
      "Removing [ 2  5 19] dates due to interpolation\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "0 missed cloudy images were removed: []\n",
      "Maximum time distance: 70\n",
      "(72, 142, 142, 14)\n",
      "17: Writing ../project-monitoring/zambia/eastern/chama//2019/processed/21/26.hkl, (12, 142, 142, 17) shape\n",
      "Interpolated amounts in this tile: [    0.     0. 12496.    40.     0. 13050.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.  2733.\n",
      "     0.     0. 12162.     0.     0.     0.     0.     0.]\n",
      "Removing [ 2  5 22] dates due to interpolation\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6 0 0 0 0 0 0 0]\n",
      "0 missed cloudy images were removed: []\n",
      "Maximum time distance: 70\n",
      "(72, 142, 142, 14)\n",
      "18: Writing ../project-monitoring/zambia/eastern/chama//2019/processed/21/27.hkl, (12, 142, 142, 17) shape\n",
      "Interpolated amounts in this tile: [0.0000e+00 0.0000e+00 1.1925e+04 1.6360e+03 0.0000e+00 2.5700e+03\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 1.8892e+04 0.0000e+00\n",
      " 0.0000e+00 1.2000e+01 0.0000e+00 0.0000e+00]\n",
      "Removing [ 2 22] dates due to interpolation\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "0 missed cloudy images were removed: []\n",
      "Maximum time distance: 70\n",
      "(72, 142, 142, 14)\n",
      "19: Writing ../project-monitoring/zambia/eastern/chama//2019/processed/21/28.hkl, (12, 142, 142, 17) shape\n",
      "Interpolated amounts in this tile: [   0.    0. 8630. 4977.    0. 4273.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.  467.    0.    0.    0. 7267.    0.\n",
      "    0. 8646.    0. 4286.]\n",
      "Removing [ 2  3  5 22 25 27] dates due to interpolation\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "0 missed cloudy images were removed: []\n",
      "Maximum time distance: 70\n",
      "(72, 142, 142, 14)\n",
      "20: Writing ../project-monitoring/zambia/eastern/chama//2019/processed/21/29.hkl, (12, 142, 142, 17) shape\n",
      "Interpolated amounts in this tile: [    0.     0.     0.  1997.     0. 20022.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0. 19966.     0.     0.]\n",
      "Removing [ 5 25] dates due to interpolation\n",
      "[ 0  0  0 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0]\n",
      "0 missed cloudy images were removed: []\n",
      "Maximum time distance: 70\n",
      "(72, 142, 142, 14)\n",
      "21: Writing ../project-monitoring/zambia/eastern/chama//2019/processed/20/25.hkl, (12, 142, 142, 17) shape\n",
      "Interpolated amounts in this tile: [    0.     0.  2488.     0.     0. 19902.  1768.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.  1960.     0.     0. 12291.     0.  5334.]\n",
      "Removing [ 5 25 27] dates due to interpolation\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0 758   0   0   0]\n",
      "0 missed cloudy images were removed: []\n",
      "Maximum time distance: 70\n",
      "(72, 142, 142, 14)\n",
      "22: Writing ../project-monitoring/zambia/eastern/chama//2019/processed/20/26.hkl, (12, 142, 142, 17) shape\n",
      "Interpolated amounts in this tile: [    0.     0. 13963.   252.     0. 19898.  2353.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0. 18934.     0.     0.   868.     0.  3465.]\n",
      "Removing [ 2  5 22] dates due to interpolation\n",
      "[ 0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 53  0\n",
      " 37]\n",
      "0 missed cloudy images were removed: []\n",
      "Maximum time distance: 70\n",
      "(72, 142, 142, 14)\n",
      "23: Writing ../project-monitoring/zambia/eastern/chama//2019/processed/20/27.hkl, (12, 142, 142, 17) shape\n",
      "Interpolated amounts in this tile: [    0.     0. 16288.  6941.     0.  6297.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0. 14629.     0.     0. 12578.     0.     0.]\n",
      "Removing [ 2  3  5 22 25] dates due to interpolation\n",
      "[0 0 0 0 0 0 0 0 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "0 missed cloudy images were removed: []\n",
      "Maximum time distance: 70\n",
      "(72, 142, 142, 14)\n",
      "24: Writing ../project-monitoring/zambia/eastern/chama//2019/processed/20/28.hkl, (12, 142, 142, 17) shape\n",
      "Interpolated amounts in this tile: [    0.     0. 19841.  3315.     0.     0.     0.     0.     0.     0.\n",
      "     0.     0.     0.     0.     0.     0.     0.     0.  3433.     0.\n",
      "     0.     0.   455.     0.     0. 18943.     0.  5197.]\n",
      "Removing [ 2 25 27] dates due to interpolation\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 25  0  0  0  0  0  0  0  0  0\n",
      "  0]\n",
      "0 missed cloudy images were removed: []\n",
      "Maximum time distance: 70\n",
      "(72, 142, 142, 14)\n",
      "25: Writing ../project-monitoring/zambia/eastern/chama//2019/processed/20/29.hkl, (12, 142, 142, 17) shape\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.ndimage import median_filter\n",
    "import time\n",
    "downloaded = 0\n",
    "\n",
    "if not os.path.exists(os.path.realpath(OUTPUT_FOLDER)):\n",
    "            os.makedirs(os.path.realpath(OUTPUT_FOLDER))\n",
    "        \n",
    "print(f\"Downloading {year} for {landscape}\")\n",
    "\n",
    "max_x = 50\n",
    "max_y = 50\n",
    "\n",
    "for y_tile in range(0, 5):\n",
    "    for x_tile in range(0, 6):\n",
    "        #contains = True\n",
    "        contains = check_contains(coords, x_tile, y_tile, OUTPUT_FOLDER)\n",
    "        print(y_tile, x_tile, contains, downloaded)\n",
    "        if contains:\n",
    "            print(f\"Download {downloaded}/{max_x*max_y}; X: {x_tile} Y:{y_tile}\")\n",
    "            downloaded += 1\n",
    "            download_large_tile(coord = coords, step_x = x_tile, step_y = y_tile)\n",
    "            s2, image_dates, interp = process_large_tile(coords, x_tile, y_tile)\n",
    "            if s2 is not None:\n",
    "                process_subtiles(coords, x_tile, y_tile, year = 2019,\n",
    "                                    s2 = s2, dates = image_dates, interp = interp)\n",
    "                print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_sensing",
   "language": "python",
   "name": "remote_sensing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
