{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/john.brandt/anaconda3/envs/remote_sensing/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from osgeo import ogr, osr\n",
    "from sentinelhub import WmsRequest, WcsRequest, MimeType, CRS, BBox, constants\n",
    "from s2cloudless import S2PixelCloudDetector, CloudMaskRequest\n",
    "import logging\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import os\n",
    "import yaml\n",
    "from sentinelhub import DataSource\n",
    "import scipy.sparse as sparse\n",
    "import scipy\n",
    "from scipy.sparse.linalg import splu\n",
    "from skimage.transform import resize\n",
    "from sentinelhub import CustomUrlParam\n",
    "from time import time as timer\n",
    "from time import sleep as sleep\n",
    "import multiprocessing\n",
    "import math\n",
    "import reverse_geocoder as rg\n",
    "import pycountry\n",
    "import pycountry_convert as pc\n",
    "import hickle as hkl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/john.brandt/anaconda3/envs/remote_sensing/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%run ../src/utils/slope.py\n",
    "%run ../src/utils/utils-bilinear.py\n",
    "%run ../src/dsen2/utils/DSen2Net.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ('2018-12-15', '2020-01-15')\n",
    "SIZE = 9*5\n",
    "IMSIZE = (SIZE * 14)+2\n",
    "\n",
    "cloud_detector = S2PixelCloudDetector(threshold=0.4, average_over=4, dilation_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../tile_data/dominican-rep-la-salvia/ (-70.462961, 18.872589)\n"
     ]
    }
   ],
   "source": [
    "landscapes = {\n",
    "    'ethiopia-tigray': (13.540810, 38.177220),\n",
    "    'kenya-makueni-2': (-1.817109, 37.44563),\n",
    "    'ghana': (9.259359, -0.83375),\n",
    "    'niger-koure': (13.18158, 2.478),\n",
    "    'cameroon-farnorth': (10.596, 14.2722),\n",
    "    'mexico-campeche': (18.232495, -92.1234215),\n",
    "    'malawi-rumphi': (-11.044, 33.818),\n",
    "    'ghana-sisala-east': (10.385, -1.765),\n",
    "    'ghana-west-mamprusi': (10.390084, -0.846330),\n",
    "    'ghana-kwahu': (6.518909, -0.826008),\n",
    "    'senegal-16b': (15.82585, -15.34166),\n",
    "    'india-kochi': (9.909, 76.254),\n",
    "    'india-sidhi': (24.0705, 81.607),\n",
    "    'brazil-esperito-santo': (-20.147, -40.837),\n",
    "    'brazil-paraiba': (-22.559943, -44.186629),\n",
    "    'brazil-goias': (-14.905595, -48.907399),\n",
    "    'colombia-talima': (4.179529, -74.889171),\n",
    "    'drc-kafubu': (-11.749636, 27.586622),\n",
    "    'thailand-khon-kaen': (15.709725, 102.546518),\n",
    "    'indonesia-west-java': (-6.721101, 108.280949),\n",
    "    'madagascar': (-18.960152, 47.469587),\n",
    "    'tanzania': (-6.272258, 36.679824),\n",
    "    'chile': (-36.431237, -71.872030),\n",
    "    'indonesia-jakarta': (-6.352580, 106.677072),\n",
    "    'caf-baboua': (5.765917, 14.791618),   \n",
    "    'honduras': (14.096664, -88.720304),\n",
    "    'nicaragua': (12.398014, -86.963042),\n",
    "    'china': (26.673679, 107.464231),\n",
    "    'australia-west': (-32.666762, 117.411197),\n",
    "    'mexico-sonora': (29.244288, -111.243230),\n",
    "    'south-africa': (-30.981698, 28.727301),\n",
    "    'maldonado-uraguay': (-34.629250, -55.004331),\n",
    "    'dominican-rep-la-salvia': (18.872589, -70.462961)\n",
    "}\n",
    "\n",
    "landscape = 'dominican-rep-la-salvia'\n",
    "\n",
    "#coords = (7.702058, -0.709011) # brong ahafo, bono east\n",
    "#coords = (7.398111, -1.269223) # cocoa\n",
    "#coords = (16.032170, -90.144511) # Guatemala\n",
    "#coords = (13.757749, -90.004949) # elsalvador imposible\n",
    "#coords = (13.727334, -90.015579) # elsalvador imposible2\n",
    "#coords = (13.933745, -84.690842) # Bonanza, Nicaragua\n",
    "\n",
    "OUTPUT_FOLDER = '../tile_data/{}/'.format(landscape)\n",
    "coords = landscapes[landscape]\n",
    "coords = (coords[1], coords[0])\n",
    "print(OUTPUT_FOLDER, coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "landscape_df = pd.DataFrame({'landscape': [x for x in landscapes.keys()], \n",
    "                             'latitude': [x[0] for x in landscapes.values()],\n",
    "                             'longitude': [x[1] for x in landscapes.values()]\n",
    "})\n",
    "\n",
    "landscape_df.to_csv(\"../data/latlongs/landscapes.csv\", index=False)\n",
    "print(len(landscape_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions (to be moved to a utils file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSG = CRS.WGS84\n",
    "GRID_SIZE_X = 1\n",
    "GRID_SIZE_Y = 1\n",
    "\n",
    "IMAGE_X = 14*GRID_SIZE_X\n",
    "IMAGE_Y = 14*GRID_SIZE_Y\n",
    "\n",
    "TEST_X = 5\n",
    "TEST_Y = 5\n",
    "\n",
    "with open(\"../config.yaml\", 'r') as stream:\n",
    "    key = (yaml.safe_load(stream))\n",
    "    API_KEY = key['key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These arrays are for smoothly overlapping the cloud and shadow interpolation\n",
    "c_arr = np.array([[1, 1, 1, 1, 1,],\n",
    "                  [1, 2, 2, 2, 1,],\n",
    "                  [1, 2, 3, 2, 1,],\n",
    "                  [1, 2, 2, 2, 1,],\n",
    "                  [1, 1, 1, 1, 1,],])\n",
    "                  \n",
    "c_arr = c_arr / 3\n",
    "o_arr = 1 - c_arr\n",
    "c_arr = np.tile(c_arr[:, :, np.newaxis], (1, 1, 10))\n",
    "o_arr = np.tile(o_arr[:, :, np.newaxis], (1, 1, 10))\n",
    "\n",
    "def convertCoords(xy, src='', targ=''):\n",
    "\n",
    "    srcproj = osr.SpatialReference()\n",
    "    srcproj.ImportFromEPSG(src)\n",
    "    targproj = osr.SpatialReference()\n",
    "    if isinstance(targ, str):\n",
    "        targproj.ImportFromProj4(targ)\n",
    "    else:\n",
    "        targproj.ImportFromEPSG(targ)\n",
    "    transform = osr.CoordinateTransformation(srcproj, targproj)\n",
    "\n",
    "    pt = ogr.Geometry(ogr.wkbPoint)\n",
    "    pt.AddPoint(xy[0], xy[1])\n",
    "    pt.Transform(transform)\n",
    "    return([pt.GetX(), pt.GetY()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_epsg(points):\n",
    "    lon, lat = points[0], points[1]\n",
    "    print(lon, lat)\n",
    "    utm_band = str((math.floor((lon + 180) / 6 ) % 60) + 1)\n",
    "    if len(utm_band) == 1:\n",
    "        utm_band = '0'+utm_band\n",
    "    if lat >= 0:\n",
    "        epsg_code = '326' + utm_band\n",
    "    else:\n",
    "        epsg_code = '327' + utm_band\n",
    "    return int(epsg_code)\n",
    "\n",
    "def PolygonArea(corners):\n",
    "    n = len(corners) # of corners\n",
    "    area = 0.0\n",
    "    for i in range(n):\n",
    "        j = (i + 1) % n\n",
    "        area += corners[i][0] * corners[j][1]\n",
    "        area -= corners[j][0] * corners[i][1]\n",
    "    area = abs(area)\n",
    "    return area\n",
    "    \n",
    "def offset_x(coord, offset):\n",
    "    ''' Converts a WGS 84 to UTM, adds meters, and converts back'''\n",
    "    epsg = calculate_epsg(coord)\n",
    "    coord = convertCoords(coord, 4326, epsg)\n",
    "    coord[0] += offset\n",
    "    coord = convertCoords(coord, epsg, 4326)\n",
    "    return coord\n",
    "    \n",
    "def offset_y(coord, offset):\n",
    "    ''' Converts a WGS 84 to UTM, adds meters, and converts back'''\n",
    "    epsg = calculate_epsg(coord)\n",
    "    coord = convertCoords(coord, 4326, epsg)\n",
    "    coord[1] += offset\n",
    "    coord = convertCoords(coord, epsg, 4326)\n",
    "    return coord\n",
    "\n",
    "def calculate_area(bbx):\n",
    "    '''\n",
    "    Calculates the area in ha of a [(min_x, min_y), (max_x, max_y)] bbx\n",
    "    '''\n",
    "\n",
    "    epsg = calculate_epsg(bbx[0])\n",
    "    \n",
    "    mins = convertCoords(bbx[0], 4326, epsg)\n",
    "    maxs = convertCoords(bbx[1], 4326, epsg)\n",
    "    area = PolygonArea([(mins[0], mins[1]), # BL\n",
    "                        (mins[0], maxs[1]), # BR\n",
    "                        (maxs[0], mins[1]), # TL\n",
    "                        (maxs[0], mins[1]) # TR\n",
    "                        ])\n",
    "    hectares = math.floor(area / 1e4)\n",
    "    print(hectares)\n",
    "    \n",
    "\n",
    "def calculate_bbx(coord, step_x, step_y, expansion, multiplier = 1.):\n",
    "    ''' Calculates the four corners of a bounding box of step_x * step_y offset from coord'''\n",
    "    coord_bl = np.copy(coord)\n",
    "    coord1 = offset_x(coord_bl, 6300*step_x - expansion)\n",
    "    coord1 = offset_y(coord1 , 6300*step_y - expansion)\n",
    "    \n",
    "    coord_tr = np.copy(coord)\n",
    "    coord2 = offset_x(coord_tr, 6300*(step_x + multiplier) + expansion)\n",
    "    coord2 = offset_y(coord2, 6300*(step_y + multiplier) + expansion)\n",
    "    bbx = (coord2, coord1)\n",
    "    return bbx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_function(func, n_tries = 5, *args, **kwargs):\n",
    "    for try_ in range(0, n_tries):\n",
    "        try:\n",
    "            returns = func(*args, **kwargs)\n",
    "        except Exception:\n",
    "            print(\"Trying again in 20 seconds\")\n",
    "            sleep(20)\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_clouds(bbox, epsg = EPSG, dates = dates):\n",
    "    '''\n",
    "    - Download the CLOUD_DETECTION layer from sentinel-hub\n",
    "    - Process cloud probabiiliity\n",
    "    - Return cloud probability map\n",
    "    '''\n",
    "    for try_ in range(0, 5):\n",
    "        try:\n",
    "            box = BBox(bbox, crs = epsg)\n",
    "            cloud_request = WmsRequest(\n",
    "                layer='CLOUD_DETECTION',\n",
    "                bbox=box,\n",
    "                time=dates,\n",
    "                width=(5*9*14)+2,\n",
    "                height=(5*9*14)+2,\n",
    "                image_format = MimeType.TIFF_d16,\n",
    "                maxcc=0.7,\n",
    "                instance_id=API_KEY,\n",
    "                custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "                time_difference=datetime.timedelta(hours=48),\n",
    "            )\n",
    "\n",
    "            cloud_img = cloud_request.get_data()\n",
    "            cloud_img = np.array(cloud_img) / 65535\n",
    "            cloud_probs = cloud_detector.get_cloud_probability_maps(np.array(cloud_img))\n",
    "            shadows = mcm_shadow_mask(np.array(cloud_img), cloud_probs)\n",
    "            print(\"Cloud probs: {}\".format(cloud_probs.shape))\n",
    "            return cloud_img, cloud_probs, shadows\n",
    "        except Exception as e:\n",
    "            #logging.fatal(e, exc_info=True)\n",
    "            sleep(30)\n",
    "    \n",
    "    \n",
    "def download_dem(bbox, epsg = EPSG):\n",
    "    #@valid\n",
    "    for try_ in range(0, 5):\n",
    "        try:\n",
    "            box = BBox(bbox, crs = epsg)\n",
    "            dem_s = (630)+4\n",
    "            dem_request = WmsRequest(data_source=DataSource.DEM,\n",
    "                                 layer='DEM',\n",
    "                                 bbox=box,\n",
    "                                 width=dem_s,\n",
    "                                 height=dem_s,\n",
    "                                 instance_id=API_KEY,\n",
    "                                 image_format=MimeType.TIFF_d32f,\n",
    "                                 custom_url_params={CustomUrlParam.SHOWLOGO: False})\n",
    "            dem_image = dem_request.get_data()[0]\n",
    "            dem_image = calcSlope(dem_image.reshape((1, dem_s, dem_s)),\n",
    "                          np.full((dem_s, dem_s), 10), np.full((dem_s, dem_s), 10), zScale = 1, minSlope = 0.02)\n",
    "            dem_image = dem_image.reshape((dem_s,dem_s, 1))\n",
    "            dem_image = dem_image[1:dem_s-1, 1:dem_s-1, :]\n",
    "            return dem_image #/ np.max(dem_image)\n",
    "        except Exception as e:\n",
    "            #logging.fatal(e, exc_info=True)\n",
    "            sleep((try_+1)*30)\n",
    "            pass\n",
    "\n",
    "def download_layer(bbox, epsg = EPSG, dates = dates, year = 2019):\n",
    "    #for try_ in range(5):\n",
    "        #try:\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    image_request = WcsRequest(\n",
    "            layer='L2A20',\n",
    "            bbox=box,\n",
    "            time=dates,\n",
    "            image_format = MimeType.TIFF_d16,\n",
    "            maxcc=0.7,\n",
    "            resx='10m', resy='10m',\n",
    "            instance_id=API_KEY,\n",
    "            custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'NEAREST',\n",
    "                                constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "            time_difference=datetime.timedelta(hours=48),\n",
    "        )\n",
    "    img_bands = image_request.get_data()\n",
    "    img_20 = np.stack(img_bands)\n",
    "    print(\"Original size: {}\".format(img_20.shape))\n",
    "    img_20 = resize(img_20, (img_20.shape[0], 632, 632, img_20.shape[-1]), order = 0)\n",
    "\n",
    "    image_request = WcsRequest(\n",
    "            layer='L2A10',\n",
    "            bbox=box,\n",
    "            time=dates,\n",
    "            image_format = MimeType.TIFF_d32f,\n",
    "            maxcc=0.7,\n",
    "            resx='10m', resy='10m',\n",
    "            instance_id=API_KEY,\n",
    "            custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'BICUBIC',\n",
    "                                constants.CustomUrlParam.UPSAMPLING: 'BICUBIC'},\n",
    "            time_difference=datetime.timedelta(hours=48),\n",
    "    )\n",
    "\n",
    "    img_bands = image_request.get_data()\n",
    "    img_10 = np.stack(img_bands)\n",
    "    print(\"Original 10 size: {}\".format(img_10.shape))\n",
    "    img_10 = resize(img_10, (img_10.shape[0], 632, 632, img_10.shape[-1]), order = 0)\n",
    "\n",
    "    img = np.concatenate([img_10, img_20], axis = -1)\n",
    "\n",
    "    image_dates = []\n",
    "    for date in image_request.get_dates():\n",
    "        if date.year == year - 1:\n",
    "            image_dates.append(-365 + starting_days[(date.month-1)] + date.day)\n",
    "        if date.year == year:\n",
    "            image_dates.append(starting_days[(date.month-1)] + date.day)\n",
    "        if date.year == year + 1:\n",
    "            image_dates.append(365 + starting_days[(date.month-1)]+date.day)\n",
    "    image_dates = np.array(image_dates)\n",
    "\n",
    "    return img, image_dates#, shadows, shadow_steps\n",
    "\n",
    "        #except Exception as e:\n",
    "            #logging.fatal(e, exc_info=True)\n",
    "            #sleep((try_+1)*30)\n",
    "        \n",
    "        \n",
    "        \n",
    "def download_sentinel_1(bbox, epsg = EPSG, imsize = 632, \n",
    "                        dates = dates, layer = \"SENT\", year = 2019):\n",
    "    #for try_ in range(5):\n",
    "        #try:\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    image_request = WcsRequest(\n",
    "            layer=layer,\n",
    "            bbox=box,\n",
    "            time=dates,\n",
    "            image_format = MimeType.TIFF_d16,\n",
    "            maxcc=1.0,\n",
    "            resx='5m', resy='5m',\n",
    "            instance_id=API_KEY,\n",
    "            custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'NEAREST',\n",
    "                                constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "            time_difference=datetime.timedelta(hours=48),\n",
    "        )\n",
    "    img_bands = image_request.get_data()\n",
    "    s1 = np.stack(img_bands)\n",
    "    s1 = resize(s1, (s1.shape[0], imsize*2, imsize*2, s1.shape[-1]), order = 0)\n",
    "    s1 = np.reshape(s1, (s1.shape[0], s1.shape[1]//2, 2, s1.shape[2] // 2, 2, s1.shape[-1]))\n",
    "    s1 = np.mean(s1, (2, 4))\n",
    "    #s1 = s1[:, 8:24, 8:24, :]\n",
    "\n",
    "    image_dates = []\n",
    "    for date in image_request.get_dates():\n",
    "        if date.year == year - 1:\n",
    "            image_dates.append(-365 + starting_days[(date.month-1)] + date.day)\n",
    "        if date.year == year:\n",
    "            image_dates.append(starting_days[(date.month-1)] + date.day)\n",
    "        if date.year == year + 1:\n",
    "            image_dates.append(365 + starting_days[(date.month-1)]+date.day)\n",
    "    image_dates = np.array(image_dates)\n",
    "    s1c = np.copy(s1)\n",
    "    s1c[np.where(s1c < 1.)] = 0\n",
    "    n_pix_oob = np.sum(s1c, axis = (1, 2, 3))\n",
    "    print(\"OOB\", n_pix_oob)\n",
    "    #to_remove = np.argwhere(np.max(s1, (1, 2, 3)) == 1.).flatten()\n",
    "    to_remove = np.argwhere(n_pix_oob > (imsize*2*imsize*2)/50)\n",
    "    s1 = np.delete(s1, to_remove, 0)\n",
    "    print(np.max(s1, (1, 2, 3)))\n",
    "    image_dates = np.delete(image_dates, to_remove)\n",
    "    return s1, image_dates\n",
    "\n",
    "        #except Exception as e:\n",
    "            #logging.fatal(e, exc_info=True)\n",
    "            #sleep((try_+1)*30)\n",
    "\n",
    "def identify_s1_layer(coords):\n",
    "    results = rg.search(coords)\n",
    "    country = results[-1]['cc']\n",
    "    continent_name = pc.country_alpha2_to_continent_code(country)\n",
    "    if continent_name in ['AF', 'OC']:\n",
    "        layer = \"SENT\"\n",
    "    if continent_name in ['SA']:\n",
    "        if coords[0] > -7.11:\n",
    "            layer = \"SENT\"\n",
    "        else:\n",
    "            layer = \"SENT_DESC\"\n",
    "    if continent_name in ['AS']:\n",
    "        if coords[0] > 23.3:\n",
    "            layer = \"SENT\"\n",
    "        else:\n",
    "            layer = \"SENT_DESC\"\n",
    "    if continent_name in ['NA']:\n",
    "        layer = \"SENT_DESC\"\n",
    "    print(continent_name)\n",
    "    print(layer)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud and shadow removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_cloud_and_shadows(tiles, c_probs, shadows, image_dates):\n",
    "    '''\n",
    "    - Iterate through a window size over each time step\n",
    "    - Identify whether or not there are clouds and shadows there\n",
    "    - If there are, interpolate the 5*5 window with clean imagery for all bands\n",
    "    '''\n",
    "    wsize = 5\n",
    "    c_probs = c_probs - np.min(c_probs, axis = 0)\n",
    "    c_probs[np.where(c_probs > 0.33)] = 1.\n",
    "    c_probs[np.where(c_probs < 0.33)] = 0.\n",
    "    c_probs = np.reshape(c_probs, (c_probs.shape[0], 632//8, 8, 632//8, 8))\n",
    "    c_probs = np.sum(c_probs, (2, 4))\n",
    "    c_probs = resize(c_probs, (c_probs.shape[0], 632, 632), 0)\n",
    "    c_probs[np.where(c_probs < 16)] = 0\n",
    "    c_probs[np.where(c_probs >= 16)] = 1\n",
    "    secondary_c_probs = np.copy(c_probs)\n",
    "    c_probs += shadows\n",
    "    c_probs[np.where(c_probs >= 1.)] = 1.\n",
    "    number_interpolated = 0\n",
    "    for cval in tnrange(0, IMSIZE - 4, 2):\n",
    "        for rval in range(0, IMSIZE - 4, 2):\n",
    "            subs = c_probs[:, cval:cval + wsize, rval:rval+wsize]\n",
    "            sums = np.sum(subs, axis = (1, 2))\n",
    "            satisfactory = [x for x in range(c_probs.shape[0]) if sums[x] < 8]\n",
    "            satisfactory = np.array(satisfactory)\n",
    "            for date in range(0, tiles.shape[0]):\n",
    "                if np.sum(subs[date, :, :]) > 8:\n",
    "                    number_interpolated += 1\n",
    "                    before, after = calculate_proximal_steps(date, satisfactory)\n",
    "                    before = date + before\n",
    "                    after = date + after\n",
    "                    bef = tiles[before, cval:cval+wsize, rval:rval+wsize, : ]\n",
    "                    aft = tiles[after, cval:cval+wsize, rval:rval+wsize, : ]\n",
    "                    before = image_dates[before]\n",
    "                    after = image_dates[after]\n",
    "                    before_diff = abs(image_dates[date] - before)\n",
    "                    after_diff = abs(image_dates[date] - after)\n",
    "                    bef_wt = 1 - before_diff / (before_diff + after_diff)\n",
    "                    aft_wt = 1 - bef_wt\n",
    "                    candidate = bef_wt*bef + aft_wt*aft\n",
    "                    candidate = candidate*c_arr + tiles[date, cval:cval+wsize, rval:rval+wsize, : ]*o_arr\n",
    "                    tiles[date, cval:cval+wsize, rval:rval+wsize, : ] = candidate \n",
    "    print(\"A total of {} pixels were interpolated\".format(number_interpolated))\n",
    "    return tiles, c_probs, secondary_c_probs\n",
    "\n",
    "def remove_missed_clouds(img):\n",
    "    iqr = np.percentile(img[:, :, :, 3].flatten(), 75) - np.percentile(img[:, :, :, 3].flatten(), 25)\n",
    "    thresh_t = np.percentile(img[:, :, :, 3].flatten(), 75) + iqr*2\n",
    "    thresh_b = np.percentile(img[:, :, :, 3].flatten(), 25) - iqr*2\n",
    "    diffs_fw = np.diff(img, 1, axis = 0)\n",
    "    diffs_fw = np.mean(diffs_fw, axis = (1, 2, 3))\n",
    "    diffs_fw = np.array([0] + list(diffs_fw))\n",
    "    diffs_bw = np.diff(np.flip(img, 0), 1, axis = 0)\n",
    "    diffs_bw = np.flip(np.mean(diffs_bw, axis = (1, 2, 3)))\n",
    "    diffs_bw = np.array(list(diffs_bw) + [0])\n",
    "    diffs = abs(diffs_fw - diffs_bw) * 100 # 3, -3 -> 6, -3, 3 -> 6, -3, -3\n",
    "    #diffs = [int(x) for x in diffs]\n",
    "    outlier_percs = []\n",
    "    for step in range(img.shape[0]):\n",
    "        bottom = len(np.argwhere(img[step, :, :, 3].flatten() > thresh_t))\n",
    "        top = len(np.argwhere(img[step, :, :, 3].flatten() < thresh_b))\n",
    "        p = 100* ((bottom + top) / (IMSIZE*IMSIZE))\n",
    "        outlier_percs.append(p)\n",
    "    to_remove = np.argwhere(np.array(outlier_percs) > 20)\n",
    "    return to_remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagonals = np.zeros(2*2+1)\n",
    "diagonals[2] = 1.\n",
    "for i in range(2):\n",
    "    diff = diagonals[:-1] - diagonals[1:]\n",
    "    diagonals = diff\n",
    "offsets = np.arange(2+1)\n",
    "shape = (70, 72)\n",
    "\n",
    "def smooth(y, lmbd = 800, diagonals = diagonals, offsets = offsets, shape = shape, d = 2):\n",
    "    ''' \n",
    "    Apply whittaker smoothing to a 1-dimensional array, returning a 1-dimensional array\n",
    "    '''\n",
    "    E = sparse.eye(72, format = 'csc')\n",
    "    D = scipy.sparse.diags(diagonals, offsets, shape)\n",
    "    coefmat = E + lmbd * D.conj().T.dot(D)\n",
    "    z = splu(coefmat).solve(y)\n",
    "    return z\n",
    "\n",
    "def calculate_and_save_best_images(img_bands, image_dates):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "\n",
    "    biweekly_dates = [day for day in range(0, 360, 5)] # ideal imagery dates are every 15 days\n",
    "    \n",
    "    # Identify the dates where there is < 20% cloud cover\n",
    "    #satisfactory_ids = list(np.argwhere(np.array(means) < 4.).reshape(-1, )) \n",
    "    satisfactory_ids = [x for x in range(0, img_bands.shape[0])]\n",
    "    satisfactory_dates = [value for idx, value in enumerate(image_dates) if idx in satisfactory_ids]\n",
    "    \n",
    "    \n",
    "    selected_images = {}\n",
    "    for i in biweekly_dates:\n",
    "        distances = [abs(date - i) for date in satisfactory_dates]\n",
    "        closest = np.min(distances)\n",
    "        closest_id = np.argmin(distances)\n",
    "        # If there is imagery within 8 days, select it\n",
    "        if closest < 8:\n",
    "            date = satisfactory_dates[closest_id]\n",
    "            image_idx = int(np.argwhere(np.array(image_dates) == date)[0])\n",
    "            selected_images[i] = {'image_date': [date], 'image_ratio': [1], 'image_idx': [image_idx]}\n",
    "        # If there is not imagery within 8 days, look for the closest above and below imagery\n",
    "        else:\n",
    "            distances = np.array([(date - i) for date in satisfactory_dates])\n",
    "            # Number of days above and below the selected date of the nearest clean imagery\n",
    "            above = distances[np.where(distances < 0, distances, -np.inf).argmax()]\n",
    "            below = distances[np.where(distances > 0, distances, np.inf).argmin()]\n",
    "            if abs(above) > 240: # If date is the last date, occassionally argmax would set above to - number\n",
    "                above = below\n",
    "            if abs(below) > 240:\n",
    "                below = above\n",
    "            if above != below:\n",
    "                below_ratio = above / (above - below)\n",
    "                above_ratio = 1 - below_ratio\n",
    "            else:\n",
    "                above_ratio = below_ratio = 0.5\n",
    "                \n",
    "            # Extract the image date and imagery index for the above and below values\n",
    "            above_date = i + above\n",
    "            above_image_idx = int(np.argwhere(np.array(image_dates) == above_date)[0])\n",
    "            \n",
    "            below_date = i + below\n",
    "            below_image_idx = int(np.argwhere(np.array(image_dates) == below_date)[0])\n",
    "            \n",
    "            selected_images[i] = {'image_date': [above_date, below_date], 'image_ratio': [above_ratio, below_ratio],\n",
    "                                 'image_idx': [above_image_idx, below_image_idx]}\n",
    "                            \n",
    "    max_distance = 0\n",
    "    \n",
    "    for i in selected_images.keys():\n",
    "        #print(i, selected_images[i])\n",
    "        if len(selected_images[i]['image_date']) == 2:\n",
    "            dist = selected_images[i]['image_date'][1] - selected_images[i]['image_date'][0]\n",
    "            if dist > max_distance:\n",
    "                max_distance = dist\n",
    "    \n",
    "    print(\"Maximum time distance: {}\".format(max_distance))\n",
    "        \n",
    "    keep_steps = []\n",
    "    for i in selected_images.keys():\n",
    "        info = selected_images[i]\n",
    "        if len(info['image_idx']) == 1:\n",
    "            step = img_bands[info['image_idx'][0]]\n",
    "        if len(info['image_idx']) == 2:\n",
    "            step1 = img_bands[info['image_idx'][0]] * 0.5#info['image_ratio'][0]\n",
    "            step2 = img_bands[info['image_idx'][1]] * 0.5 #info['image_ratio'][1]\n",
    "            step = step1 + step2\n",
    "        keep_steps.append(step)\n",
    "        \n",
    "    keep_steps = np.stack(keep_steps)\n",
    "    return keep_steps\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiling and coordinate selection functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbolic Model Created.\n"
     ]
    }
   ],
   "source": [
    "MDL_PATH = \"../src/dsen2/models/\"\n",
    "\n",
    "input_shape = ((4, None, None), (6, None, None))\n",
    "model = s2model(input_shape, num_layers=6, feature_size=128)\n",
    "predict_file = MDL_PATH+'s2_032_lr_1e-04.hdf5'\n",
    "print('Symbolic Model Created.')\n",
    "\n",
    "model.load_weights(predict_file)\n",
    "\n",
    "def DSen2(d10, d20):\n",
    "    test = [d10, d20]\n",
    "    input_shape = ((4, None, None), (6, None, None))\n",
    "    prediction = _predict(test, input_shape, deep=False)\n",
    "    #prediction *= 5\n",
    "    return prediction\n",
    "\n",
    "def _predict(test, input_shape, model = model, deep=False, run_60=False):\n",
    "    \n",
    "    print(\"Predicting using file: {}\".format(predict_file))\n",
    "    prediction = model.predict(test, verbose=1)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_per_month = [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30]\n",
    "starting_days = np.cumsum(days_per_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_proximal_steps(date, satisfactory):\n",
    "    arg_before = None\n",
    "    arg_after = None\n",
    "    if date > 0:\n",
    "        idx_before = satisfactory - date\n",
    "        arg_before = idx_before[np.where(idx_before < 0, idx_before, -np.inf).argmax()]\n",
    "    if date < np.max(satisfactory):\n",
    "        idx_after = satisfactory - date\n",
    "        arg_after = idx_after[np.where(idx_after > 0, idx_after, np.inf).argmin()]\n",
    "    if not arg_after and not arg_before:\n",
    "        arg_after = date\n",
    "        arg_before = date\n",
    "    if not arg_after:\n",
    "        arg_after = arg_before\n",
    "    if not arg_before:\n",
    "        arg_before = arg_after\n",
    "    #print(arg_before, date, arg_after)\n",
    "    return arg_before, arg_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_array(arr):\n",
    "    order = arr.argsort()\n",
    "    ranks = order.argsort()\n",
    "    return ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def mcm_shadow_mask(arr, c_probs):\n",
    "    #' From \"Cloud and cloud shadow masking for Sentinel-2 using multi-\n",
    "    #         temporal images in global area. Candra et al. 2020.\"\n",
    "    #'[B02,B03,B04, B08, B05,B06,B07, B8A,B11,B12]\n",
    "    #'[ 0 , 1 , 2 , 3  , 4  , 5,  6   7  , 8  , 9]'\n",
    "    mean_c_probs = np.mean(c_probs, axis = (1, 2))\n",
    "    cloudy_steps = np.argwhere(mean_c_probs > 0.25)\n",
    "    images_clean = np.delete(arr, cloudy_steps, 0)\n",
    "    cloud_ranks = rank_array(mean_c_probs)\n",
    "    diffs = abs(np.sum(arr - np.mean(images_clean, axis = 0), axis = (1, 2, 3)))\n",
    "    diff_ranks = rank_array(diffs)\n",
    "    overall_rank = diff_ranks + cloud_ranks\n",
    "    reference_idx = np.argmin(overall_rank)\n",
    "    ri = arr[reference_idx]\n",
    "    print(reference_idx)\n",
    "    \n",
    "    nir_means = np.mean(arr[:, :, :, 4], axis = (0))\n",
    "    \n",
    "    shadows = np.zeros((arr.shape[0], 632, 632))    \n",
    "    # Candra et al. 2020\n",
    "    \n",
    "    for time in tnrange(arr.shape[0]):\n",
    "        for x in range(arr.shape[1]):\n",
    "            for y in range(arr.shape[2]):\n",
    "                ti_slice = arr[time, x, y]\n",
    "                ri_slice = ri[x, y]\n",
    "                deltab2 = ti_slice[1] - ri_slice[1]\n",
    "                deltab3 = ti_slice[2] - ri_slice[2]\n",
    "                deltab4 = ti_slice[3] - ri_slice[3]\n",
    "                deltab8a = ti_slice[6] - ri_slice[6]\n",
    "                deltab11 = ti_slice[8] - ri_slice[8]\n",
    "\n",
    "                if deltab2 < 0.1:\n",
    "                    if deltab3 < 0.08:\n",
    "                        if deltab4 < 0.08:\n",
    "                            if deltab8a < -0.04:\n",
    "                                if deltab11 < -0.04:\n",
    "                                    if ti_slice[1] < 0.0950:\n",
    "                                        shadows[time, x, y] = 1.\n",
    "                                                       \n",
    "                            \n",
    "    # Remove shadows if cannot coreference a cloud\n",
    "    shadow_large = np.reshape(shadows, (shadows.shape[0], 79, 8, 79, 8))\n",
    "    shadow_large = np.sum(shadow_large, axis = (2, 4))\n",
    "    \n",
    "    cloud_large = np.copy(c_probs)\n",
    "    cloud_large[np.where(c_probs > 0.33)] = 1.\n",
    "    cloud_large[np.where(c_probs < 0.33)] = 0.\n",
    "    cloud_large = np.reshape(cloud_large, (shadows.shape[0], 79, 8, 79, 8))\n",
    "    cloud_large = np.sum(cloud_large, axis = (2, 4))\n",
    "    for time in tnrange(shadow_large.shape[0]):\n",
    "        for x in range(shadow_large.shape[1]):\n",
    "            x_low = np.max([x - 8, 0])\n",
    "            x_high = np.min([x + 8, shadow_large.shape[1] - 1])\n",
    "            for y in range(shadow_large.shape[2]):\n",
    "                y_low = np.max([y - 8, 0])\n",
    "                y_high = np.min([y + 8, shadow_large.shape[1] - 1])\n",
    "                if shadow_large[time, x, y] < 8:\n",
    "                    shadow_large[time, x, y] = 0.\n",
    "                if shadow_large[time, x, y] >= 8:\n",
    "                    shadow_large[time, x, y] = 1.\n",
    "                c_prob_window = cloud_large[time, x_low:x_high, y_low:y_high]\n",
    "                if np.max(c_prob_window) < 16:\n",
    "                    shadow_large[time, x, y] = 0.\n",
    "                    \n",
    "    shadow_large = resize(shadow_large, (shadow_large.shape[0], 632, 632), order = 0)\n",
    "    shadows *= shadow_large\n",
    "    \n",
    "    # Go through and aggregate the shadow map to an 80m grid, and extend it one grid size around\n",
    "    # any positive ID\n",
    "    \n",
    "    shadows = np.reshape(shadows, (shadows.shape[0], 79, 8, 79, 8))\n",
    "    shadows = np.sum(shadows, axis = (2, 4))\n",
    "    shadows[np.where(shadows < 12)] = 0.\n",
    "    shadows[np.where(shadows >= 12)] = 1.\n",
    "    \n",
    "    shadows = resize(shadows, (shadows.shape[0], 632, 632), order = 0)\n",
    "    shadows = np.reshape(shadows, (shadows.shape[0], 632//4, 4, 632//4, 4))\n",
    "    shadows = np.max(shadows, (2, 4))\n",
    "    \n",
    "    shadows_new = np.zeros_like(shadows)\n",
    "    for time in range(shadows.shape[0]):\n",
    "        for x in range(shadows.shape[1]):\n",
    "            for y in range(shadows.shape[2]):\n",
    "                if shadows[time, x, y] == 1:\n",
    "                    min_x = np.max([x - 1, 0])\n",
    "                    max_x = np.min([x + 2, 157])\n",
    "                    min_y = np.max([y - 1, 0])\n",
    "                    max_y = np.min([y + 2, 157])\n",
    "                    for x_idx in range(min_x, max_x):\n",
    "                        for y_idx in range(min_y, max_y):\n",
    "                            shadows_new[time, x_idx, y_idx] = 1.\n",
    "    shadows_new = resize(shadows_new, (shadows.shape[0], 632, 632), order = 0)\n",
    "    return shadows_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_and_make_dirs(dirs):\n",
    "    if not os.path.exists(os.path.realpath(dirs)):\n",
    "        os.makedirs(os.path.realpath(dirs))\n",
    "\n",
    "def make_output_and_temp_folders(idx, output_folder = OUTPUT_FOLDER):\n",
    "    find_and_make_dirs(output_folder + \"raw/\")\n",
    "    find_and_make_dirs(output_folder + \"raw/clouds/\")\n",
    "    find_and_make_dirs(output_folder + \"raw/s1/\")\n",
    "    find_and_make_dirs(output_folder + \"raw/s2/\")\n",
    "    find_and_make_dirs(output_folder + \"raw/misc/\")\n",
    "    find_and_make_dirs(output_folder + \"processed/\")\n",
    "\n",
    "def download_large_tile(coord, step_x, step_y, folder = OUTPUT_FOLDER, year = 2019, s1_layer = \"SENT\"):\n",
    "    '''\n",
    "    Ideally the folder structure should be as follows:\n",
    "     - Raw\n",
    "         - Cloud\n",
    "              - Y*5, X*5\n",
    "         - Sentinel 1\n",
    "         - Sentinel 2\n",
    "         - DEM\n",
    "         - Dates\n",
    "     - Processed\n",
    "         - Y, X\n",
    "         \n",
    "    Methodological questions to address:\n",
    "        - Should we download all images for a region first, and then process?     - Seems unnecessary\n",
    "          This would allow for matching up the borders\n",
    "        - Remove dirty steps for the whole tile or by subtiles \n",
    "        - Is this the proper time to address the interpolation of clouds (?)\n",
    "        - Make sure that cloud shadows line up with clouds                        - X\n",
    "        - Make sure that sentinel 1 is properly matched up and items are removed  - X\n",
    "         \n",
    "    The strategy is as follows:\n",
    "        - Download 5*9 by 5*9 cloud, S1, S2, DEM, dates, save to folders          - X\n",
    "        - Load each start:start+128 tile                                          - X\n",
    "        - Remove dirty steps, missing images, missed clouds, ratios               - X\n",
    "        - Superresolve                                                            - X\n",
    "        - Indices                                                                 - X\n",
    "        - Calculate_and_save_best_images                                          - X\n",
    "        - Whittaker smooth                                                        - X\n",
    "        - Fuse S1\n",
    "        - Save to processed / y / x\n",
    "        - Iterate for each of the large sub-tiles\n",
    "    \n",
    "    '''\n",
    "    print(coord)\n",
    "    bbx = calculate_bbx(coord, step_x, step_y, expansion = 10)\n",
    "    print(bbx)\n",
    "    dem_bbx = calculate_bbx(coord, step_x, step_y, expansion = 20)\n",
    "    idx = str(step_y) + \"_\" + str(step_x)\n",
    "    print(idx)\n",
    "    idx = str(idx)\n",
    "    #if 1 != 0:\n",
    "    print(\"Making raw folders\")\n",
    "    make_output_and_temp_folders(idx)\n",
    "\n",
    "    print(\"Calculating cloud cover\")\n",
    "    if not os.path.exists(folder + \"raw/clouds/clouds_{}.npy\".format(idx)):\n",
    "        l1c, cloud_probs, shadows = identify_clouds(bbx) # integrate cloud shadow here\n",
    "        hkl.dump(cloud_probs, \"raw/clouds/clouds_{}.npy\".format(idx), mode='w', compression='gzip')\n",
    "        hkl.dump(shadows, \"raw/clouds/shadows_{}.npy\".format(idx), mode='w', compression='gzip')\n",
    "\n",
    "    if not os.path.exists(folder + \"raw/s1/{}.npy\".format(idx)):\n",
    "        print(\"Downloading S1\")\n",
    "        s1_layer = identify_s1_layer((coord[1], coord[0]))\n",
    "        s1, s1_dates = download_sentinel_1(bbx, layer = s1_layer)\n",
    "        s1 = process_sentinel_1_tile(s1, s1_dates)\n",
    "        hkl.dump(s1, \"raw/s1/{}.npy\".format(idx), mode='w', compression='gzip')\n",
    "        hkl.dump(s1_dates, \"raw/misc/s1_dates{}.npy\".format(idx), mode='w', compression='gzip')\n",
    "\n",
    "    if not os.path.exists(folder + \"raw/s2/{}.npy\".format(idx)):\n",
    "        print(\"Downloading S2\")\n",
    "        s2, s2_dates = download_layer(bbx)\n",
    "        hkl.dump(s2, \"raw/s2/{}.npy\".format(idx), mode='w', compression='gzip')\n",
    "        hkl.dump(s2_dates, \"raw/misc/s2_dates_{}.npy\".format(idx), mode='w', compression='gzip')\n",
    "\n",
    "    if not os.path.exists(folder + \"raw/misc/dem_{}.npy\".format(idx)):\n",
    "        print(\"Downloading DEM\")\n",
    "        dem = download_dem(dem_bbx) # get the DEM BBOX\n",
    "        hkl.dump(dem, \"raw/misc/dem_{}.npy\".format(idx), mode='w', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bad_steps(sentinel2, clouds):\n",
    "    n_cloud_px = np.array([len(np.argwhere(clouds[x, :, :].reshape((632)*(632)) > 0.3)) for x in range(clouds.shape[0])])\n",
    "    cloud_steps = np.argwhere(n_cloud_px > 632**2 / 5)\n",
    "    missing_images = [np.argwhere(sentinel2[x, :, : :].flatten() == 0.0) for x in range(sentinel2.shape[0])]\n",
    "    missing_images = np.array([len(x) for x in missing_images])\n",
    "    missing_images_p = [np.argwhere(sentinel2[x, :, : :10].flatten() >= 1) for x in range(sentinel2.shape[0])]\n",
    "    missing_images_p = np.array([len(x) for x in missing_images_p])\n",
    "    missing_images += missing_images_p\n",
    "    missing_images = np.argwhere(missing_images >= 100)\n",
    "    print(cloud_steps)\n",
    "    print(missing_images)\n",
    "    #perc_shadow_px = np.sum(shadows, axis = (1, 2) / (79**2))\n",
    "    #shadow_steps = np.argwhere(perc_shadow_px > 20)\n",
    "    to_remove = np.unique(np.concatenate([cloud_steps.flatten(), missing_images.flatten()]))#, shadow_steps]))\n",
    "    return to_remove\n",
    "\n",
    "def superresolve(sentinel2):\n",
    "    d10 = sentinel2[:, :, :, 0:4]\n",
    "    d20 = sentinel2[:, :, :, 4:10]\n",
    "\n",
    "    d10 = np.swapaxes(d10, 1, -1)\n",
    "    d10 = np.swapaxes(d10, 2, 3)\n",
    "    d20 = np.swapaxes(d20, 1, -1)\n",
    "    d20 = np.swapaxes(d20, 2, 3)\n",
    "    superresolved = DSen2(d10, d20)\n",
    "    superresolved = np.swapaxes(superresolved, 1, -1)\n",
    "    superresolved = np.swapaxes(superresolved, 1, 2)\n",
    "\n",
    "    # returns band IDXs 3, 4, 5, 7, 8, 9\n",
    "    return superresolved\n",
    "\n",
    "def process_sentinel_1_tile(sentinel1, dates):\n",
    "    s1 = calculate_and_save_best_images(sentinel1, dates)\n",
    "    # Retain only iamgery every 15 days\n",
    "    biweekly_dates = np.array([day for day in range(0, 360, 5)])\n",
    "    to_remove = np.argwhere(biweekly_dates % 15 != 0)\n",
    "    s1 = np.delete(s1, to_remove, 0)\n",
    "    return s1\n",
    "\n",
    "\n",
    "def interpolate_array(x):\n",
    "    no_dem = np.delete(x, 10, -1)\n",
    "    no_dem = np.reshape(no_dem, (72, 128*128*14))\n",
    "    no_dem = np.swapaxes(no_dem, 0, 1)\n",
    "\n",
    "    pool = multiprocessing.Pool(6)\n",
    "    no_dem = pool.map(smooth, no_dem)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    no_dem = np.swapaxes(no_dem, 0, 1)\n",
    "    no_dem = np.reshape(no_dem, (72, 128, 128, 14))\n",
    "    x[:, :, :, :10] = no_dem[:, :, :, :10]\n",
    "    x[:, :, :, 11:] = no_dem[:, :, :, 10:]\n",
    "\n",
    "    biweekly_dates = np.array([day for day in range(0, 360, 5)])\n",
    "    to_remove = np.argwhere(biweekly_dates % 15 != 0)\n",
    "    x = np.delete(x, to_remove, 0)\n",
    "    return x\n",
    "\n",
    "def process_large_tile(step_x, step_y, folder = OUTPUT_FOLDER):\n",
    "    idx = str(step_y) + \"_\" + str(step_x)\n",
    "    x_vals = []\n",
    "    y_vals = []\n",
    "    # save to disk\n",
    "    for i in range(25):\n",
    "        y_val = (24 - i) // 5\n",
    "        x_val = 5 - ((25 - i) % 5)\n",
    "        x_val = 0 if x_val == 5 else x_val\n",
    "        x_vals.append(x_val)\n",
    "        y_vals.append(y_val)\n",
    "        \n",
    "    y_vals = [i + (5*step_y) for i in y_vals]\n",
    "    x_vals = [i + (5*step_x) for i in x_vals]\n",
    "    print(y_vals, x_vals)\n",
    "    \n",
    "    \n",
    "    processed = True\n",
    "    for x, y in zip(x_vals, y_vals):\n",
    "        if not os.path.exists(folder + \"processed/{}/{}.npy\".format(str(y), str(x))):\n",
    "            processed = False\n",
    "    if not processed:\n",
    "        \n",
    "        clouds = hkl.load(folder + \"raw/clouds/clouds_{}.npy\".format(idx))\n",
    "        sentinel1 = hkl.load(folder + \"raw/s1/{}.npy\".format(idx))\n",
    "        radar_dates = hkl.load(folder + \"raw/misc/s1_dates_{}.npy\".format(idx))\n",
    "        sentinel2 = hkl.load(folder + \"raw/s2/{}.npy\".format(idx))\n",
    "        dem = hkl.load(folder + \"raw/misc/dem_{}.npy\".format(idx))\n",
    "        image_dates = hkl.load(folder + \"raw/misc/s2_dates_{}.npy\".format(idx))\n",
    "        if os.path.exists(folder + \"raw/clouds/shadows_{}.npy\".format(idx)):\n",
    "            shadows = hkl.load(folder + \"raw/clouds/shadows_{}.npy\".format(idx))\n",
    "        else:\n",
    "            print(\"No shadows file, so calculating shadows with L2A\")\n",
    "            shadows = mcm_shadow_mask(sentinel2, clouds)\n",
    "        print(\"The files have been loaded\")\n",
    "\n",
    "        #sentinel1 = process_sentinel_1_tile(sentinel1, radar_dates)\n",
    "        to_remove = calculate_bad_steps(sentinel2, clouds)\n",
    "        sentinel2 = np.delete(sentinel2, to_remove, axis = 0)\n",
    "        clouds = np.delete(clouds, to_remove, axis = 0)\n",
    "        shadows = np.delete(shadows, to_remove, axis = 0)\n",
    "        image_dates = np.delete(image_dates, to_remove)\n",
    "        print(\"Cloudy and missing images removed, radar processed\")\n",
    "\n",
    "        to_remove = remove_missed_clouds(sentinel2)\n",
    "        sentinel2 = np.delete(sentinel2, to_remove, axis = 0)\n",
    "        clouds = np.delete(clouds, to_remove, axis = 0)\n",
    "        image_dates = np.delete(image_dates, to_remove)\n",
    "        shadows = np.delete(shadows, to_remove, axis = 0)\n",
    "        print(\"Missed cloudy images removed\")\n",
    "\n",
    "        x, _, _ = remove_cloud_and_shadows(sentinel2, clouds, shadows, image_dates)\n",
    "        print(\"Clouds and shadows interpolated\")\n",
    "\n",
    "\n",
    "        index = 0\n",
    "        for start_x, end_x in zip(range(0, 633, 126), range(128, 633, 126)):\n",
    "            for start_y, end_y in zip(range(0, 633, 126), range(128, 633, 126)):\n",
    "                print(index)\n",
    "                if not os.path.exists(folder + \"processed/{}/{}.npy\".format(str(y_vals[index]), str(x_vals[index]))):\n",
    "                    subtile = x[:, start_x:end_x, start_y:end_y, :]\n",
    "                    resolved = superresolve(subtile)\n",
    "                    subtile[:, :, :, 4:10] = resolved\n",
    "                    dem_i = np.tile(dem[np.newaxis, start_x:end_x, start_y:end_y, :], (x.shape[0], 1, 1, 1))\n",
    "                    subtile = np.concatenate([subtile, dem_i / 90], axis = -1)\n",
    "                    subtile, amin = evi(subtile, verbose = True)\n",
    "                    subtile = bi(subtile, verbose = True)\n",
    "                    subtile = msavi2(subtile, verbose = True)\n",
    "                    subtile = si(subtile, verbose = True)\n",
    "\n",
    "                    subtile = calculate_and_save_best_images(subtile, image_dates)\n",
    "                    subtile = interpolate_array(subtile)\n",
    "                    subtile = np.concatenate([subtile, sentinel1[:, start_x:end_x,\n",
    "                                                                start_y:end_y, :]], axis = -1)\n",
    "\n",
    "\n",
    "                    out_y_folder = folder + \"processed/{}/\".format(str(y_vals[index]))\n",
    "                    if not os.path.exists(os.path.realpath(out_y_folder)):\n",
    "                        os.makedirs(os.path.realpath(out_y_folder))\n",
    "                    np.save(folder + \"processed/{}/{}.npy\".format(str(y_vals[index]), str(x_vals[index])), subtile)\n",
    "                index += 1\n",
    "            \n",
    "def clean_up_folders():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-70.462961 18.872589\n",
      "-70.40316756213878 18.873049611040003\n",
      "-70.462961 18.872589\n",
      "3969\n"
     ]
    }
   ],
   "source": [
    "coord1 = offset_x(coords, 6300)\n",
    "coord1 = offset_y(coord1 , 6300)\n",
    "#coord1 = (14.355, 10.694)\n",
    "calculate_area([coords, coord1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: 0 Y:0\n",
      "(-70.462961, 18.872589)\n",
      "-70.462961 18.872589\n",
      "-70.46305590883179 18.87258825360633\n",
      "-70.462961 18.872589\n",
      "-70.4030726505521 18.873050326902963\n",
      "([-70.40354799691623, 18.930061533071747], [-70.4630551247406, 18.872497905130146])\n",
      "-70.462961 18.872589\n",
      "-70.46315081765911 18.872587507164276\n",
      "-70.462961 18.872589\n",
      "-70.40297773896116 18.873051042717535\n",
      "0_0\n",
      "Making raw folders\n",
      "Calculating cloud cover\n",
      "34\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7146b5da4c254419895baf7eba1bd4c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=81), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fe0b5d1b75843df99132139b293f67b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=81), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cloud probs: (81, 632, 632)\n",
      "Downloading S1\n",
      "Loading formatted geocoded file...\n",
      "NA\n",
      "SENT_DESC\n",
      "OOB [798848. 798848. 798848. 798848. 798848. 798848. 798848. 798848. 798848.\n",
      " 798848. 798848. 798848. 798848. 798848. 798848. 798848. 798848. 798848.\n",
      " 798848. 798848. 798848. 798848. 798848. 798848. 798848. 798848. 798848.\n",
      " 798848. 798848. 798848. 798848. 798848. 798848. 798848. 798848. 798848.\n",
      " 798848.  12202.  14889.   5918. 798848. 798848. 798848. 798848. 798848.\n",
      " 798848. 798848. 798848. 798848. 798848. 798848. 798848.  10511. 798848.\n",
      " 798848.  12408. 798848.  10229. 798848. 798848. 798848. 798848.  12947.\n",
      " 798848.  11680. 798848.   4372.  10932. 798848.  11797. 798848. 798848.\n",
      " 798848. 798848. 798848. 798848.  11086. 798848. 798848. 798848. 798848.\n",
      " 798848. 798848. 798848. 798848.]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Maximum time distance: 65\n",
      "Downloading S2\n",
      "Original size: (81, 640, 626, 6)\n",
      "Original 10 size: (81, 640, 626, 4)\n",
      "Downloading DEM\n",
      "[4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0] [0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4]\n",
      "The files have been loaded\n",
      "[[ 3]\n",
      " [ 7]\n",
      " [ 8]\n",
      " [ 9]\n",
      " [11]\n",
      " [12]\n",
      " [16]\n",
      " [19]\n",
      " [23]\n",
      " [25]\n",
      " [26]\n",
      " [27]\n",
      " [28]\n",
      " [30]\n",
      " [31]\n",
      " [33]\n",
      " [36]\n",
      " [37]\n",
      " [38]\n",
      " [39]\n",
      " [41]\n",
      " [44]\n",
      " [46]\n",
      " [47]\n",
      " [48]\n",
      " [49]\n",
      " [52]\n",
      " [55]\n",
      " [56]\n",
      " [64]\n",
      " [65]\n",
      " [66]\n",
      " [67]\n",
      " [72]\n",
      " [75]\n",
      " [76]\n",
      " [79]\n",
      " [80]]\n",
      "[[ 1]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 7]\n",
      " [ 8]\n",
      " [ 9]\n",
      " [12]\n",
      " [14]\n",
      " [15]\n",
      " [16]\n",
      " [17]\n",
      " [18]\n",
      " [23]\n",
      " [24]\n",
      " [25]\n",
      " [26]\n",
      " [29]\n",
      " [30]\n",
      " [32]\n",
      " [35]\n",
      " [36]\n",
      " [37]\n",
      " [39]\n",
      " [41]\n",
      " [44]\n",
      " [45]\n",
      " [47]\n",
      " [48]\n",
      " [50]\n",
      " [51]\n",
      " [52]\n",
      " [55]\n",
      " [56]\n",
      " [59]\n",
      " [60]\n",
      " [64]\n",
      " [65]\n",
      " [67]\n",
      " [71]\n",
      " [72]\n",
      " [75]\n",
      " [76]\n",
      " [77]\n",
      " [78]\n",
      " [79]\n",
      " [80]]\n",
      "Cloudy and missing images removed, radar processed\n",
      "Missed cloudy images removed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b794b557d2494cba40655fe18b3497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=314), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A total of 91060 pixels were interpolated\n",
      "Clouds and shadows interpolated\n",
      "0\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "24/24 [==============================] - 13s 555ms/step\n",
      "There are: 0 out of bounds EVI\n",
      "()\n",
      "evis error: 0.10265700631953827, 1.2288784350191515, 0 steps, clipping to -1.5, 1.5\n",
      "Maximum time distance: 50\n",
      "1\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "24/24 [==============================] - 15s 615ms/step\n",
      "There are: 0 out of bounds EVI\n",
      "()\n",
      "evis error: 0.012182785367310541, 2.1839703046037338, 0 steps, clipping to -1.5, 1.5\n",
      "Maximum time distance: 50\n",
      "2\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "24/24 [==============================] - 13s 558ms/step\n",
      "There are: 4 out of bounds EVI\n",
      "()\n",
      "evis error: -150.0064077727774, 7.621863495294457, 3 steps, clipping to -1.5, 1.5\n",
      "Maximum time distance: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/john.brandt/Documents/GitHub/restoration-mapper/src/utils-bilinear.py:102: RuntimeWarning: invalid value encountered in power\n",
      "  sis = np.power( (1-BLUE) * (1 - GREEN) * (1 - RED), 1/3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "24/24 [==============================] - 14s 589ms/step\n",
      "There are: 5 out of bounds EVI\n",
      "()\n",
      "evis error: -4.129746345482756, 18.849354315370018, 5 steps, clipping to -1.5, 1.5\n",
      "Maximum time distance: 50\n",
      "4\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "24/24 [==============================] - 14s 579ms/step\n",
      "There are: 34 out of bounds EVI\n",
      "()\n",
      "evis error: -115.58368020027218, 92.35926719411596, 13 steps, clipping to -1.5, 1.5\n",
      "Maximum time distance: 50\n",
      "5\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "24/24 [==============================] - 16s 653ms/step\n",
      "There are: 1 out of bounds EVI\n",
      "()\n",
      "evis error: 0.009705759954959155, 4.047812485296937, 1 steps, clipping to -1.5, 1.5\n",
      "Maximum time distance: 50\n",
      "6\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "24/24 [==============================] - 15s 622ms/step\n",
      "There are: 4 out of bounds EVI\n",
      "()\n",
      "evis error: -34.740409028981915, 4.047812485296937, 4 steps, clipping to -1.5, 1.5\n",
      "Maximum time distance: 50\n",
      "7\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "24/24 [==============================] - 15s 605ms/step\n",
      "There are: 5 out of bounds EVI\n",
      "()\n",
      "evis error: -9.938969651864715, 7.287176273227479, 3 steps, clipping to -1.5, 1.5\n",
      "Maximum time distance: 50\n",
      "8\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "24/24 [==============================] - 14s 596ms/step\n",
      "There are: 6 out of bounds EVI\n",
      "()\n",
      "evis error: -2.1001864838302158, 52.58521071938727, 5 steps, clipping to -1.5, 1.5\n",
      "Maximum time distance: 50\n",
      "9\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "24/24 [==============================] - 14s 587ms/step\n",
      "There are: 22 out of bounds EVI\n",
      "()\n",
      "evis error: -10.988183018604818, 93.51172132602852, 14 steps, clipping to -1.5, 1.5\n",
      "Maximum time distance: 50\n",
      "10\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "24/24 [==============================] - 14s 584ms/step\n",
      "There are: 0 out of bounds EVI\n",
      "()\n",
      "evis error: -0.15315846516610118, 2.1937211236546954, 0 steps, clipping to -1.5, 1.5\n",
      "Maximum time distance: 50\n",
      "11\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "24/24 [==============================] - 14s 587ms/step\n",
      "There are: 27 out of bounds EVI\n",
      "()\n",
      "evis error: -50.80717919268612, 27.441043401841764, 15 steps, clipping to -1.5, 1.5\n",
      "Maximum time distance: 50\n",
      "12\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "24/24 [==============================] - 14s 592ms/step\n",
      "There are: 0 out of bounds EVI\n",
      "Maximum time distance: 50\n",
      "13\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "24/24 [==============================] - 14s 585ms/step\n",
      "There are: 5 out of bounds EVI\n",
      "()\n",
      "evis error: -2.8207478448217422, 8.496507343427457, 5 steps, clipping to -1.5, 1.5\n",
      "Maximum time distance: 50\n",
      "14\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "24/24 [==============================] - 14s 589ms/step\n",
      "There are: 4 out of bounds EVI\n",
      "()\n",
      "evis error: -85.410042865891, 3.48928903511909, 4 steps, clipping to -1.5, 1.5\n",
      "Maximum time distance: 50\n",
      "15\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "24/24 [==============================] - 15s 605ms/step\n",
      "There are: 2 out of bounds EVI\n",
      "()\n",
      "evis error: -1.6192935421633312, 3.7454599268971887, 2 steps, clipping to -1.5, 1.5\n",
      "Maximum time distance: 50\n",
      "16\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "24/24 [==============================] - 16s 665ms/step\n",
      "There are: 3 out of bounds EVI\n",
      "()\n",
      "evis error: -11.12711128970892, 7.022752208497232, 3 steps, clipping to -1.5, 1.5\n",
      "Maximum time distance: 50\n",
      "17\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "24/24 [==============================] - 14s 589ms/step\n",
      "There are: 2 out of bounds EVI\n",
      "()\n",
      "evis error: -3.5399577336418178, 7.359880318390591, 2 steps, clipping to -1.5, 1.5\n",
      "Maximum time distance: 50\n",
      "18\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "24/24 [==============================] - 14s 584ms/step\n",
      "There are: 2 out of bounds EVI\n",
      "()\n",
      "evis error: -5.467763264752654, 6.467407677356657, 2 steps, clipping to -1.5, 1.5\n",
      "Maximum time distance: 50\n",
      "19\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "24/24 [==============================] - 14s 575ms/step\n",
      "There are: 1 out of bounds EVI\n",
      "()\n",
      "evis error: -34.52384480403909, 2.015862099394755, 1 steps, clipping to -1.5, 1.5\n",
      "Maximum time distance: 50\n",
      "20\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "24/24 [==============================] - 13s 557ms/step\n",
      "There are: 4 out of bounds EVI\n",
      "()\n",
      "evis error: -96.20970145419044, 7.627148680526207, 1 steps, clipping to -1.5, 1.5\n",
      "Maximum time distance: 50\n",
      "21\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "24/24 [==============================] - 14s 569ms/step\n",
      "There are: 2 out of bounds EVI\n",
      "()\n",
      "evis error: -51.8744304668023, 3.9231361211333655, 1 steps, clipping to -1.5, 1.5\n",
      "Maximum time distance: 50\n",
      "22\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "24/24 [==============================] - 13s 558ms/step\n",
      "There are: 0 out of bounds EVI\n",
      "()\n",
      "evis error: -1.1917089455185934, 0.9288122632165906, 0 steps, clipping to -1.5, 1.5\n",
      "Maximum time distance: 50\n",
      "23\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "24/24 [==============================] - 13s 551ms/step\n",
      "There are: 1 out of bounds EVI\n",
      "()\n",
      "evis error: 0.03068567508347056, 13.601781684504505, 1 steps, clipping to -1.5, 1.5\n",
      "Maximum time distance: 50\n",
      "24\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "24/24 [==============================] - 14s 575ms/step\n",
      "There are: 8 out of bounds EVI\n",
      "()\n",
      "evis error: -38.180783749450626, 13.94408860969204, 3 steps, clipping to -1.5, 1.5\n",
      "Maximum time distance: 50\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tnrange, tqdm_notebook\n",
    "import math\n",
    "#start at 410\n",
    "# 745 after CC\n",
    "for x_tile in range(0, 1):\n",
    "    for y_tile in range(0, 1):\n",
    "        print(\"X: {} Y:{}\".format(x_tile, y_tile))\n",
    "        s1 = download_large_tile(coord = coords, step_x = x_tile, step_y = y_tile)\n",
    "        process_large_tile(x_tile, y_tile)\n",
    "        #clean_up_folders(x_tile, y_tile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_sensing",
   "language": "python",
   "name": "remote_sensing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
