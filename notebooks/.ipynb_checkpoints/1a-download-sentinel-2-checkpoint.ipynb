{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and process sentinel 2 data\n",
    "\n",
    "## John Brandt\n",
    "## December 2, 2020\n",
    "\n",
    "## Package imports, API import, source scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import scipy.sparse as sparse\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "\n",
    "from collections import Counter\n",
    "from osgeo import ogr, osr\n",
    "from random import shuffle\n",
    "from scipy.sparse.linalg import splu\n",
    "from sentinelhub import WmsRequest, WcsRequest, MimeType\n",
    "from sentinelhub import CRS, BBox, constants, DataSource, CustomUrlParam\n",
    "from skimage.transform import resize\n",
    "from typing import Tuple, List\n",
    "\n",
    "with open(\"../config.yaml\", 'r') as stream:\n",
    "        key = (yaml.safe_load(stream))\n",
    "        API_KEY = key['key'] \n",
    "        \n",
    "%matplotlib inline\n",
    "%run ../src/preprocessing/slope.py\n",
    "%run ../src/preprocessing/indices.py\n",
    "%run ../src/downloading/utils.py\n",
    "%run ../src/preprocessing/cloud_removal.py\n",
    "%run ../src/preprocessing/whittaker_smoother.py\n",
    "%run ../src/dsen2/utils/DSen2Net.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"../config.yaml\"):\n",
    "    with open(\"../config.yaml\", 'r') as stream:\n",
    "        key = (yaml.safe_load(stream))\n",
    "        API_KEY = key['key']\n",
    "        AWSKEY = key['awskey']\n",
    "        AWSSECRET = key['awssecret']\n",
    "else:\n",
    "    API_KEY = \"none\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "YEAR = 2019\n",
    "TIME = ('{}-12-01'.format(str(YEAR - 1)), '{}-02-01'.format(str(YEAR + 1)))\n",
    "EPSG = CRS.WGS84\n",
    "IMSIZE = 48\n",
    "\n",
    "# Constants\n",
    "starting_days = np.cumsum([0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bbox(plot_id: int, df: \"DataFrame\") -> List:\n",
    "    \"\"\" Calculates the corners of a bounding box from an input\n",
    "        pandas dataframe as output by Collect Earth Online\n",
    "\n",
    "        Parameters:\n",
    "         plot_id (int): plot_id of associated plot\n",
    "         df (pandas.DataFrame): dataframe of associated CEO survey\n",
    "    \n",
    "        Returns:\n",
    "         bounding_box (list): [(min(x), min(y)),\n",
    "                              (max(x), max_y))]\n",
    "    \"\"\"\n",
    "    subs = df[df['PLOT_ID'] == plot_id]\n",
    "    # (min x, min y), (max x, max y)\n",
    "    return [(min(subs['LON']), min(subs['LAT'])),\n",
    "            (max(subs['LON']), max(subs['LAT']))]\n",
    "\n",
    "def bounding_box(points: List[Tuple[float, float]], \n",
    "                 expansion: int = 160) -> ((Tuple, Tuple), str):\n",
    "    \"\"\" Calculates the corners of a bounding box with an\n",
    "        input expansion in meters from a given bounding_box\n",
    "        \n",
    "        Subcalls:\n",
    "         calculate_epsg, convertCoords\n",
    "\n",
    "        Parameters:\n",
    "         points (list): output of calc_bbox\n",
    "         expansion (float): number of meters to expand or shrink the\n",
    "                            points edges to be\n",
    "    \n",
    "        Returns:\n",
    "         bl (tuple): x, y of bottom left corner with edges of expansion meters\n",
    "         tr (tuple): x, y of top right corner with edges of expansion meters\n",
    "    \"\"\"\n",
    "    bl = list(points[0])\n",
    "    tr = list(points[1])\n",
    "    inproj = Proj('epsg:4326')\n",
    "    outproj_code = calculate_epsg(bl)\n",
    "    outproj = Proj('epsg:' + str(outproj_code))\n",
    "    bl_utm =  transform(inproj, outproj, bl[1], bl[0])\n",
    "    tr_utm =  transform(inproj, outproj, tr[1], tr[0])\n",
    "\n",
    "    distance1 = tr_utm[0] - bl_utm[0]\n",
    "    distance2 = tr_utm[1] - bl_utm[1]\n",
    "    expansion1 = (expansion - distance1)/2\n",
    "    expansion2 = (expansion - distance2)/2\n",
    "        \n",
    "    bl_utm = [bl_utm[0] - expansion1, bl_utm[1] - expansion2]\n",
    "    tr_utm = [tr_utm[0] + expansion1, tr_utm[1] + expansion2]\n",
    "\n",
    "\n",
    "    zone = str(outproj_code)[3:]\n",
    "    zone = zone[1:] if zone[0] == \"0\" else zone\n",
    "    direction = 'N' if tr[1] >= 0 else 'S'\n",
    "    utm_epsg = \"UTM_\" + zone + direction\n",
    "    return (bl_utm, tr_utm), CRS[utm_epsg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud and cloud shadow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_clouds(bbox: List[Tuple[float, float]], epsg: 'CRS', time: dict = TIME):\n",
    "    \"\"\" Downloads and calculates cloud cover and shadow\n",
    "        \n",
    "        Parameters:\n",
    "         bbox (list): output of calc_bbox\n",
    "         epsg (float): EPSG associated with bbox \n",
    "         time (tuple): YY-MM-DD - YY-MM-DD bounds for downloading \n",
    "    \n",
    "        Returns:\n",
    "         cloud_img (np.array): (X, 96, 96) array of cloud probs\n",
    "         shadows (np.array):  (X, 96, 96) array of shadow binary\n",
    "         clean_steps (np.array): (N,) array of clean idx\n",
    "         cloud_dates (np.array): (N,) array of clean cloud datets\n",
    "    \"\"\"\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    cloud_request = WcsRequest(\n",
    "        layer='CLOUD_NEW',\n",
    "        bbox=box, time=time,\n",
    "        resx='160m', resy='160m',\n",
    "        image_format = MimeType.TIFF_d8,\n",
    "        maxcc=0.75, instance_id=API_KEY,\n",
    "        custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "        time_difference=datetime.timedelta(hours=96))\n",
    "\n",
    "    shadow_request = WcsRequest(\n",
    "        layer='SHADOW',\n",
    "        bbox=box, time=time,\n",
    "        resx='20m', resy='20m',\n",
    "        image_format =  MimeType.TIFF_d16,\n",
    "        maxcc=0.75, instance_id=API_KEY,\n",
    "        custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "        time_difference=datetime.timedelta(hours=96))\n",
    "\n",
    "    cloud_img = np.array(cloud_request.get_data())\n",
    "    if not isinstance(cloud_img.flat[0], np.floating):\n",
    "        assert np.max(cloud_img) > 1\n",
    "        print(f\"Original cloud image max is {np.max(cloud_img)}, {cloud_img.shape}\")\n",
    "        cloud_img = cloud_img / 255\n",
    "    assert np.max(cloud_img) <= 1.\n",
    "\n",
    "    cloud_img = resize(cloud_img, (cloud_img.shape[0], 96, 96), order = 0)\n",
    "    n_cloud_px = np.sum(cloud_img > 0.33, axis = (1, 2))\n",
    "    cloud_steps = np.argwhere(n_cloud_px > (96**2 * 0.15))\n",
    "    clean_steps = [x for x in range(cloud_img.shape[0]) if x not in cloud_steps]\n",
    "    \n",
    "    cloud_dates = []\n",
    "    for date in cloud_request.get_dates():\n",
    "        if date.year == YEAR - 1:\n",
    "            cloud_dates.append(-365 + starting_days[(date.month-1)] + date.day)\n",
    "        if date.year == YEAR:\n",
    "            cloud_dates.append(starting_days[(date.month-1)] + date.day)\n",
    "        if date.year == YEAR + 1:\n",
    "            cloud_dates.append(365 + starting_days[(date.month-1)]+date.day)\n",
    "    cloud_dates = [val for idx, val in enumerate(cloud_dates) if idx in clean_steps]\n",
    "    \n",
    "    shadow_dates = []\n",
    "    for date in shadow_request.get_dates():\n",
    "        if date.year == YEAR - 1:\n",
    "            shadow_dates.append(-365 + starting_days[(date.month-1)] + date.day)\n",
    "        if date.year == YEAR:\n",
    "            shadow_dates.append(starting_days[(date.month-1)] + date.day)\n",
    "        if date.year == YEAR + 1:\n",
    "            shadow_dates.append(365 + starting_days[(date.month-1)]+date.day)\n",
    "    shadow_steps = [idx for idx, val in enumerate(shadow_dates) if val in cloud_dates]\n",
    "    \n",
    "    shadow_img = np.array(shadow_request.get_data(data_filter = shadow_steps))\n",
    "    shadow_pus = (shadow_img.shape[1]*shadow_img.shape[2])/(512*512) * shadow_img.shape[0]\n",
    "    shadow_img = resize(shadow_img, (shadow_img.shape[0], 96, 96, shadow_img.shape[-1]), order = 0)\n",
    "    \n",
    "    print(f\"The max shadows is {np.max(shadow_img)}\")\n",
    "    if not isinstance(shadow_img.flat[0], np.floating):\n",
    "        assert np.max(shadow_img) > 1\n",
    "        shadow_img = shadow_img / 65535\n",
    "    assert np.max(shadow_img) <= 1\n",
    "\n",
    "    cloud_img = np.delete(cloud_img, cloud_steps, 0)\n",
    "    assert shadow_img.shape[0] == cloud_img.shape[0], (shadow_img.shape, cloud_img.shape)\n",
    "    shadows = mcm_shadow_mask(np.array(shadow_img), cloud_img) # Make usre this makes sense??\n",
    "    print(f\"Shadows ({shadows.shape}) used {round(shadow_pus, 1)} processing units\")\n",
    "    return cloud_img, shadows, clean_steps, np.array(cloud_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEM and slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dem(plot_id: int, df: 'DataFrame', epsg: 'CRS') -> (np.ndarray, np.ndarray):\n",
    "    \"\"\" Downloads MapZen digital elevation model and return slope\n",
    "\n",
    "        Parameters:\n",
    "         plot_id (tuple): plot id from collect earth online (CEO)\n",
    "         df (pandas.DataFrame): data associated with plot_id from CEO\n",
    "         epsg (int): UTM EPSG associated with plot_id\n",
    "    \n",
    "        Returns:\n",
    "         slope (arr): (X, Y, 1) array of per-pixel slope from [0, 1]\n",
    "    \"\"\"\n",
    "    location = calc_bbox(plot_id, df = df)\n",
    "    bbox, epsg = bounding_box(location, expansion = (32+2)*10)\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    dem_request = WcsRequest(data_source=DataSource.DEM,\n",
    "                         layer='DEM', bbox=box,\n",
    "                         resx = \"10m\", resy = \"10m\",\n",
    "                         instance_id=API_KEY,\n",
    "                         image_format= MimeType.TIFF_d32f,\n",
    "                         custom_url_params={CustomUrlParam.SHOWLOGO: False})\n",
    "    dem_image_init = dem_request.get_data()[0]\n",
    "    dem_image = np.copy(dem_image_init)\n",
    "    dem_image = median_filter(dem_image_init, size = 5)\n",
    "    slope = calcSlope(dem_image.reshape((1, 32+2, 32+2)),\n",
    "                      np.full((32+2, 32+2), 10),\n",
    "                      np.full((32+2, 32+2), 10), \n",
    "                      zScale = 1, minSlope = 0.02)\n",
    "    slope = slope / 90\n",
    "    slope = slope.reshape((32+2, 32+2, 1))\n",
    "    slope = slope[1:32+1, 1:32+1, :]\n",
    "    return slope, dem_image_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 and 20 meter L2A bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_layer(bbox: List[Tuple[float, float]],\n",
    "                   clean_steps: np.ndarray, epsg: 'CRS',\n",
    "                   dates: dict = TIME, year: int = YEAR) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\" Downloads the L2A sentinel layer with 10 and 20 meter bands\n",
    "        \n",
    "        Parameters:\n",
    "         bbox (list): output of calc_bbox\n",
    "         epsg (float): EPSG associated with bbox \n",
    "         time (tuple): YY-MM-DD - YY-MM-DD bounds for downloading \n",
    "    \n",
    "        Returns:\n",
    "         img (arr):\n",
    "         img_request (obj): \n",
    "    \"\"\"\n",
    "    try:\n",
    "        box = BBox(bbox, crs = epsg)\n",
    "        image_request = WcsRequest(\n",
    "                layer='L2A20',\n",
    "                bbox=box, time=dates,\n",
    "                image_format = MimeType.TIFF_d16,\n",
    "                data_source = DataSource.SENTINEL2_L2A,\n",
    "                maxcc=0.75,\n",
    "                resx='20m', resy='20m',\n",
    "                instance_id=API_KEY,\n",
    "                custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'NEAREST',\n",
    "                                    constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "                time_difference=datetime.timedelta(hours=96),\n",
    "            )\n",
    "        \n",
    "        image_dates = []\n",
    "        for date in image_request.get_dates():\n",
    "            if date.year == YEAR - 1:\n",
    "                image_dates.append(-365 + starting_days[(date.month-1)] + date.day)\n",
    "            if date.year == YEAR:\n",
    "                image_dates.append(starting_days[(date.month-1)] + date.day)\n",
    "            if date.year == YEAR + 1:\n",
    "                image_dates.append(365 + starting_days[(date.month-1)]+date.day)\n",
    "        \n",
    "        steps_to_download = [i for i, val in enumerate(image_dates) if val in clean_steps]\n",
    "        dates_to_download = [val for i, val in enumerate(image_dates) if val in clean_steps]\n",
    "        print(f\"The cloud-free image dates are {dates_to_download}\")\n",
    "              \n",
    "        img_bands = image_request.get_data(data_filter = steps_to_download)\n",
    "        img_20 = np.stack(img_bands)\n",
    "        if np.max(img_20) >= 10:\n",
    "            img_20 = img_20 / 65535\n",
    "        assert np.max(img_20) <= 2.\n",
    "\n",
    "        s2_20_usage = (img_20.shape[1]*img_20.shape[2])/(512*512) * (6/3) * img_20.shape[0]\n",
    "        if (img_20.shape[1] * img_20.shape[2]) != 24*24:\n",
    "            print(f\"Original 20 meter bands size: {img_20.shape}, using {s2_20_usage} PU\")\n",
    "        img_20 = resize(img_20, (img_20.shape[0], IMSIZE, IMSIZE, img_20.shape[-1]), order = 0)\n",
    "        \n",
    "        image_request = WcsRequest(\n",
    "                layer='L2A10',\n",
    "                bbox=box, time=dates,\n",
    "                image_format = MimeType.TIFF_d16,\n",
    "                data_source = DataSource.SENTINEL2_L2A,\n",
    "                maxcc=0.75,\n",
    "                resx='10m', resy='10m',\n",
    "                instance_id=API_KEY,\n",
    "                custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'BICUBIC',\n",
    "                                    constants.CustomUrlParam.UPSAMPLING: 'BICUBIC'},\n",
    "                time_difference=datetime.timedelta(hours=96),\n",
    "        )\n",
    "        \n",
    "        img_bands = image_request.get_data(data_filter = steps_to_download)\n",
    "        img_10 = np.stack(img_bands)\n",
    "        if (img_10.shape[1] * img_10.shape[2]) != 48*48:\n",
    "            print(f\"The original L2A image size is: {img_10.shape}\")\n",
    "            \n",
    "        img_10 = resize(img_10, (img_10.shape[0], IMSIZE, IMSIZE, img_10.shape[-1]), order = 0)\n",
    "        img = np.concatenate([img_10, img_20], axis = -1)\n",
    "\n",
    "        if np.max(img_10) >= 10:\n",
    "            img_10 = img_10 / 65535\n",
    "        assert np.max(img_10) <= 2.\n",
    "        return img, np.array(dates_to_download)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.fatal(e, exc_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/john.brandt/anaconda3/envs/remote_sensing/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ../models/supres/model\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "from keras import backend as K\n",
    "K.set_session(sess)\n",
    "\n",
    "MDL_PATH = \"../models/supres/\"\n",
    "\n",
    "model = tf.train.import_meta_graph(MDL_PATH + 'model.meta')\n",
    "model.restore(sess, tf.train.latest_checkpoint(MDL_PATH))\n",
    "\n",
    "logits = tf.get_default_graph().get_tensor_by_name(\"Add_6:0\")\n",
    "inp = tf.get_default_graph().get_tensor_by_name(\"Placeholder:0\")\n",
    "inp_bilinear = tf.get_default_graph().get_tensor_by_name(\"Placeholder_1:0\")\n",
    "\n",
    "def superresolve(input_data):\n",
    "    bilinear_upsample = input_data[..., 4:]\n",
    "    x = sess.run([logits], \n",
    "                 feed_dict={inp: input_data,\n",
    "                            inp_bilinear: bilinear_upsample})\n",
    "    return x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_new_dem(data_location: 'os.Path',\n",
    "                     output_folder: 'os.Path',\n",
    "                     image_format: 'MimeType' = MimeType.TIFF_d16):\n",
    "    \"\"\" Downloads and saves DEM and slope files\n",
    "        \n",
    "        Parameters:\n",
    "         data_location (os.path): \n",
    "         output_folder (os.path): \n",
    "         image_format (MimeType): \n",
    "    \n",
    "        Returns:\n",
    "         None\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(data_location)\n",
    "    df.columns = [x.upper() for x in df.columns]\n",
    "    for column in ['IMAGERY_TITLE', 'STACKINGPROFILEDG', 'PL_PLOTID', 'IMAGERYYEARDG',\n",
    "                  'IMAGERYMONTHPLANET', 'IMAGERYYEARPLANET', 'IMAGERYDATESECUREWATCH',\n",
    "                  'IMAGERYENDDATESECUREWATCH', 'IMAGERYFEATUREPROFILESECUREWATCH',\n",
    "                  'IMAGERYSTARTDATESECUREWATCH','IMAGERY_ATTRIBUTIONS',\n",
    "                  'SAMPLE_GEOM']:\n",
    "        if column in df.columns:\n",
    "            df = df.drop(column, axis = 1)\n",
    "    \n",
    "    df = df.dropna(axis = 0)\n",
    "    plot_ids = sorted(df['PLOT_ID'].unique())\n",
    "    existing = [int(x[:-4]) for x in os.listdir(output_folder) if \".DS\" not in x]\n",
    "    to_download = [x for x in plot_ids if x not in existing]\n",
    "    print(f\"Starting download of {len(to_download)}\"\n",
    "          f\" plots from {data_location} to {output_folder}\")\n",
    "    errors = []\n",
    "    for i, val in enumerate(to_download):\n",
    "        print(f\"Downloading {i + 1}/{len(to_download)}, {val}\")\n",
    "        initial_bbx = calc_bbox(val, df = df)\n",
    "        dem_bbx, epsg = bounding_box(initial_bbx, expansion = 32*10)\n",
    "        slope, dem = download_dem(val, epsg = epsg, df = df)\n",
    "        print(dem.shape)\n",
    "        np.save(output_folder + str(val), dem)\n",
    "        np.save(\"../data/train-slope/\" + str(val), slope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download of 0 plots from ../data/train-csv/ceo-ethiopia-finetune-sample-data.csv to ../data/train-dem/\n"
     ]
    }
   ],
   "source": [
    "from scipy.ndimage import median_filter\n",
    "for i in reversed(os.listdir(\"../data/train-csv/\")):\n",
    "    if \"ethiopia\" in i:\n",
    "    #if \".csv\" in i:\n",
    "        #if any(x in i for x in [\"africa-west\", \"cameroon\", \"koure\", \"niger\"]):\n",
    "        tile = download_new_dem(\"../data/train-csv/\" + i, \"../data/train-dem/\", image_format = MimeType.TIFF_d16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../src/io/upload.py\n",
    "uploader = FileUploader(awskey = AWSKEY, awssecret = AWSSECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def id_missing_px(sentinel2):\n",
    "    missing_images = np.sum(sentinel2[..., :10] == 0.0, axis = (1, 2, 3))\n",
    "    missing_images_p = np.sum(sentinel2[..., :10] >= 1., axis = (1, 2, 3))\n",
    "    missing_images = missing_images + missing_images_p\n",
    "    print(missing_images)\n",
    "    print(np.max(sentinel2[..., :10]), np.min(sentinel2[..., :10]))\n",
    "    missing_images = np.argwhere(missing_images >= ((sentinel2.shape[1]**2) / 11))\n",
    "    print(missing_images)\n",
    "    return missing_images\n",
    "\n",
    "def download_plots(data_location, output_folder, image_format = MimeType.TIFF_d16):\n",
    "    \"\"\" Downloads slope and sentinel-2 data for all plots associated\n",
    "        with an input CSV from a collect earth online survey\n",
    "        \n",
    "        Parameters:\n",
    "         data_location (os.path)\n",
    "         output_folder (os.path)\n",
    "        \n",
    "        Creates:\n",
    "         output_folder/{plot_id}.npy\n",
    "    \n",
    "        Returns:\n",
    "         None\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(data_location)\n",
    "    df.columns = [x.upper() for x in df.columns]\n",
    "    for column in ['IMAGERY_TITLE', 'STACKINGPROFILEDG', 'PL_PLOTID', 'IMAGERYYEARDG',\n",
    "                  'IMAGERYMONTHPLANET', 'IMAGERYYEARPLANET', 'IMAGERYDATESECUREWATCH',\n",
    "                  'IMAGERYENDDATESECUREWATCH', 'IMAGERYFEATUREPROFILESECUREWATCH',\n",
    "                  'IMAGERYSTARTDATESECUREWATCH','IMAGERY_ATTRIBUTIONS',\n",
    "                  'SAMPLE_GEOM']:\n",
    "        if column in df.columns:\n",
    "            df = df.drop(column, axis = 1)\n",
    "    \n",
    "    df = df.dropna(axis = 0)\n",
    "    plot_ids = sorted(df['PLOT_ID'].unique())\n",
    "    existing = [int(x[:-4]) for x in os.listdir(\"../data/test-dates/\") if \".DS\" not in x]\n",
    "    to_download = [x for x in plot_ids if x not in existing]\n",
    "    \n",
    "    print(f\"Starting download of {len(to_download)}\"\n",
    "          f\" plots from {data_location} to {output_folder}\")\n",
    "\n",
    "    for i, val in enumerate(to_download):\n",
    "        print(f\"Downloading {i + 1}/{len(to_download)}, {val}\")\n",
    "        initial_bbx = calc_bbox(val, df = df)\n",
    "        sentinel2_bbx, epsg = bounding_box(initial_bbx, expansion = IMSIZE*10)\n",
    "        cloud_bbx, _ = bounding_box(initial_bbx, expansion = 96*10)\n",
    "        try:\n",
    "            # Identify cloud steps, download DEM, and download L2A series\n",
    "            cloud_probs, shadows, _, image_dates = identify_clouds(cloud_bbx, epsg = epsg)\n",
    "            dem, _ = download_dem(val, epsg = epsg, df = df)\n",
    "            to_remove, _ = calculate_cloud_steps(cloud_probs, image_dates)\n",
    "            cloud_probs = np.delete(cloud_probs, to_remove, 0)\n",
    "            clean_dates = np.delete(image_dates, to_remove)\n",
    "            shadows = np.delete(shadows, to_remove, 0)\n",
    "            s2, s2_dates = download_layer(sentinel2_bbx, clean_steps = clean_dates, epsg = epsg)    \n",
    "            \n",
    "            # Step to ensure that shadows, clouds, sentinel l2a have aligned dates\n",
    "            print(f\"Shadows {shadows.shape}, clouds {cloud_probs.shape}, S2, {s2.shape}, S2d, {s2_dates.shape}\")\n",
    "            to_remove_clouds = [i for i, val in enumerate(clean_dates) if val not in s2_dates]\n",
    "            to_remove_dates = [val for i, val in enumerate(clean_dates) if val not in s2_dates]\n",
    "            if len(to_remove_clouds) >= 1:\n",
    "                print(f\"Removing {to_remove_dates} from clouds because not in S2\")\n",
    "                cloud_probs = np.delete(cloud_probs, to_remove_clouds, 0)\n",
    "                shadows = np.delete(shadows, to_remove_clouds, 0)\n",
    "            print(f\"Shadows {shadows.shape}, clouds {cloud_probs.shape}, S2, {s2.shape}, S2d, {s2_dates.shape}\")\n",
    "\n",
    "            \n",
    "            to_remove = remove_missed_clouds(s2)\n",
    "            s2 = np.delete(s2, to_remove, 0)\n",
    "            cloud_probs = np.delete(cloud_probs, to_remove, 0)\n",
    "            s2_dates = np.delete(s2_dates, to_remove)\n",
    "            shadows = np.delete(shadows, to_remove, 0)\n",
    "            if len(to_remove) > 0:\n",
    "                print(f\"Removing {len(to_remove)} steps based on ratio\")\n",
    "            \n",
    "            cloud_probs = cloud_probs[:, 24:-24, 24:-24]\n",
    "            shadows = shadows[:, 24:-24, 24:-24]\n",
    "            x, interp = remove_cloud_and_shadows(s2, cloud_probs, shadows, s2_dates)\n",
    "            to_remove = np.argwhere(np.mean(interp, axis = (1, 2, 3)) > 0.5)\n",
    "            x = np.delete(x, to_remove, 0)\n",
    "            cloud_probs = np.delete(cloud_probs, to_remove, 0)\n",
    "            s2_dates = np.delete(s2_dates, to_remove)\n",
    "            shadows = np.delete(shadows, to_remove, 0)\n",
    "            if len(to_remove) > 0:\n",
    "                print(f\"Removing {len(to_remove)} steps with >50% interpolation\")\n",
    "            \n",
    "            x_to_save = np.copy(x)\n",
    "            x_to_save = np.clip(x_to_save, 0, 1)\n",
    "            x_to_save = np.trunc(x_to_save * 65535).astype(np.uint16)\n",
    "            np.save(f\"../data/test-dates/{str(val)}\", s2_dates)\n",
    "            np.save(f\"../data/test-raw/{str(val)}\", x_to_save)\n",
    "            file = f\"../data/test-raw/{str(val)}.npy\"\n",
    "            key = f'restoration-mapper/model-data/test/raw/{str(val)}.npy'\n",
    "            uploader.upload(bucket = 'restoration-monitoring', key = key, file = file)\n",
    "            \n",
    "            missing_px = id_missing_px(x)\n",
    "            if len(missing_px) > 0:\n",
    "                print(f\"Deleting {missing_px} because of missing data\")\n",
    "                x = np.delete(x, missing_px, 0)\n",
    "                s2_dates = np.delete(s2_dates, missing_px)\n",
    "            \n",
    "            x = superresolve(x, model)\n",
    "            x = x[:, 16:-16, 16:-16, :]\n",
    "            dem = np.tile(dem.reshape((1, 32, 32, 1)), (x.shape[0], 1, 1, 1))\n",
    "            dem = dem[:, 8:-8, 8:-8, :]\n",
    "            x = np.concatenate([x, dem], axis = -1)\n",
    "            x[:, :, :, -1] /= 90\n",
    "            print(f\"Shape after DEM: {x.shape}\")\n",
    "\n",
    "            # Calculate indices\n",
    "            tiles = evi(x, True)\n",
    "            tiles = bi(tiles, True)\n",
    "            tiles = msavi2(tiles, True)\n",
    "            x = si(tiles, True)\n",
    "            \n",
    "            for band in range(0, 15):\n",
    "                for time in range(0, x.shape[0]):\n",
    "                    x_i = x[time, :, :, band]\n",
    "                    x_i[np.argwhere(np.isnan(x_i))] = np.mean(x_i)\n",
    "                    x[time, :, :, band] = x_i\n",
    "\n",
    "            # Interpolate linearly to 5 day frequency\n",
    "            tiles, max_distance = calculate_and_save_best_images(x, s2_dates)\n",
    "            sm = Smoother(lmbd = 800, size = tiles.shape[0], nbands = 14, dim = 16)\n",
    "            tiles = sm.interpolate_array(tiles)\n",
    "            print(f\"There are {np.sum(np.isnan(tiles))} NA values\")\n",
    "            \n",
    "            if max_distance <= 240:\n",
    "                np.save(output_folder + str(val), tiles)\n",
    "                print(f\"Saved array of {tiles.shape} shape to {val} \\n\")\n",
    "            else:\n",
    "                print(f\"Skipping {val} because there is a {max_distance} distance \\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            logging.fatal(e, exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in reversed(os.listdir(\"../data/test-csv/\")):\n",
    "    if \".csv\" in i:\n",
    "    #if \".csv\" in i:\n",
    "        #if any(x in i for x in [\"africa-west\", \"cameroon\", \"koure\", \"niger\"]):\n",
    "        tile = download_plots(\"../data/test-csv/\" + i, \"../data/test-s2-new/\", image_format = MimeType.TIFF_d16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_raw(plot_id):\n",
    "    \"\"\" Downloads slope and sentinel-2 data for all plots associated\n",
    "        with an input CSV from a collect earth online survey\n",
    "        \n",
    "        Parameters:\n",
    "         data_location (os.path)\n",
    "         output_folder (os.path)\n",
    "        \n",
    "        Creates:\n",
    "         output_folder/{plot_id}.npy\n",
    "    \n",
    "        Returns:\n",
    "         None\n",
    "    \"\"\"\n",
    "    x = np.load(f\"../data/train-raw/{plot_id}\")\n",
    "    x = np.float32(x) / 65535\n",
    "    s2_dates = np.load(f\"../data/train-dates/{plot_id}\")\n",
    "    dem = np.load(f\"../data/train-slope/{plot_id}\")\n",
    "    print(np.max(dem))\n",
    "    \n",
    "    assert x.shape[0] == s2_dates.shape[0]\n",
    "\n",
    "    missing_px = id_missing_px(x)\n",
    "    if len(missing_px) > 0:\n",
    "        print(f\"Deleting {missing_px} because of missing data\")\n",
    "        x = np.delete(x, missing_px, 0)\n",
    "        s2_dates = np.delete(s2_dates, missing_px)\n",
    "\n",
    "    #x[..., 4:] = superresolve(x)\n",
    "    twentym = x[..., 4:]\n",
    "    twentym = np.reshape(twentym, (x.shape[0], 24, 2, 24, 2, 6))\n",
    "    twentym = np.mean(twentym, (2, 4))\n",
    "    twentym = resize(twentym, (x.shape[0], 48, 48, 6), 2)\n",
    "    x[..., 4:] = twentym\n",
    "    x[..., 4:] = superresolve(x)\n",
    "    x = x[:, 16:-16, 16:-16, :]\n",
    "    dem = np.tile(dem.reshape((1, 32, 32, 1)), (x.shape[0], 1, 1, 1))\n",
    "    dem = dem[:, 8:-8, 8:-8, :]\n",
    "    x = np.concatenate([x, dem], axis = -1)\n",
    "    #x[:, :, :, -1] /= 90\n",
    "    print(f\"Shape after DEM: {x.shape}\")\n",
    "\n",
    "    # Calculate indices\n",
    "    tiles = evi(x, True)\n",
    "    tiles = bi(tiles, True)\n",
    "    tiles = msavi2(tiles, True)\n",
    "    x = si(tiles, True)\n",
    "\n",
    "    for band in range(0, 15):\n",
    "        for time in range(0, x.shape[0]):\n",
    "            x_i = x[time, :, :, band]\n",
    "            x_i[np.argwhere(np.isnan(x_i))] = np.mean(x_i)\n",
    "            x[time, :, :, band] = x_i\n",
    "\n",
    "    # Interpolate linearly to 5 day frequency\n",
    "    tiles, max_distance = calculate_and_save_best_images(x, s2_dates)\n",
    "    sm = Smoother(lmbd = 800, size = tiles.shape[0], nbands = 14, dim = 16)\n",
    "    tiles = sm.interpolate_array(tiles)\n",
    "    print(f\"There are {np.sum(np.isnan(tiles))} NA values\")\n",
    "\n",
    "    if max_distance <= 240:\n",
    "        np.save(f\"../data/train-s2-new/{plot_id}\", tiles)\n",
    "        print(f\"Saved array of {tiles.shape} shape to ../data/train-s2-new/{plot_id} \\n\")\n",
    "    else:\n",
    "        print(f\"Skipping {plot_id} because there is a {max_distance} distance \\n\")\n",
    "        \n",
    "    return tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = [x for x in os.listdir(\"../data/train-raw/\") if \".npy\" in x]\n",
    "for plot in plots:\n",
    "    if not os.path.exists(\"../data/train-s2-new/\" + plot):\n",
    "        if not \"137526937\" in plot:\n",
    "            print(plot)\n",
    "            tiles = process_raw(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in reversed(os.listdir(\"../data/train-csv/\")):\n",
    "    if \".csv\" in i:\n",
    "        if \"brazil-sao-paulo-2\" not in i:\n",
    "        #if \".csv\" in i:\n",
    "            #if any(x in i for x in [\"africa-west\", \"cameroon\", \"koure\", \"niger\"]):\n",
    "            fix_dates(\"../data/train-csv/\" + i, \"../data/train-dates/\", image_format = MimeType.TIFF_d16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_sensing",
   "language": "python",
   "name": "remote_sensing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
