{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM - UNET -- 16x16 none padding\n",
    "\n",
    "## John Brandt\n",
    "\n",
    "### Last updated: Oct 3 2019, 75% precision, 76% recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/john.brandt/anaconda3/envs/remote_sensing/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook, tnrange\n",
    "import tensorflow as tf\n",
    "#tf.reset_default_graph()\n",
    "\n",
    "sess = tf.Session()\n",
    "from keras import backend as K\n",
    "K.set_session(sess)\n",
    "\n",
    "import keras\n",
    "from tensorflow.python.keras.layers import *\n",
    "from tensorflow.python.keras.layers import ELU, LeakyReLU\n",
    "from keras.losses import binary_crossentropy\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.keras.layers import Conv2D, Lambda, Dense, Multiply, Add\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import itertools\n",
    "from tflearn.layers.conv import global_avg_pool\n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "from keras.regularizers import l1\n",
    "from tensorflow.layers import batch_normalization\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../src/zoneout.py\n",
    "%run ../src/convgru.py\n",
    "%run ../src/lovasz.py\n",
    "%run ../src/utils.py\n",
    "%run ../src/adabound.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "536 135\n"
     ]
    }
   ],
   "source": [
    "ZONE_OUT_PROB = 0.3 #(0.05, 0.20, 0.05) --> 4\n",
    "L2_REG = 0.005 #(1-e6, 1-e1, x10) --> 5\n",
    "INITIAL_LR = 2e-6 #(1e-6, 1e-3, x5) --> 10\n",
    "FINAL_LR = 2e-4 # (1e - 5, 1e-2, x5) --> 10\n",
    "LOSS_WEIGHTING = 0.5 #(0.2, 1, 0.2) --> 5\n",
    "SQUEEZE_RATIO = 4 # --> 4, 8, 12, 16 --> 4\n",
    "BN_MOMENTUM = 0.9 # --> 3\n",
    "N_LAYERS = 4 # --> 3\n",
    "REG_TYPE = 'kernel' # kernel # --> 2\n",
    "SQUEEZE = True\n",
    "LAYER_NORM = True \n",
    "BATCH_SIZE = 4 # -->4\n",
    "LOSS_TYPE = 'bce-jaccard' #bce-jaccard, bce-dice, bce-lovasz, focal-jaccard, etc. --> 4\n",
    "N_CONV_PER_LAYER = 1 # --> 2\n",
    "ACTIVATION_TYPE = 'ELU' #RELU, PRELU --> 2\n",
    "MASK_LOSS = False # --> 2\n",
    "PAD_INPUT_TYPE = 'none' # zero, reflect, none # --> 2\n",
    "RENORM_CLIPPING = None # --> 5\n",
    "FRESH_START = False\n",
    "TRAIN_RATIO = 0.8\n",
    "TEST_RATIO = 0.2\n",
    "\n",
    "\n",
    "AUGMENTATION_RATIO = 4\n",
    "IMAGE_SIZE = 16\n",
    "existing = [int(x[:-4]) for x in os.listdir('../data/2018/') if \".DS\" not in x]\n",
    "#existing = [x for x in existing1 if x in existing2]\n",
    "N_SAMPLES = len(existing)\n",
    "RESIZE_OUTPUT = False\n",
    "\n",
    "LABEL_SIZE = 14\n",
    "#if LABEL_SIZE == 16 and not RESIZE_OUTPUT:\n",
    "#    LABEL_SIZE = IMAGE_SIZE\n",
    "    \n",
    "TRAIN_SAMPLES = int((N_SAMPLES * AUGMENTATION_RATIO) * TRAIN_RATIO)\n",
    "TEST_SAMPLES = int((N_SAMPLES * AUGMENTATION_RATIO) - TRAIN_SAMPLES)\n",
    "print(TRAIN_SAMPLES // AUGMENTATION_RATIO, N_SAMPLES - (TRAIN_SAMPLES // AUGMENTATION_RATIO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_bn_elu(inp, is_training, kernel_size, scope, filter_count = 16):\n",
    "    if kernel_size == 3:\n",
    "        padded = ReflectionPadding2D((1, 1,))(inp)\n",
    "        padding = 'valid'\n",
    "    else:\n",
    "        padded = inp\n",
    "        padding = 'same'\n",
    "    conv = Conv2D(filters = filter_count, kernel_size = (kernel_size, kernel_size),\n",
    "                      padding = padding, kernel_regularizer=reg)(padded)\n",
    "    elu = ELU()(conv)\n",
    "    bn = Batch_Normalization(elu, training=is_training, scope = scope + \"bn\")\n",
    "    return bn\n",
    "    \n",
    "def fpa(inp, filter_count):\n",
    "    one = conv_bn_elu(inp, is_training, 1, 'forward1', filter_count)\n",
    "    three = conv_bn_elu(inp, is_training, 3, 'down1', filter_count)\n",
    "    three_f = conv_bn_elu(three, is_training, 3, 'down1_f', filter_count)\n",
    "    two = conv_bn_elu(three, is_training, 2, 'down2', filter_count)\n",
    "    two_f = conv_bn_elu(two, is_training, 2, 'down2_f', filter_count)\n",
    "    \n",
    "    # top block\n",
    "    pooled = tf.keras.layers.GlobalAveragePooling2D()(inp)\n",
    "    one_top = conv_bn_elu(tf.reshape(pooled, (-1, 1, 1, pooled.shape[-1])), is_training, 1, 'top1', filter_count)\n",
    "    four_top = tf.keras.layers.UpSampling2D((4, 4))(one_top)\n",
    "    \n",
    "    \n",
    "    concat_1 = tf.multiply(one, tf.add(three_f, two_f))\n",
    "    concat_2 = tf.add(concat_1, four_top)\n",
    "    print(\"Feature pyramid attention shape {}\".format(concat_2.shape))\n",
    "    return concat_2\n",
    "    \n",
    "    \n",
    "\n",
    "def gau(x_low_level, x_high_level, scope, filter_count, size = 4):\n",
    "    \"\"\"\n",
    "    The global attention upsample to replace the up_cat_conv element\n",
    "    \"\"\"\n",
    "    low_feat = conv_bn_elu(x_low_level, is_training, 3, 'gauforward' + scope, filter_count)\n",
    "    high_gap = tf.keras.layers.GlobalAveragePooling2D()(x_high_level)\n",
    "    high_feat = tf.keras.layers.Reshape((1, 1, -1))(high_gap)\n",
    "    high_feat_gate = tf.keras.layers.UpSampling2D((size, size))(high_feat)\n",
    "    gated_low = tf.keras.layers.multiply([low_feat, high_feat_gate])\n",
    "    gated_low = conv_bn_elu(gated_low, is_training, 3, 'gauforward5' + scope, filter_count)\n",
    "    gated_high = tf.keras.layers.Conv2DTranspose(filters = filter_count, kernel_size = (3, 3),\n",
    "                                             strides=(2, 2), padding='same', kernel_regularizer = reg)(gated_low)\n",
    "    high_clamped = conv_bn_elu(x_high_level, is_training, 3, 'gauforward1' + scope, filter_count)\n",
    "    return tf.keras.layers.add([gated_high, high_clamped])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.contrib.slim import conv2d\n",
    "from tensorflow.contrib.resampler import resampler\n",
    "\n",
    "def cse_block(prevlayer, prefix):\n",
    "    mean = Lambda(lambda xin: K.mean(xin, axis=[1, 2]))(prevlayer)\n",
    "    lin1 = Dense(K.int_shape(prevlayer)[3] // 2, name=prefix + 'cse_lin1', activation='relu')(mean)\n",
    "    lin2 = Dense(K.int_shape(prevlayer)[3], name=prefix + 'cse_lin2', activation='sigmoid')(lin1)\n",
    "    x = Multiply()([prevlayer, lin2])\n",
    "    return x\n",
    "\n",
    "\n",
    "def sse_block(prevlayer, prefix):\n",
    "    # Bug? Should be 1 here?\n",
    "    conv = Conv2D(K.int_shape(prevlayer)[3], (1, 1), padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "                  activation='sigmoid', strides=(1, 1),\n",
    "                  name=prefix + \"_conv\")(prevlayer)\n",
    "    conv = Multiply(name=prefix + \"_mul\")([prevlayer, conv])\n",
    "    return conv\n",
    "\n",
    "\n",
    "def csse_block(x, prefix):\n",
    "    '''\n",
    "    Implementation of Concurrent Spatial and Channel ‘Squeeze & Excitation’ in Fully Convolutional Networks\n",
    "    https://arxiv.org/abs/1803.02579\n",
    "    '''\n",
    "    cse = cse_block(x, prefix)\n",
    "    sse = sse_block(x, prefix)\n",
    "    x = Add(name=prefix + \"_csse_mul\")([cse, sse])\n",
    "\n",
    "    return x\n",
    "\n",
    "def Batch_Normalization(x, training, scope):\n",
    "    return batch_normalization(inputs=x, \n",
    "                               momentum = BN_MOMENTUM, \n",
    "                               training=training,\n",
    "                               renorm = True,\n",
    "                               reuse=None,\n",
    "                               name = scope)\n",
    "\n",
    "class ReflectionPadding2D(Layer):\n",
    "    def __init__(self, padding=(1, 1), **kwargs):\n",
    "        self.padding = tuple(padding)\n",
    "        self.input_spec = [InputSpec(ndim=4)]\n",
    "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, s):\n",
    "        \"\"\" If you are using \"channels_last\" configuration\"\"\"\n",
    "        return (s[0], s[1] + 2 * self.padding[0], s[2] + 2 * self.padding[1], s[3])\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        w_pad,h_pad = self.padding\n",
    "        return tf.pad(x, [[0,0], [h_pad,h_pad], [w_pad,w_pad], [0,0] ], 'REFLECT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/john.brandt/Documents/GitHub/collect-earth-automation/src/utils.py:76: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/john.brandt/anaconda3/envs/remote_sensing/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/john.brandt/anaconda3/envs/remote_sensing/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Down block shape: (?, 16, 16, 20)\n",
      "WARNING:tensorflow:From <ipython-input-5-0e3155a70fe2>:41: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.batch_normalization instead.\n",
      "Down block shape: (?, 8, 8, 30)\n",
      "Down block shape: (?, 4, 4, 45)\n",
      "Feature pyramid attention shape (?, 4, 4, 45)\n",
      "Up block conv 1 shape: (?, 8, 8, 30)\n",
      "Up block conv 1 shape: (?, 16, 16, 20)\n",
      "(?, 12, 12, 1)\n"
     ]
    }
   ],
   "source": [
    "weights = tf.ones([14, 14], tf.float32)\n",
    "weights = tf.pad(weights, [[1, 1], [1, 1]], 'constant')\n",
    "weights = tf.reshape(weights, (16*16,)) \n",
    "\n",
    "reg = keras.regularizers.l2(L2_REG)\n",
    "inp = tf.placeholder(tf.float32, shape=(None, 24, IMAGE_SIZE, IMAGE_SIZE, 15))\n",
    "length = tf.placeholder(tf.int32, shape = (None, 1))\n",
    "labels = tf.placeholder(tf.float32, shape=(None, 14, 14))#, 1))\n",
    "\n",
    "\n",
    "\n",
    "length2 = tf.reshape(length, (-1,))\n",
    "is_training = tf.placeholder_with_default(False, (), 'is_training')\n",
    "power = tf.placeholder_with_default(1.0, (), 'power')\n",
    "\n",
    "if PAD_INPUT_TYPE == 'zero':\n",
    "    inp_pad = tf.pad(inp, [[0, 0], [0, 0], [1, 1], [1, 1], [0, 0]], \"CONSTANT\")\n",
    "\n",
    "if PAD_INPUT_TYPE == 'reflect':\n",
    "    inp_pad = tf.pad(inp, [[0, 0], [0, 0], [1,1], [1,1], [0,0] ], 'REFLECT')\n",
    "    \n",
    "if PAD_INPUT_TYPE == 'none':\n",
    "    inp_pad = inp\n",
    "    \n",
    "FILTER_SIZE = LABEL_SIZE if RESIZE_OUTPUT else IMAGE_SIZE\n",
    "\n",
    "down_16 = 10\n",
    "down_8 = 30\n",
    "down_4f = 45\n",
    "#down_2f = 2\n",
    "#up_4 = 30\n",
    "up_8 = 30\n",
    "up_16 = 20\n",
    "\n",
    "def down_block(inp, length, size, flt, scope, train):\n",
    "    with tf.variable_scope(scope):\n",
    "        cell_fw = ConvGRUCell(shape = size, filters = flt,\n",
    "                           kernel = [3, 3], padding = 'SAME')\n",
    "        cell_bw = ConvGRUCell(shape = size, filters = flt,\n",
    "                           kernel = [3, 3], padding = 'SAME')\n",
    "        cell_fw = ZoneoutWrapper(\n",
    "            cell_fw, zoneout_drop_prob = ZONE_OUT_PROB, is_training = train)\n",
    "        cell_bw = ZoneoutWrapper(\n",
    "            cell_bw, zoneout_drop_prob = ZONE_OUT_PROB, is_training = train)\n",
    "        gru = convGRU(inp, cell_fw, cell_bw, length)\n",
    "        down = TimeDistributed(MaxPool2D(pool_size = (2, 2)))(gru[0])\n",
    "        print(\"Down block shape: {}\".format(gru[1].shape))\n",
    "    return down, gru[1]\n",
    "\n",
    "def down_block_no_gru(inp, flt, scope, train):\n",
    "    with tf.variable_scope(scope):\n",
    "        padded = ReflectionPadding2D((1, 1))(inp)\n",
    "        \n",
    "        # Conv block 1\n",
    "        conv = Conv2D(filters = flt, kernel_size = (3, 3),\n",
    "                      padding = 'valid', kernel_regularizer=reg)(padded)\n",
    "        elu = ELU()(conv)\n",
    "        bn = Batch_Normalization(elu, training=is_training, scope = scope + \"bn\")\n",
    "        x = csse_block(bn, prefix='csse_block_{}'.format(scope))\n",
    "        down = MaxPool2D(pool_size = (2, 2))(x)\n",
    "        print(\"Down block shape: {}\".format(down.shape))\n",
    "    return down\n",
    "\n",
    "\n",
    "def up_block(inp, concat_inp, flt, sq, scope, concat, is_training, padding = True):\n",
    "    with tf.variable_scope(scope):\n",
    "        \n",
    "        gau_layer = gau(inp, concat_inp, scope, flt, inp.shape[-2])\n",
    "        x = csse_block(gau_layer, prefix='csse_block_{}'.format(scope))\n",
    "        print(\"Up block conv 1 shape: {}\".format(x.shape))\n",
    "        return x\n",
    "        \n",
    "        \n",
    "down_1, copy_1 = down_block(inp_pad, length2, [FILTER_SIZE, FILTER_SIZE], down_16, 'down_16', is_training)\n",
    "down_2 = down_block_no_gru(copy_1, down_8, 'down_8', is_training)\n",
    "down_3 = down_block_no_gru(down_2, down_4f, 'down_4', is_training)\n",
    "\n",
    "down_fpa = fpa(down_3, down_4f)\n",
    "up_3 = up_block(down_fpa, down_2, up_8, up_8, 'up_8', True, is_training, padding =  True) # 4 - 8\n",
    "up_2 = up_block(up_3, copy_1, up_16, up_16, 'up_16', True, is_training, padding = True) # 8 - 16\n",
    "up_4_16 = tf.keras.layers.Conv2DTranspose(filters = up_8, kernel_size = (3, 3),\n",
    "                                             strides=(2, 2), padding='same', kernel_regularizer = reg)(down_2)\n",
    "\n",
    "up_8_16 = tf.keras.layers.Conv2DTranspose(filters = up_16, kernel_size = (3, 3),\n",
    "                                             strides=(2, 2), padding='same', kernel_regularizer = reg)(up_3)\n",
    "\n",
    "concat_final = tf.concat([up_2, up_4_16, up_8_16], axis = -1)\n",
    "\n",
    "up_2 = Conv2D(filters = 20, kernel_size = (3, 3), padding = 'valid', kernel_regularizer=reg)(concat_final)\n",
    "elu = ELU()(up_2)\n",
    "bn = Batch_Normalization(elu, training=is_training, scope = \"out1bn\")\n",
    "x = csse_block(bn, prefix='csse_block_{}'.format(\"out1\"))\n",
    "\n",
    "up_2 = Conv2D(filters = 20, kernel_size = (3, 3), padding = 'valid', kernel_regularizer=reg)(x)\n",
    "elu = ELU()(up_2)\n",
    "\n",
    "#B = tf.Variable([-np.log(0.99/0.01)]) \n",
    "init = tf.constant_initializer([-np.log(0.9/0.1)])\n",
    "fm = Conv2D(filters = 1,\n",
    "            kernel_size = (1, 1), \n",
    "            padding = 'valid',\n",
    "            activation = 'sigmoid',\n",
    "            bias_initializer = init,\n",
    "            )(elu)\n",
    "print(fm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181413\n"
     ]
    }
   ],
   "source": [
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    # shape is an array of tf.Dimension\n",
    "    shape = variable.get_shape()\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        variable_parameters *= dim.value\n",
    "    total_parameters += variable_parameters\n",
    "print(total_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/subplot.csv\")\n",
    "df1 = pd.read_csv(\"../data/subplot2.csv\")\n",
    "df2 = pd.read_csv(\"../data/subplot3.csv\")\n",
    "df3 = pd.read_csv(\"../data/subplot4.csv\")\n",
    "\n",
    "df = df.drop('IMAGERY_TITLE', axis = 1).dropna(axis = 0)\n",
    "df1 = df1.drop('IMAGERY_TITLE', axis = 1).dropna(axis = 0)\n",
    "df2 = df2.drop('IMAGERY_TITLE', axis = 1).dropna(axis = 0)\n",
    "df3 = df3.drop('IMAGERY_TITLE', axis = 1).dropna(axis = 0)\n",
    "\n",
    "lens = [len(x) for x in [df, df1, df2, df3]]\n",
    "\n",
    "df = pd.concat([df, df1, df2, df3], ignore_index = True)\n",
    "df = df.dropna(axis = 0)\n",
    "\n",
    "#existing1 = [int(x[:-4]) for x in os.listdir('../data/2017_data/') if \".DS\" not in x]\n",
    "existing = [int(x[:-4]) for x in os.listdir('../data/2018/') if \".DS\" not in x]\n",
    "#existing = [x for x in existing1 if x in existing2]\n",
    "N_SAMPLES = len(existing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007645337976493324\n",
      "0.06145349779655118\n",
      "0.02159828769608784\n",
      "0.036396335584602264\n",
      "0.05958174431952329\n",
      "0.015659071604993958\n",
      "0.006922198399429235\n",
      "0.05653792502509732\n",
      "0.03334231339288227\n",
      "0.035555267952816376\n",
      "0.009224977795043464\n",
      "0.051151759717544015\n",
      "0.02033889876651264\n",
      "0.10514105643266275\n",
      "0.010219931060355307\n",
      "0.09422083417625086\n",
      "0.012836200410902783\n",
      "0.03386923365171557\n",
      "0.0412027049343479\n",
      "0.09063004772850719\n",
      "0.06782835446419418\n",
      "0.08799683356683438\n",
      "0.02142840188180541\n",
      "0.01280754804611206\n",
      "0.038316447643567106\n",
      "0.010488472550472375\n",
      "0.07968146568036828\n",
      "0.0779661563323633\n",
      "0.058934999288968996\n",
      "0.011759744035803647\n",
      "0.049817633743075164\n",
      "0.14108447071167649\n",
      "0.04128869794336327\n",
      "0.023929936222841987\n",
      "0.09464384341386474\n",
      "0.037169234125166234\n",
      "0.06561265313823232\n",
      "0.019670492901993925\n",
      "0.033322994324417744\n",
      "0.00968771122466613\n",
      "0.034927496543327496\n",
      "0.009153006574797515\n",
      "0.025065112378307934\n",
      "0.028253784620502886\n",
      "0.02848392781984829\n",
      "0.01902704607403893\n",
      "0.007252629691754645\n",
      "0.023127033726423867\n",
      "0.021971028901605905\n",
      "0.014187926551777738\n",
      "0.03576439206300848\n",
      "0.04524419162729365\n",
      "0.04876765983926623\n",
      "0.05401138866627139\n",
      "0.014587049870063802\n",
      "0.008855792091189238\n",
      "0.06987340159584503\n",
      "0.007691047177621223\n",
      "0.020107030868530273\n",
      "0.009588758196041412\n",
      "0.029316595987711247\n",
      "0.06004810333251953\n",
      "0.023373893858515675\n",
      "0.03207929205350316\n",
      "0.014860741886327591\n",
      "0.04855743819459957\n",
      "0.06744387364161977\n",
      "0.07699044668413507\n",
      "0.04392489912617893\n",
      "0.020309611953657117\n",
      "0.0334987519474879\n",
      "0.01757999009844685\n",
      "0.02350020277113073\n",
      "0.060369334655549396\n",
      "0.06379469458384698\n",
      "0.012983394865592853\n",
      "0.0360655781454708\n",
      "0.020673863438323312\n",
      "0.049000059718822576\n",
      "0.018406564589934678\n",
      "0.04148226976398935\n",
      "0.05119435051106286\n",
      "0.013609230518341064\n",
      "0.01862475342639801\n",
      "0.025743827477698142\n",
      "0.010314275542330793\n",
      "0.03137598674739196\n",
      "0.008312817316189831\n",
      "0.008794965520957838\n",
      "0.029146786146629997\n",
      "0.05487829007464005\n",
      "0.04865808735558485\n",
      "0.026150643825531006\n",
      "0.04437648725863727\n",
      "0.020828828405349675\n",
      "0.019123811422123355\n",
      "0.009706142943130106\n",
      "0.06264765196238849\n",
      "0.01543059189853026\n",
      "0.021979877383075823\n",
      "0.01285086121502696\n",
      "0.011960689565462111\n",
      "0.03054508971138513\n",
      "0.009341006035469444\n",
      "0.02362162735064342\n",
      "0.13982346583744273\n",
      "0.013642609119415283\n",
      "0.03581798740936211\n",
      "0.017147809623785194\n",
      "0.02055204834185082\n",
      "0.021459817886683646\n",
      "0.03729109905219156\n",
      "0.10415645305397284\n",
      "0.013585615149001582\n",
      "0.04621250723155021\n",
      "0.06242615997396874\n",
      "0.02015201218728114\n",
      "0.021311818479483734\n",
      "0.032823039205897034\n",
      "0.017094153954094245\n",
      "0.00720858623814104\n",
      "0.029184885737465638\n",
      "0.023062527179718018\n",
      "0.0520521234917359\n",
      "0.010095834732055664\n",
      "0.01584092264790791\n",
      "0.010547515607923627\n",
      "0.027209630161138983\n",
      "0.03975929767417879\n",
      "0.006547272205352783\n",
      "0.02606131197154949\n",
      "0.020552054487172698\n",
      "0.046197259158111084\n",
      "0.038641975789595856\n",
      "0.03027016162849301\n",
      "0.08067031000706343\n",
      "0.04387845438455222\n",
      "0.01962721347808838\n",
      "0.025444625239127133\n",
      "0.03206827262925359\n",
      "0.012886252213469673\n",
      "0.1296069699088421\n",
      "0.03770176538647749\n",
      "0.022414342589297254\n",
      "0.025181397165094037\n",
      "0.20931696858678198\n",
      "0.013462364674142302\n",
      "0.00994452650193087\n",
      "0.040896629453492636\n",
      "0.03206611078067281\n",
      "0.007606654607954497\n",
      "0.04449726670040463\n",
      "0.010168733744021628\n",
      "0.011675930070636185\n",
      "0.056894281331241414\n",
      "0.032879728177272485\n",
      "0.021815276536772586\n",
      "0.012777557194406082\n",
      "0.04350851727577128\n",
      "0.03648166818654085\n",
      "0.010170622248422318\n",
      "0.013341584896123507\n",
      "0.013955253037674156\n",
      "0.14736832388766202\n",
      "0.1603025233190779\n",
      "0.01488854362825753\n",
      "0.012578382930639419\n",
      "0.048989792470477495\n",
      "0.05784845337445738\n",
      "0.027339580658853287\n",
      "0.0810864284757165\n",
      "0.02077714263799409\n",
      "0.03382444831390276\n",
      "0.009949606527966755\n",
      "0.008550541894244475\n",
      "0.018965660049991902\n",
      "0.040752291679382324\n",
      "0.035381603216715456\n",
      "0.0861351563904568\n",
      "0.09820617944503368\n",
      "0.03701687190366401\n",
      "0.012844450206929862\n",
      "0.07421629561063336\n",
      "0.015431829141069242\n",
      "0.01962721347808838\n",
      "0.014426542183103266\n",
      "0.0316030557569129\n",
      "0.021428465843200684\n",
      "0.016286806675518142\n",
      "0.048472816124972694\n",
      "0.012725397970894236\n",
      "0.014692037091947436\n",
      "0.013643211037919878\n",
      "0.011441604965252403\n",
      "0.04228546136652453\n",
      "0.010007772595743522\n",
      "0.08241772651672363\n",
      "0.029715491353466635\n",
      "0.018148720264434814\n",
      "0.015282630920410156\n",
      "0.02546849028947313\n",
      "0.0318838982648051\n",
      "0.01602928942566322\n",
      "0.07801633424376335\n",
      "0.10371598262497739\n",
      "0.08213361678807803\n",
      "0.03460227315798145\n",
      "0.015180063155511767\n",
      "0.029744270912510827\n",
      "0.029887309566542432\n",
      "0.021814644337026158\n",
      "0.036499551833476605\n",
      "0.06825768272333824\n",
      "0.04521600966208818\n",
      "0.04012249469859185\n",
      "0.038607939401935805\n",
      "0.04144351272040191\n",
      "0.01940312336129748\n",
      "0.05812926657725161\n",
      "0.02848021680297999\n",
      "0.011204316388864606\n",
      "0.0368923102724028\n",
      "0.008968710899353027\n",
      "0.034131589380850015\n",
      "0.04213714655631218\n",
      "0.03671038836552616\n",
      "0.07906985586894338\n",
      "0.006026923295410901\n",
      "0.011811126500321461\n",
      "0.02353746496458718\n",
      "0.022808533307431017\n",
      "0.02992603321837874\n",
      "0.06636383905484285\n",
      "0.05857405263946464\n",
      "0.012342875615659252\n",
      "0.01644843435738334\n",
      "0.018144781753100653\n",
      "0.01839726881372884\n",
      "0.013827621937311954\n",
      "0.037077721537067426\n",
      "0.009384767053870466\n",
      "0.04908193194857442\n",
      "0.038766951257713726\n",
      "0.008104417500159814\n",
      "0.015477724815513553\n",
      "0.015438039991644852\n",
      "0.03408324718558731\n",
      "0.015094280242919922\n",
      "0.0121184587484501\n",
      "0.016502387701974302\n",
      "0.022676636984171553\n",
      "0.02016776598842267\n",
      "0.04830096102087478\n",
      "0.009496021352640771\n",
      "0.024122364304493268\n",
      "0.00931471586227417\n",
      "0.027588236732792287\n",
      "0.012763330134554041\n",
      "0.02084347417614555\n",
      "0.023275540232847447\n",
      "0.009705853818403738\n",
      "0.015552043914794922\n",
      "0.05275535579307347\n",
      "0.04922929962461835\n",
      "0.04065813797159595\n",
      "0.057420593043855\n",
      "0.028872274396817974\n",
      "0.016161799430847168\n",
      "0.032024018464669\n",
      "0.011225120049841334\n",
      "0.03926999397987164\n",
      "0.03959632212219819\n",
      "0.025931844431031553\n",
      "0.02834911312544982\n",
      "0.028498955987402617\n",
      "0.028650868910770122\n",
      "0.026048260081632774\n",
      "0.07959796482728088\n",
      "0.16946287650390712\n",
      "0.01137537095059673\n",
      "0.016449592903071075\n",
      "0.06295145404771642\n",
      "0.012951576707832648\n",
      "0.05459321537740592\n",
      "0.05515694618225098\n",
      "0.040758467459524474\n",
      "0.017458248052893574\n",
      "0.04369483764785355\n",
      "0.02692891112253008\n",
      "0.043795287200697114\n",
      "0.028834987568716313\n",
      "0.024910204768940944\n",
      "0.010478476547660438\n",
      "0.02962052822113037\n",
      "0.008463930117957739\n",
      "0.057061598232314166\n",
      "0.011012337391610585\n",
      "0.007842920627570527\n",
      "0.014050756395984432\n",
      "0.05681652187313858\n",
      "0.007807544596310716\n",
      "0.0117221045338118\n",
      "0.043533892668403275\n",
      "0.02867838628903718\n",
      "0.02355017270415778\n",
      "0.017439165494499496\n",
      "0.013405572741671454\n",
      "0.06915454903667492\n",
      "0.011330202435003961\n",
      "0.033647211120779855\n",
      "0.009868992597911793\n",
      "0.020335479660792095\n",
      "0.02957373068840071\n",
      "0.05401124750756181\n",
      "0.017481683837712\n",
      "0.01005834539712486\n",
      "0.00852818976007413\n",
      "0.021647782505217688\n",
      "0.018491487393053067\n",
      "0.04123074461174741\n",
      "0.029239654541015625\n",
      "0.07130367220170673\n",
      "0.014776959658983273\n",
      "0.04307419828199367\n",
      "0.015581627785608626\n",
      "0.06019193115579473\n",
      "0.008280968547332086\n",
      "0.05140397798094894\n",
      "0.07136141666437548\n",
      "0.022172927856445312\n",
      "0.026511663159568547\n",
      "0.05279439778494199\n",
      "0.06848506097217817\n",
      "0.015138099675030176\n",
      "0.05142817401225332\n",
      "0.01162797212600708\n",
      "0.010200885949630301\n",
      "0.009257511334369776\n",
      "0.01381127716777025\n",
      "0.014601805593324909\n",
      "0.011491338246495258\n",
      "0.01682967119600602\n",
      "0.015059203283721557\n",
      "0.019975413728617587\n",
      "0.010142078183157741\n",
      "0.013758988159048946\n",
      "0.023657672839493438\n",
      "0.02027735165728614\n",
      "0.026925086976153242\n",
      "0.015432739561850551\n",
      "0.013510704611272205\n",
      "0.015299856663214657\n",
      "0.006550047442568967\n",
      "0.01415966988477669\n",
      "0.014952480793474467\n",
      "0.007857015499400865\n",
      "0.0179848082592381\n",
      "0.016747912833255368\n",
      "0.0120844128758388\n",
      "0.012389150621088614\n",
      "0.013199865818158255\n",
      "0.006096228914907001\n",
      "0.016678693566669678\n",
      "0.010799169540405273\n",
      "0.009389313174785476\n",
      "0.013034343719482422\n",
      "0.00982450107084729\n",
      "0.006515394632820918\n",
      "0.016631274405842552\n",
      "0.0158076552343966\n",
      "0.02923154711364548\n",
      "0.008030660515265749\n",
      "0.014398812885718124\n",
      "0.008750105849030975\n",
      "0.014888644218444824\n",
      "0.023426714167714326\n",
      "0.017692971913728303\n",
      "0.009824253101927015\n",
      "0.008875727653503418\n",
      "0.014054438567608424\n",
      "0.0146065679239453\n",
      "0.018885781312687408\n",
      "0.01715734303556643\n",
      "0.01582720608260267\n",
      "0.015731453895568848\n",
      "0.016181584717809148\n",
      "0.016654007084875477\n",
      "0.008305609226226807\n",
      "0.00976581476422579\n",
      "0.01715820305502177\n",
      "0.01872778931462828\n",
      "0.012460947036743164\n",
      "0.010325235785822114\n",
      "0.009517967700958252\n",
      "0.013676285743713379\n",
      "0.020553671941580965\n",
      "0.01884739678252887\n",
      "0.038979985932088436\n",
      "0.010096841438377871\n",
      "0.012852473426368367\n",
      "0.021407826605293417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0192183256149292\n",
      "0.016260211730023465\n",
      "0.010886392601201434\n",
      "0.011806780598917432\n",
      "0.009065787114693306\n",
      "0.016342339514959767\n",
      "0.009300863214791337\n",
      "0.011746570318606311\n",
      "0.007826669768473021\n",
      "0.006592697054566568\n",
      "0.013511397083469214\n",
      "0.008127598266840804\n",
      "0.011377285417197323\n",
      "0.028775057896625482\n",
      "0.018122122693820047\n",
      "0.008973556518103427\n",
      "0.008680522441864014\n",
      "0.011634100694462062\n",
      "0.007619939479441081\n",
      "0.007148474810910297\n",
      "0.009024730585149646\n",
      "0.008998907945307293\n",
      "0.018782589230388835\n",
      "0.008348447753037891\n",
      "0.013030522706248463\n",
      "0.010190918533951186\n",
      "0.012902840429649034\n",
      "0.01038880692864582\n",
      "0.008081236733478821\n",
      "0.01798542137498617\n",
      "0.0077663997751024425\n",
      "0.008735686635108155\n",
      "0.010827441957944856\n",
      "0.02318074879731069\n",
      "0.010109752757965981\n",
      "0.015701287705834666\n",
      "0.0212993737408718\n",
      "0.007985774124382148\n",
      "0.011556109227826546\n",
      "0.02151114018714123\n",
      "0.014084279537200928\n",
      "0.010037507569483312\n",
      "0.011449421620793307\n",
      "0.019672554427026596\n",
      "0.00631680737695709\n",
      "0.02912059752314643\n",
      "0.020845438938110836\n",
      "0.02066204352015905\n",
      "0.013357325357498463\n",
      "0.028139650745934025\n",
      "0.009894346041820174\n",
      "0.025727384051492046\n",
      "0.01084721329204485\n",
      "0.008203462594160413\n",
      "0.018339553585487448\n",
      "0.010162153979202835\n",
      "0.011345987637353958\n",
      "0.01439399170344053\n",
      "0.024876584334074442\n",
      "0.010839998722076416\n",
      "0.007203786026469686\n",
      "0.009921788095095831\n",
      "0.011173958265037687\n",
      "0.008198404972339106\n",
      "0.019436688362747537\n",
      "0.010705374274085837\n",
      "0.008777081966400146\n",
      "0.009350246663084393\n",
      "0.0070185267743551084\n",
      "0.007698543348999904\n",
      "0.015932547862769618\n",
      "0.02655191930956308\n",
      "0.00911214864805529\n",
      "0.011725311298402786\n",
      "0.010977353268105351\n",
      "0.010220677025474929\n",
      "0.016556084156036377\n",
      "0.013095974922722742\n",
      "0.01757224340325344\n",
      "0.029524710318851486\n",
      "0.024956687020470835\n",
      "0.01074671745300293\n",
      "0.017017195187042543\n",
      "0.02576015380985843\n",
      "0.22981670576130678\n",
      "0.03282480855725449\n",
      "1.7677669529663689\n",
      "0.30327439855659194\n",
      "0.015552187100519898\n",
      "0.028920324511205263\n",
      "0.03694518702071754\n",
      "0.01992856432781146\n",
      "0.06127035029604544\n",
      "0.035211145877838135\n",
      "0.04351519599961608\n",
      "0.0272694022019293\n",
      "0.05122959613800049\n",
      "0.023546385364723726\n",
      "0.03275821653660728\n",
      "0.02994578261728592\n",
      "0.04280187652058734\n",
      "0.03236161469211977\n",
      "0.03229763408913265\n",
      "0.03731317085847434\n",
      "0.031780409651154536\n",
      "0.012431200364952297\n",
      "0.01854151487350464\n",
      "0.029835754060861884\n",
      "0.02543862763918603\n",
      "0.052385123824436955\n",
      "0.007779043829655711\n",
      "0.03135725529210516\n",
      "0.01961482311777139\n",
      "0.07311958859628166\n",
      "0.01857280697008841\n",
      "0.03212004899978638\n",
      "0.04518258773727837\n",
      "0.03135725529210516\n",
      "0.033995648008887124\n",
      "0.0\n",
      "0.1208030748684205\n",
      "0.02476717405894177\n",
      "0.0\n",
      "0.04225260087573335\n",
      "0.0600963830947876\n",
      "0.027675938809723424\n",
      "0.02774290620425157\n",
      "0.024383216269007524\n",
      "0.03897079673900131\n",
      "0.030348201159459996\n",
      "0.01665182164164724\n",
      "0.006175339221954346\n",
      "0.3194389298601521\n",
      "0.014593185383298308\n",
      "0.016387604746334827\n",
      "0.025880984080481333\n",
      "0.05763082133329449\n",
      "0.0248680745515973\n",
      "0.04619992563651785\n",
      "0.029499537098402737\n",
      "0.030927956104278564\n",
      "0.03579974174499512\n",
      "0.025458335876464844\n",
      "0.038033273039772476\n",
      "0.04816468741776414\n",
      "0.0731534550097297\n",
      "0.01922775976711945\n",
      "0.025728940963745117\n",
      "0.2840914726257574\n",
      "0.013105530078675737\n",
      "0.027470348527579402\n",
      "0.015298878601930554\n",
      "0.23971555765509478\n",
      "0.054229795932769775\n",
      "0.009506138803057673\n",
      "0.007165019483802879\n",
      "0.01481534302291673\n",
      "0.026393649637385126\n",
      "0.08963286864044401\n",
      "0.01763671636581421\n",
      "0.05140397798094894\n",
      "0.0240437187636472\n",
      "0.010561374679858623\n",
      "0.10352193712136293\n",
      "0.019329284074918466\n",
      "0.024304947637132503\n",
      "0.016939892765296893\n",
      "0.027949533949112607\n",
      "0.011310810381801107\n",
      "0.015507132362252506\n",
      "0.010550315648780235\n",
      "0.0675861532185158\n",
      "0.33421706459277545\n",
      "0.05521178865286636\n",
      "0.0531918406487847\n",
      "0.03344664885290332\n",
      "0.038759708404541016\n",
      "0.01896742247377081\n",
      "0.03607685938835507\n",
      "0.02588031768029387\n",
      "0.018127398629056345\n",
      "0.04333611814363679\n",
      "0.014694416491433191\n",
      "0.02442494164903331\n",
      "0.02827917124866391\n",
      "0.040438445571236284\n",
      "0.017530443572308693\n",
      "0.01846284845868224\n",
      "0.047769858706283856\n",
      "0.05541803351502314\n",
      "0.007327651082104036\n",
      "0.028091433502294462\n",
      "0.18856331436379947\n",
      "0.008262122797084633\n",
      "0.03483268442031348\n",
      "0.037324656548751015\n",
      "0.023047115787560028\n",
      "0.021035982749355964\n",
      "0.013326004548687724\n",
      "0.04283548663027616\n",
      "0.03419221292021571\n",
      "0.03142700832709723\n",
      "0.023504601011990034\n",
      "0.021657579575808005\n",
      "0.02273624973882567\n",
      "0.0463910781658804\n",
      "0.03480285406112671\n",
      "0.020144611266804082\n",
      "0.04854381084442139\n",
      "0.06243781652409192\n",
      "0.015815494389291977\n",
      "0.028815144105878744\n",
      "0.024253683936131414\n",
      "0.012268877742667586\n",
      "0.024898461492945004\n",
      "0.013139864131133727\n",
      "0.014596163305991259\n",
      "0.07405912426311725\n",
      "0.05307883024215698\n",
      "0.019474208071815893\n",
      "0.027402908829904676\n",
      "0.008849784518584014\n",
      "0.017075838403539243\n",
      "0.14416495088792125\n",
      "0.13311251731444163\n",
      "0.4999041363208704\n",
      "0.03347408109752795\n",
      "0.05473876319922573\n",
      "0.13642121443384442\n",
      "0.09207130785863299\n",
      "0.05410873097303401\n",
      "0.039193206141799955\n",
      "0.03562251636140763\n",
      "0.01562678702238418\n",
      "0.04001937195994899\n",
      "0.013581068415018417\n",
      "0.04106520603424187\n",
      "0.2117087198389283\n",
      "0.013701361807558989\n",
      "0.19031680222622188\n",
      "0.046326931718824384\n",
      "0.031813683204145425\n",
      "0.02904038736924107\n",
      "0.08349896156414188\n",
      "0.2525371727815153\n",
      "0.025030170393649746\n",
      "0.0060535793029094015\n",
      "0.03281216450270122\n",
      "0.08751335158283807\n",
      "0.02391580771902171\n",
      "0.047175369017932775\n",
      "0.021201558961184655\n",
      "0.10477781295776367\n",
      "0.05576236848630008\n",
      "0.011504530907294867\n",
      "0.045110366018959125\n",
      "0.024464883682123503\n",
      "0.06976723670959473\n",
      "0.011295080184936523\n",
      "0.024735442158233284\n",
      "0.15371882882590293\n",
      "0.018062110680983755\n",
      "0.03562251636140763\n",
      "0.03851333946215805\n",
      "0.027071174815289367\n",
      "0.023196057725194896\n",
      "0.04285988933490038\n",
      "0.03601693074067085\n",
      "0.0295536802799281\n",
      "0.022684908106767085\n",
      "Finished data loading\n",
      "(2684, 14, 14, 1)\n"
     ]
    }
   ],
   "source": [
    "df = df[df['PLOT_ID'].isin(existing)]\n",
    "N_SAMPLES = int(df.shape[0]/196)\n",
    "N_YEARS = 1\n",
    "\n",
    "plot_ids = sorted(df['PLOT_ID'].unique())\n",
    "\n",
    "locs_ls = []\n",
    "\n",
    "def reconstruct_images(plot_id):\n",
    "    subs = df[df['PLOT_ID'] == plot_id]\n",
    "    rows = []\n",
    "    lats = reversed(sorted(subs['LAT'].unique()))\n",
    "    for i, val in enumerate(lats):\n",
    "        subs_lat = subs[subs['LAT'] == val]\n",
    "        subs_lat = subs_lat.sort_values('LON', axis = 0)\n",
    "        rows.append(list(subs_lat['TREE']))\n",
    "    return rows\n",
    "\n",
    "data = [reconstruct_images(x) for x in plot_ids]\n",
    "\n",
    "# Initiate empty lists to store the X and Y data in\n",
    "data_x, data_y, lengths = [], [], []\n",
    "\n",
    "# Iterate over each plot\n",
    "pad = True\n",
    "flip = True\n",
    "for i in plot_ids:\n",
    "    # Load the sentinel imagery\n",
    "    for year in [\"2018\"]: #\"2017_data\", \n",
    "        x = np.load(\"../data/\" + year + \"/\" + str(i) + \".npy\")\n",
    "        # Shape check\n",
    "        x = ndvi(x, image_size = 16)\n",
    "        x = evi(x, image_size = 16)\n",
    "        x = savi(x, image_size = 16)\n",
    "        x = remove_blank_steps(x)\n",
    "        x_grad, y_grad = np.gradient(np.reshape(x[0, :, :, 10], (16, 16)))\n",
    "        #x[:, :, :, 10] = (x[:, :, :, 10] - np.min(x[:, :, :, 10]) / np.max(x[:, :, :, 10])\n",
    "        mag = np.stack([np.reshape(np.sqrt(x_grad**2 + y_grad**2)*10, (16, 16, 1))]*x.shape[0])\n",
    "        #if np.max(mag) > 0:\n",
    "        #    mag = (mag - np.min(mag)) / np.max(mag)\n",
    "        x = np.concatenate([x, mag], axis = -1)\n",
    "        print(np.max(x[:, :, :, 14]))\n",
    "        y = reconstruct_images(i)\n",
    "        lengths.append(x.shape[0])\n",
    "        if x.shape[0] < 24:\n",
    "            padding = np.zeros((24 - x.shape[0], IMAGE_SIZE, IMAGE_SIZE, 13))\n",
    "            x = np.concatenate((x, padding), axis = 0)\n",
    "        data_x.append(x)\n",
    "        data_y.append(y)\n",
    "        if flip:\n",
    "                # FLIP HORIZONTAL\n",
    "            x1 = np.flip(x, 1)\n",
    "            data_x.append(x1)\n",
    "            data_y.append(np.flip(y, 0))\n",
    "            lengths.append(x.shape[0])\n",
    "\n",
    "                # FLIP BOTH\n",
    "            x2 = np.flip(x, 2)\n",
    "            x2 = np.flip(x2, 1)\n",
    "            data_x.append(x2)\n",
    "            data_y.append(np.flip(y, [0, 1]))\n",
    "            lengths.append(x.shape[0])\n",
    "                # FLIP VERTICAL\n",
    "            x3 = np.flip(x, 2)\n",
    "            data_x.append(x3)\n",
    "            data_y.append(np.flip(y, 1))\n",
    "            lengths.append(x.shape[0])\n",
    "\n",
    "data_x = np.stack(data_x)\n",
    "data_y = np.stack(data_y)\n",
    "data_y = np.reshape(data_y, (N_SAMPLES*4*N_YEARS, 14, 14, 1))\n",
    "lengths = np.stack(lengths)\n",
    "lengths = np.reshape(lengths, (lengths.shape[0], 1))\n",
    "\n",
    "if PAD_INPUT_TYPE == 'zero' and RESIZE_OUTPUT:\n",
    "    data_y = np.pad(data_y, [[0, 0], [1, 1], [1, 1], [0, 0]], 'constant')\n",
    "    \n",
    "if PAD_INPUT_TYPE == 'reflect' and RESIZE_OUTPUT:\n",
    "    data_y = np.pad(data_y, [[0, 0], [1, 1], [1, 1], [0, 0]], 'reflect')\n",
    "print(\"Finished data loading\")\n",
    "print(data_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200, 136, 162, 191]\n"
     ]
    }
   ],
   "source": [
    "len2 = [x//196 for x in lens]\n",
    "print(len2)\n",
    "MULT = 4 * N_YEARS\n",
    "\n",
    "ordering = [[x for x in range(0,int(200*TRAIN_RATIO))],\n",
    "            [x for x in range(200,200+(int(136*TRAIN_RATIO)))], \n",
    "            [x for x in range(200+136,200+136+(int(162*TRAIN_RATIO)))],\n",
    "           [x for x in range(200+136+162,200+136+162+(int(len2[3]*TRAIN_RATIO)))]]\n",
    "\n",
    "ordering = [item for sublist in ordering for item in sublist]\n",
    "test_ordering = [x for x in range(0, N_SAMPLES) if x not in ordering]\n",
    "ordering = test_ordering + ordering\n",
    "ordering = [[x*MULT, (x*MULT)+1, (x*MULT)+2, (x*MULT)+3] for x in ordering]\n",
    "ordering = [item for sublist in ordering for item in sublist]\n",
    "#randomized = [[x*4, (x*4)+1, (x*4)+2, (x*4)+3] for x in ordering]\n",
    "##shuffle(randomized)\n",
    "#randomized = [item for sublist in randomized for item in sublist]\n",
    "\n",
    "#randomized = [x for x in range(0, N_SAMPLES)]\n",
    "#shuffle(randomized)\n",
    "#randomized = [[x*4, (x*4)+1, (x*4)+2, (x*4)+3] for x in randomized]\n",
    "#randomized = [item for sublist in randomized for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215 93 102 91 104 26 40\n"
     ]
    }
   ],
   "source": [
    "data_x = data_x[ordering]\n",
    "data_y = data_y[ordering]\n",
    "lengths = lengths[ordering]\n",
    "\n",
    "\n",
    "percs = [sum(sum(val)) for x, val in enumerate(data_y) if x % MULT == 0]\n",
    "percs = np.array(percs).flatten()\n",
    "zero = len([x for x in percs if x == 0])# number with 0\n",
    "one = len([x for x in percs if 0 < x <= 8])\n",
    "two = len([x for x in percs if 8 < x <= 20])\n",
    "three = len([x for x in percs if 20 < x <= 35])\n",
    "four = len([x for x in percs if 35 < x <= 70])\n",
    "five = len([x for x in percs if 70 < x <= 100])\n",
    "six = len([x for x in percs if 100 < x])\n",
    "\n",
    "print(\"{} {} {} {} {} {} {}\".format(zero, one, two, three, four, five, six))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "671"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(percs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [x for x in range(0, len(percs))]\n",
    "zero_ids = [x for x, z in zip(idx, percs) if z == 0]\n",
    "one_ids = [x for x, z in zip(idx, percs) if 0 < z <= 8]\n",
    "two_ids = [x for x, z in zip(idx, percs) if 8 < z <= 20]\n",
    "three_ids = [x for x, z in zip(idx, percs) if 20 < z <= 35]\n",
    "four_ids = [x for x, z in zip(idx, percs) if 35 < z <= 70]\n",
    "five_ids = [x for x, z in zip(idx, percs) if 70 < z < 100]\n",
    "six_ids = [x for x, z in zip(idx, percs) if 100 < z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = (zero_ids[(int(len(zero_ids) * TEST_RATIO)):] + \n",
    "             one_ids[(int(len(one_ids) * TEST_RATIO)):] +\n",
    "             two_ids[(int(len(two_ids) * TEST_RATIO)):] + \n",
    "             three_ids[(int(len(three_ids) * TEST_RATIO)):] + \n",
    "             four_ids[(int(len(four_ids) * TEST_RATIO)):] + \n",
    "             five_ids[(int(len(five_ids) * TEST_RATIO)):] + \n",
    "             six_ids[(int(len(six_ids) * TEST_RATIO)):])\n",
    "\n",
    "test_ids = (zero_ids[:(int(len(zero_ids) * TEST_RATIO))] + \n",
    "             one_ids[:(int(len(one_ids) * TEST_RATIO))] +\n",
    "             two_ids[:(int(len(two_ids) * TEST_RATIO))] + \n",
    "             three_ids[:(int(len(three_ids) * TEST_RATIO))] + \n",
    "             four_ids[:(int(len(four_ids) * TEST_RATIO))] + \n",
    "             five_ids[:(int(len(five_ids) * TEST_RATIO))] + \n",
    "             six_ids[:(int(len(six_ids) * TEST_RATIO))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_ids = [[(x*8), (x*8)+1, (x*8)+2, (x*8)+3, (x*8)+4, (x*8)+5, (x*8)+6, (x*8)+7] for x in train_ids]\n",
    "train_ids = [[(x*MULT), (x*MULT)+1, (x*MULT)+2, (x*MULT)+3] for x in train_ids]\n",
    "train_ids = [item for sublist in train_ids for item in sublist]\n",
    "\n",
    "test_ids = [x*4 for x in test_ids]\n",
    "#test_ids = [item for sublist in test_ids for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def bin_foc(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: A tensor of the same shape as `y_pred`\n",
    "        :param y_pred:  A tensor resulting from a sigmoid\n",
    "        :return: Output tensor.\n",
    "        \"\"\"\n",
    "        y_pred = tf.reshape(y_pred, (-1, 14,14))\n",
    "        #y_true = tf.reshape(y_true, (-1, 14*14))\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "        epsilon = K.epsilon()\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n",
    "        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n",
    "\n",
    "        return -K.sum(0.5 * K.pow(1. - pt_1, 2) * K.log(pt_1)) \\\n",
    "               -K.sum((1 - 0.5) * K.pow(pt_0, 2) * K.log(1. - pt_0))  \n",
    "    \n",
    "def focal_loss(target_tensor, prediction_tensor, weights=None, alpha=0.25, gamma=1.5):\n",
    "    sigmoid_p = tf.reshape(prediction_tensor, (-1, 14, 14))\n",
    "    zeros = array_ops.zeros_like(sigmoid_p, dtype=sigmoid_p.dtype)\n",
    "    \n",
    "    # For poitive prediction, only need consider front part loss, back part is 0;\n",
    "    # target_tensor > zeros <=> z=1, so poitive coefficient = z - p.\n",
    "    pos_p_sub = array_ops.where(target_tensor > zeros, target_tensor - sigmoid_p, zeros)\n",
    "    \n",
    "    # For negative prediction, only need consider back part loss, front part is 0;\n",
    "    # target_tensor > zeros <=> z=1, so negative coefficient = 0.\n",
    "    neg_p_sub = array_ops.where(target_tensor > zeros, zeros, sigmoid_p)\n",
    "    per_entry_cross_ent = - alpha * (pos_p_sub ** gamma) * tf.log(tf.clip_by_value(sigmoid_p, 1e-8, 1.0)) \\\n",
    "                          - (1 - alpha) * (neg_p_sub ** gamma) * tf.log(tf.clip_by_value(1.0 - sigmoid_p, 1e-8, 1.0))\n",
    "    return tf.reduce_sum(per_entry_cross_ent)\n",
    "\n",
    "def foc_lovasz(y_true, y_pred):\n",
    "    #jaccard_loss = jaccard_distance(y_true, y_pred)\n",
    "    lovasz = lovasz_hinge(y_pred, y_true)\n",
    "    #pred_reshape = tf.reshape(y_pred, (-1, 14, 14))\n",
    "    #true_reshape = tf.reshape(y_true, (-1, 14, 14))\n",
    "    focal_loss = bin_foc(y_true, y_pred)\n",
    "    summed = lovasz + np.log(focal_loss)\n",
    "    return summed\n",
    "\n",
    "def weighted_cross_entropy(y_true, y_pred):\n",
    "    y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "    y_pred = tf.log(y_pred / (1 - y_pred))\n",
    "    loss = tf.nn.weighted_cross_entropy_with_logits(logits=y_pred, targets=y_true, pos_weight=1.5)\n",
    "    return loss\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return 1. - score\n",
    "\n",
    "def smooth_jaccard(y_true, y_pred, smooth=1):\n",
    "    y_true = tf.reshape(y_true, (-1, 12*12))\n",
    "    y_pred = tf.reshape(y_pred, (-1, 12*12))\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return (1 - jac) * smooth\n",
    "\n",
    "def bce_dice(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, (-1, 14, 14, 1))\n",
    "    return 0.5*binary_crossentropy(y_true, y_pred) + (dice_loss(y_true, y_pred))\n",
    "\n",
    "\n",
    "def bce_lovasz(y_true, y_pred):\n",
    "    #return 0.5*binary_crossentropy(tf.reshape(y_true, (-1, 14, 14, 1)), y_pred) + \n",
    "    y_true = tf.reshape(y_true, (-1, 14, 14, 1))\n",
    "    return lovasz_softmax(y_pred, y_true, classes=[1], per_image=True)\n",
    "\n",
    "def focal_loss_fixed(y_true, y_pred, gamma = 2., alpha = 0.25):\n",
    "    y_true = tf.reshape(y_true, (-1, 14, 14, 1))\n",
    "    pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "    pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "    return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(K.epsilon()+pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0 + K.epsilon()))\n",
    "\n",
    "def focal_dice(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, (-1, 14, 14, 1))\n",
    "    foc = focal_loss_fixed(y_true, y_pred, gamma = 0.5, alpha = 0.25)\n",
    "    foc = foc / 196\n",
    "    dice = dice_loss(y_true, y_pred)\n",
    "    return 0.5*foc + tf.log(dice)\n",
    "\n",
    "def foc_jaccard(y_true, y_pred):\n",
    "    jac = dice_loss(y_true, y_pred)\n",
    "    foc = focal_loss_fixed(y_true, y_pred, gamma = 1.3, alpha = 0.25)\n",
    "    return (foc / 196) + 0.5*jac\n",
    "\n",
    "\n",
    "def soft_dice_loss(y_true, y_pred, epsilon=1e-6): \n",
    "    y_true = tf.reshape(y_true, (-1, 14, 14, 1))\n",
    "    print(y_true.shape)\n",
    "    print(y_pred.shape)\n",
    "    ''' \n",
    "    Soft dice loss calculation for arbitrary batch size, number of classes, and number of spatial dimensions.\n",
    "    Assumes the `channels_last` format.\n",
    "  \n",
    "    # Arguments\n",
    "        y_true: b x X x Y( x Z...) x c One hot encoding of ground truth\n",
    "        y_pred: b x X x Y( x Z...) x c Network output, must sum to 1 over c channel (such as after softmax) \n",
    "        epsilon: Used for numerical stability to avoid divide by zero errors\n",
    "    \n",
    "    # References\n",
    "        V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation \n",
    "        https://arxiv.org/abs/1606.04797\n",
    "        More details on Dice loss formulation \n",
    "        https://mediatum.ub.tum.de/doc/1395260/1395260.pdf (page 72)\n",
    "        \n",
    "        Adapted from https://github.com/Lasagne/Recipes/issues/99#issuecomment-347775022\n",
    "    '''\n",
    "    # skip the batch and class axis for calculating Dice score\n",
    "    axes = tuple(range(1, len(y_pred.shape)-1)) \n",
    "    numerator = 2. * np.sum(y_pred * y_true, (1, 2))\n",
    "    denominator = np.sum(np.square(y_pred) + np.square(y_true), axes)\n",
    "    \n",
    "    return 1 - np.mean(numerator / (denominator + epsilon)) # average over classes and batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import binary_crossentropy\n",
    "import keras.backend as K\n",
    "import tensorflow as tf \n",
    "\n",
    "epsilon = 1e-5\n",
    "smooth = 1\n",
    "\n",
    "def dsc(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    score = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return score\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    loss = 1 - dsc(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "def log_dice(y_true, y_pred):\n",
    "    loss = tf.log(dsc(y_true, y_pred))\n",
    "    return loss\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    loss = 0.5*binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "def confusion(y_true, y_pred):\n",
    "    smooth=1\n",
    "    y_pred_pos = K.clip(y_pred, 0, 1)\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "    y_pos = K.clip(y_true, 0, 1)\n",
    "    y_neg = 1 - y_pos\n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg) \n",
    "    prec = (tp + smooth)/(tp+fp+smooth)\n",
    "    recall = (tp+smooth)/(tp+fn+smooth)\n",
    "    return prec, recall\n",
    "\n",
    "def tp(y_true, y_pred):\n",
    "    smooth = 1\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    tp = (K.sum(y_pos * y_pred_pos) + smooth)/ (K.sum(y_pos) + smooth) \n",
    "    return tp \n",
    "\n",
    "def tn(y_true, y_pred):\n",
    "    smooth = 1\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    y_neg = 1 - y_pos \n",
    "    tn = (K.sum(y_neg * y_pred_neg) + smooth) / (K.sum(y_neg) + smooth )\n",
    "    return tn \n",
    "\n",
    "def tversky(y_true, y_pred):\n",
    "    y_true_pos = K.flatten(y_true)\n",
    "    y_pred_pos = K.flatten(y_pred)\n",
    "    true_pos = K.sum(y_true_pos * y_pred_pos)\n",
    "    false_neg = K.sum(y_true_pos * (1-y_pred_pos))\n",
    "    false_pos = K.sum((1-y_true_pos)*y_pred_pos)\n",
    "    alpha = 0.7\n",
    "    return (true_pos + smooth)/(true_pos + alpha*false_neg + (1-alpha)*false_pos + smooth)\n",
    "\n",
    "def tversky_loss(y_true, y_pred):\n",
    "    return 1 - tversky(y_true,y_pred)\n",
    "\n",
    "def focal_tversky(y_true,y_pred):\n",
    "    pt_1 = tversky(y_true, y_pred)\n",
    "    gamma = 0.75\n",
    "    return K.pow((1-pt_1), gamma)\n",
    "\n",
    "def ftl_bce(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, (-1, 14, 14, 1))\n",
    "    return focal_tversky(y_true, y_pred) + 0.5*binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "\n",
    "def focal_dice(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, (-1, 14, 14, 1))\n",
    "    #foc = focal_loss_fixed(y_true, y_pred, gamma = 0.5, alpha = 0.25)\n",
    "    #foc = foc / 196\n",
    "    #dice = dice_loss(y_true, y_pred)\n",
    "    return 0.5*binary_crossentropy(y_true, y_pred) - log_dice(y_true, y_pred)\n",
    "\n",
    "def bce_dice_count(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, (-1, 14, 14, 1))\n",
    "    bce = 0.5*binary_crossentropy(y_true, y_pred)\n",
    "    dce = -log_dice(y_true, y_pred)\n",
    "    count = 0.5*count_loss(y_true, y_pred)\n",
    "    return bce + dce + count\n",
    "\n",
    "def lvz_bce(y_true, y_pred):\n",
    "    y_true_r = tf.reshape(y_true, (-1, 14, 14, 1))\n",
    "    return lovasz_softmax(tf.reshape(y_pred, (-1, 14, 14)), y_true, classes=[1], per_image=True) + 0.5*binary_crossentropy(y_true_r, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_loss(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    pt_0 = tf.where(tf.math.greater(y_pred_f, 0.5), y_pred_f, tf.zeros_like(y_pred_f))\n",
    "    true_sum = K.sum(y_true_f) # 5 10 15\n",
    "    pred_sum = K.sum(y_pred_f) # 1 10 25\n",
    "    score = K.abs(pred_sum - true_sum) / 196\n",
    "    return score\n",
    "\n",
    "def dsc_np(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = y_true.flatten().astype(np.float32)\n",
    "    y_pred_f = y_pred.flatten().astype(np.float32)\n",
    "    intersection = sum(y_true_f * y_pred_f)\n",
    "    score = (2. * intersection + smooth) / (sum(y_true_f) + sum(y_pred_f) + smooth)\n",
    "    return score\n",
    "\n",
    "def bce_shift(true, pred, power):\n",
    "    losses = []\n",
    "    for i in range(BATCH_SIZE):\n",
    "        true_i = tf.reshape(true[i], (1, 14, 14, 1))\n",
    "        pred_i = tf.reshape(pred[i], (1, 12, 12, 1))\n",
    "        true_p = true_i\n",
    "        #loss_o = binary_crossentropy(true_p, pred)\n",
    "        # extract out the candidate shifts\n",
    "        true_m = true_i[:, 1:13, 1:13]\n",
    "        true_l = true_i[:, 0:12, 1:13]\n",
    "        true_r = true_i[:, 2:14, 1:13]\n",
    "        true_u = true_i[:, 1:13, 0:12]\n",
    "        true_d = true_i[:, 1:13, 2:14]\n",
    "        true_dr = true_i[:, 2:14, 0:12]\n",
    "        true_dl = true_i[:, 0:12, 0:12]\n",
    "        true_ur = true_i[:, 2:14, 2:14]\n",
    "        true_ul = true_i[:, 0:12, 2:14]\n",
    "        true_shifts = [true_m, true_l, true_r, true_u, true_d, true_dr, true_dl, true_ur, true_ul]\n",
    "        bce_shifts = tf.stack([binary_crossentropy(x, pred_i) for x in true_shifts])\n",
    "        jac_shifts = tf.stack([smooth_jaccard(x, pred_i) for x in true_shifts])\n",
    "\n",
    "        # Calculate BCE\n",
    "        \n",
    "        \n",
    "        bce_power = tf.math.pow(1/(tf.reduce_mean(bce_shifts, axis = [2,3])), power)\n",
    "        jac_power = tf.math.pow(1/(jac_shifts+0.1), power)\n",
    "        \n",
    "        sums = tf.reduce_sum(bce_power)\n",
    "        sum_jac = tf.reduce_sum(jac_power)\n",
    "        weights = bce_power/sums\n",
    "        weights_jac = jac_power/sum_jac\n",
    "    \n",
    "        weights = (2*weights + weights_jac)/3\n",
    "        loss = tf.reshape(bce_shifts, (1, 9, 12, 12)) * tf.reshape(weights, (1, 9, 1, 1))\n",
    "        loss = tf.reduce_sum(loss, axis = 1)\n",
    "        loss_j = tf.reshape(jac_shifts, (1, 9)) * tf.reshape(weights, (1, 9))\n",
    "        loss_j = tf.reduce_sum(loss_j, axis = 1)\n",
    "        losses.append(loss + 0.5*loss_j)\n",
    "    loss = tf.reshape(tf.stack(losses), (BATCH_SIZE, 12, 12, 1))\n",
    "    return loss\n",
    "\n",
    "def get_shifts_batched(arr):\n",
    "    true_m = arr[:, 1:13, 1:13]\n",
    "    true_l = arr[:, 0:12, 1:13]\n",
    "    true_r = arr[:, 2:14, 1:13]\n",
    "    true_u = arr[:, 1:13, 0:12]\n",
    "    true_d = arr[:, 1:13, 2:14]\n",
    "    true_dr = arr[:, 2:14, 0:12]\n",
    "    true_dl = arr[:, 0:12, 0:12]\n",
    "    true_ur = arr[:, 2:14, 2:14]\n",
    "    true_ul = arr[:, 0:12, 2:14]\n",
    "    true_shifts = [true_m, true_l, true_r, true_u, true_d, true_dr, true_dl, true_ur, true_ul]\n",
    "    return true_shifts\n",
    "\n",
    "def lovasz_shift(true, pred, power):\n",
    "    batch_shifted = get_shifts_batched(tf.reshape(true, (-1, 14, 14, 1)))\n",
    "    shift_weights = []\n",
    "    for i in range(BATCH_SIZE):\n",
    "        true_i = tf.reshape(true[i], (1, 14, 14, 1))\n",
    "        pred_i = tf.reshape(pred[i], (1, 12, 12, 1))\n",
    "        true_p = true_i\n",
    "        true_m = true_i[:, 1:13, 1:13]\n",
    "        true_l = true_i[:, 0:12, 1:13]\n",
    "        true_r = true_i[:, 2:14, 1:13]\n",
    "        true_u = true_i[:, 1:13, 0:12]\n",
    "        true_d = true_i[:, 1:13, 2:14]\n",
    "        true_dr = true_i[:, 2:14, 0:12]\n",
    "        true_dl = true_i[:, 0:12, 0:12]\n",
    "        true_ur = true_i[:, 2:14, 2:14]\n",
    "        true_ul = true_i[:, 0:12, 2:14]\n",
    "        true_shifts = [true_m, true_l, true_r, true_u, true_d, true_dr, true_dl, true_ur, true_ul]\n",
    "        bce_shifts = tf.stack([binary_crossentropy(x, pred_i) for x in true_shifts])\n",
    "        bce_power = tf.math.pow(1/(tf.reduce_mean(bce_shifts, axis = [2,3])), power)\n",
    "        sums = tf.reduce_sum(bce_power)\n",
    "        weights = bce_power/sums\n",
    "        weights = tf.reshape(weights, (1, 9))\n",
    "        shift_weights.append(weights)\n",
    "    weights = tf.reshape(tf.stack(shift_weights), (BATCH_SIZE, 9))\n",
    "    print(\"WEIGHT\", weights.shape)\n",
    "    lovasz = tf.stack([lovasz_softmax(tf.reshape(pred, (-1, 12, 12)), x, classes = [1], per_image = True) for x in batch_shifted])\n",
    "    #losses = []\n",
    "    #for i in range(0, 9):\n",
    "    #    lovasz = lovasz_softmax(tf.reshape(pred, (-1, 12, 12)), batch_shifted[:, i, :, :, :], classes=[1], per_image=True)\n",
    "    #    lovasz = lovasz * weights[:, i]\n",
    "    #losses.append(lovasz)\n",
    "    #losses = tf.reshape(tf.stack(losses), (-1,))\n",
    "   \n",
    "    return lovasz\n",
    "\n",
    "def ce(targets, predictions, epsilon=1e-12):\n",
    "    \"\"\"\n",
    "    Computes cross entropy between targets (encoded as one-hot vectors)\n",
    "    and predictions. \n",
    "    Input: predictions (N, k) ndarray\n",
    "           targets (N, k) ndarray        \n",
    "    Returns: scalar\n",
    "    \"\"\"\n",
    "    targets = targets.reshape(1, 144)\n",
    "    predictions = predictions.reshape(1, 144)\n",
    "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "    N = predictions.shape[0]\n",
    "    ce = -np.mean(targets*np.log(predictions+1e-9))/N\n",
    "    return ce\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "def prec_shift(true, pred):\n",
    "    true_m = true[1:13, 1:13]\n",
    "    true_l = true[0:12, 1:13]\n",
    "    true_r = true[2:14, 1:13]\n",
    "    true_u = true[1:13, 0:12]\n",
    "    true_d = true[1:13, 2:14]\n",
    "    true_dr = true[2:14, 0:12]\n",
    "    true_dl = true[0:12, 0:12]\n",
    "    true_ur = true[2:14, 2:14]\n",
    "    true_ul = true[0:12, 2:14]\n",
    "    #norm = ce(true_m, pred)\n",
    "    '''\n",
    "    l = ce(true_l, pred)\n",
    "    r = ce(true_r, pred)\n",
    "    u = ce(true_u, pred)\n",
    "    d = ce(true_d, pred)\n",
    "    dr = ce(true_dr, pred)\n",
    "    dl = ce(true_dl, pred)\n",
    "    ur = ce(true_ur, pred)\n",
    "    ul = ce(true_ul, pred)\n",
    "    for_weights = [(1/(i+0.1)**3) for i in [norm, l, r, u, d, dr, dl, ur, ul]]\n",
    "    #print([1/i for i in for_weights])\n",
    "    #print([norm, l, r, u ,d])\n",
    "    sum_for_weights = sum(for_weights)\n",
    "    sum_for_weights = max(sum_for_weights, 1)\n",
    "    #sums = sum([1/i for i in [norm, l, r, u, d, dr, dl, ur, ul]])\n",
    "    #sums = max(sums, 1)\n",
    "    weights = [i/sum_for_weights for i in for_weights]\n",
    "    #weights = [i/sum(weights) for i in weights]\n",
    "    '''\n",
    "    match = dsc_np(true_m, pred)\n",
    "    match_l = dsc_np(true_l, pred)\n",
    "    match_r = dsc_np(true_r, pred)\n",
    "    match_u = dsc_np(true_u, pred)\n",
    "    match_d = dsc_np(true_d, pred)\n",
    "    match_dr = dsc_np(true_dr, pred)\n",
    "    match_dl = dsc_np(true_dl, pred)\n",
    "    match_ur = dsc_np(true_ur, pred)\n",
    "    match_ul = dsc_np(true_ul, pred)\n",
    "    return max([match, match_l, match_r, match_u, match_d, match_dr, match_dl, match_ur, match_ul])\n",
    "    #return sum([(i * l) for i, l in zip([match, match_l, match_r, match_u, match_d, match_dr, match_dl, match_ur, match_ul], weights)])\n",
    "    #best_shift = max([match_l, match_r, match_u, match_d, match_dr, match_dl, match_ur, match_ul])\n",
    "    #if match < (best_shift - 0.2):\n",
    "    #    print(\"The shifted data performs better by {}\".format(best_shift - match))\n",
    "    #    return best_shift\n",
    "    #else:\n",
    "    #    return match\n",
    "    \n",
    "def multi_shift(true, pred, power):\n",
    "    return bce_shift(true, pred, power) + lovasz_shift(true, pred, power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_1 = np.array([[0, 0, 0], [1, 1, 1], [0, 0, 0]])\n",
    "img_2 = np.array([[0, 0, 0], [1, 1, 0], [0, 0, 0]])\n",
    "dsc_np(img_1, img_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model with: \n",
      " 0.3 zone out \n",
      " 0.005 l2 \n",
      " 2e-06 initial LR \n",
      " 0.0002 final LR \n",
      " 181413 parameters\n",
      "Resuming training with a best validation score of 0.66\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8dd23441e43489f9be91226144d4c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=89), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18: Loss 0.7133147716522217 Val: 0.8143258094787598 P 0.8389458277644106 R 0.5924968256335635 F1 0.6945059777965663 iou 0.4823765668062471\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4825f4cae0404f5ebd68019a8bb702c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=89), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19: Loss 0.7100421786308289 Val: 0.8201144337654114 P 0.8054847144014895 R 0.6068623917921716 F1 0.6922071467985086 iou 0.48183961444624185\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d25d40e2f44766b6da69325318e210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=89), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20: Loss 0.702033281326294 Val: 0.8167523145675659 P 0.8804494014553382 R 0.5763976062309422 F1 0.6966948824809012 iou 0.4796720956619583\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1328ef30838d40a1855f3313a47c3b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=89), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21: Loss 0.6953979134559631 Val: 0.8274024128913879 P 0.7939345390138502 R 0.6170738918014498 F1 0.6944200546580521 iou 0.4881879047623709\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8530afc965bd4c34aa819b119e3399a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=89), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22: Loss 0.6918622255325317 Val: 0.8159503936767578 P 0.8030814664585869 R 0.6089722899052065 F1 0.6926851862482774 iou 0.49051471415456555\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8620a81b4c5c4600a7246bdb2a4022d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=89), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-2c654e7c6234>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m                                          \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                                          \u001b[0mis_training\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                                          \u001b[0mpower\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.06\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m                                          })\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/remote_sensing/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/remote_sensing/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/remote_sensing/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/remote_sensing/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/remote_sensing/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/remote_sensing/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "FRESH_START = False\n",
    "FINE_TUNE = False\n",
    "from tensorflow.python.keras.optimizers import SGD\n",
    "\n",
    "\n",
    "BATCH_SIZE = 24\n",
    "print(\"Starting model with: \\n {} zone out \\n {} l2 \\n {} initial LR \\n {} final LR \\n {} parameters\"\n",
    "     .format(ZONE_OUT_PROB, L2_REG, INITIAL_LR, FINAL_LR, total_parameters))\n",
    "best_val = 0.66\n",
    "if not FRESH_START:\n",
    "    print(\"Resuming training with a best validation score of {}\".format(best_val))\n",
    "if FRESH_START:\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1e-6)\n",
    "    print(\"Restarting training from scratch on {} train and {} test samples, total {}\".format(len(train_ids), len(test_ids), N_SAMPLES))\n",
    "    #optimizer = AdaBoundOptimizer(learning_rate=INITIAL_LR/3,\n",
    "    #                              final_lr=FINAL_LR/6,\n",
    "    #                              beta1=0.9, beta2=0.999, \n",
    "    #                              amsbound=True)\n",
    "    \n",
    "    optimizer2 = AdaBoundOptimizer(learning_rate=INITIAL_LR/5,\n",
    "                                  final_lr=FINAL_LR/5,\n",
    "                                  beta1=0.9, beta2=0.999, \n",
    "                                  amsbound=True)\n",
    "    \n",
    "    loss = bce_shift(labels, fm, power)\n",
    "    #loss = bce_dice_count(labels, fm)\n",
    "    l2_loss = tf.losses.get_regularization_loss()\n",
    "    loss += l2_loss\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "    with tf.control_dependencies(update_ops):\n",
    "        train_op = optimizer.minimize(loss)    \n",
    "\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    saver = tf.train.Saver(max_to_keep = 2)\n",
    "    \n",
    "if FINE_TUNE:\n",
    "    loss_to_run = loss2\n",
    "    op_to_run = tune_op\n",
    "else:\n",
    "    loss_to_run = loss\n",
    "    op_to_run = train_op\n",
    "\n",
    "# Run training loop\n",
    "for i in range(17, 100):\n",
    "    randomize = train_ids\n",
    "    np.random.shuffle(randomize)\n",
    "    test_randomize = test_ids\n",
    "    np.random.shuffle(test_randomize)\n",
    "\n",
    "    losses = []\n",
    "    val_loss = []\n",
    "    \n",
    "    for k in tnrange(int(len(train_ids) // BATCH_SIZE)):\n",
    "        batch_ids = randomize[k*BATCH_SIZE:(k+1)*BATCH_SIZE]\n",
    "        batch_y = data_y[batch_ids, :, :].reshape(BATCH_SIZE, 14, 14)\n",
    "        #if sum(sum(sum(batch_y))) > 0:\n",
    "        op, tr = sess.run([op_to_run, loss_to_run],\n",
    "                              feed_dict={inp: data_x[batch_ids, :, :, :],\n",
    "                                         length: lengths[batch_ids],\n",
    "                                         labels: data_y[batch_ids, :, :].reshape(BATCH_SIZE, 14, 14),\n",
    "                                         is_training: True,\n",
    "                                         power: 1 + (i*0.06),\n",
    "                                         })\n",
    " \n",
    "        #else:\n",
    "        #    print(\"Skipping minibatch for equibatch reasons\")\n",
    "        losses.append(tr)\n",
    "    for j in range(len(test_ids) // BATCH_SIZE):\n",
    "        batch_ids = test_randomize[j*BATCH_SIZE:(j+1)*BATCH_SIZE]\n",
    "        vl, y = sess.run([loss, fm], \n",
    "                         feed_dict={inp: data_x[batch_ids, :, :, :],\n",
    "                                    length: lengths[batch_ids],\n",
    "                                    labels: data_y[batch_ids, :, :].reshape(BATCH_SIZE, 14, 14),\n",
    "                                    is_training: False,\n",
    "                                    power: 1 + (i*0.06)\n",
    "                                    })\n",
    "        val_loss.append(vl)\n",
    "        \n",
    "    recalls = []\n",
    "    precisions = []\n",
    "    ious = []\n",
    "    for m in test_ids:\n",
    "        y = sess.run([fm], feed_dict={inp: data_x[m].reshape(1, 24, IMAGE_SIZE, IMAGE_SIZE, 15),\n",
    "                                  length: lengths[m].reshape(1, 1),\n",
    "                                  is_training: False,\n",
    "                                  })[0]\n",
    "        true = data_y[m].reshape((LABEL_SIZE, LABEL_SIZE))\n",
    "        pred = y.reshape((12, 12))\n",
    "        #TODO @jombrandt figure out difference between this in train and inference time\n",
    "        #TODO @jombrandt convert to ROC-AUC instead of threshold F1\n",
    "        pred[np.where(pred > 0.45)] = 1\n",
    "        pred[np.where(pred < 0.45)] = 0\n",
    "        shifts = get_shifts(true)\n",
    "        f1s = []\n",
    "        precs = []\n",
    "        recs = []\n",
    "        for s in shifts:\n",
    "            rec, prec = thirty_meter(s, pred)\n",
    "            rec = np.mean(rec)\n",
    "            prec = np.mean(prec)\n",
    "            f1_score = 2 * ((prec * rec) / (prec + rec))\n",
    "            f1s.append(f1_score)\n",
    "            precs.append(prec)\n",
    "            recs.append(rec)\n",
    "        rec = recs[np.argmax(f1s)]\n",
    "        prec = precs[np.argmax(f1s)]\n",
    "        recalls.append(rec)\n",
    "        precisions.append(prec)\n",
    "        iou = prec_shift(true, pred)\n",
    "        ious.append(iou)\n",
    "    precision = np.mean([x for x in precisions if not np.isnan(x)])\n",
    "    recall = np.mean([x for x in recalls if not np.isnan(x)])\n",
    "    iou = np.mean(ious)\n",
    "    f1_score = 2 * ((precision * recall) / (precision + recall))\n",
    "    save_path = saver.save(sess, \"../models/dev/model\")\n",
    "    if np.mean(val_loss) < best_val:\n",
    "        best_val = np.mean(val_loss)\n",
    "        print(\"Saving model with {}\".format(best_val))\n",
    "        save_path = saver.save(sess, \"../models/dev_best/model\")\n",
    "    print(\"Epoch {}: Loss {} Val: {} P {} R {} F1 {} iou {}\".format(i + 1,\n",
    "                                                             np.mean(losses), np.mean(val_loss),\n",
    "                                                             precision, recall, f1_score, iou))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model validation and sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8181818181818182\n",
      "444\n",
      "448\n",
      "452\n",
      "456\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#TODO @jombrandt \n",
    "#TODO @jombrandt -- remove augmentation of val set\n",
    "import random \n",
    "\n",
    "def multiplot(matrices):\n",
    "    '''Plot multiple heatmaps with subplots'''\n",
    "    fig, axs = plt.subplots(ncols=4)\n",
    "    fig.set_size_inches(20, 4)\n",
    "    for i, matrix in enumerate(matrices):\n",
    "        sns.heatmap(data = matrix, ax = axs[i], vmin = 0, vmax = 0.5)\n",
    "        axs[i].set_xlabel(\"\")\n",
    "        axs[i].set_ylabel(\"\")\n",
    "        axs[i].set_yticks([])\n",
    "        axs[i].set_xticks([])\n",
    "    plt.show()\n",
    "    \n",
    "test_losses = []\n",
    "#start = 28\n",
    "start = start + 4\n",
    "print(start/len(test_ids))\n",
    "#matrix_ids = random.sample(train_ids, 4)\n",
    "test_ids = sorted(test_ids)\n",
    "#matrix_ids = [504, 976]\n",
    "matrix_ids = [test_ids[start], test_ids[start + 1], test_ids[start + 2], test_ids[start + 3],]\n",
    "#matrix_ids = random.sample(train_ids, 4)\n",
    "#matrix = [matrix_ids[0], matrix_ids[0] + 4, matrix_ids[0] + 8, matrix_ids[0] + 12]\n",
    "#matrix_ids = [988, 900, 2055, test]\n",
    "# 63\"\"\n",
    "preds = []\n",
    "trues = []\n",
    "for i in matrix_ids:\n",
    "    idx = i\n",
    "    print(i)\n",
    "    y = sess.run([fm], feed_dict={inp: data_x[idx].reshape(1, 24, IMAGE_SIZE, IMAGE_SIZE, 15),\n",
    "                                  length: lengths[idx].reshape(1, 1),\n",
    "                                  is_training: False,\n",
    "                                  #labels: data_y[idx].reshape(1, 14, 14)\n",
    "                                  })\n",
    "    #print(idx, np.mean(lr))\n",
    "    y = np.array(y).reshape(12, 12)\n",
    "    y = np.pad(y, [[1, 1], [1, 1]], mode = \"constant\")#, constant_values = min([min(i) for i in y]))\n",
    "    #y[np.where(y < 0.05)] = 0\n",
    "    preds.append(y)\n",
    "    true = data_y[idx].reshape(LABEL_SIZE, LABEL_SIZE)\n",
    "    trues.append(true)\n",
    "\n",
    "multiplot(preds)\n",
    "#plot_ids[ordering[976]//4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "multiplot(trues) # 140, 160, 236, 296, 324, 416, 460, 504, 976"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "for j in range(4, 12):\n",
    "    f1_all = []\n",
    "    for i in test_ids:\n",
    "        y = sess.run([fm], feed_dict={inp: data_x[i].reshape(1, 24, 16, 16, 13),\n",
    "                                  length: lengths[i].reshape(1, 1),\n",
    "                                  is_training: False,\n",
    "                                  })[0]\n",
    "        true = data_y[i].reshape((14, 14))\n",
    "        pred = y.reshape((12, 12))\n",
    "        #pred = pred[1:15, 1:15]\n",
    "        #true = true[1:15, 1:15]\n",
    "        pred[np.where(pred > j*0.05)] = 1\n",
    "        pred[np.where(pred < j*0.05)] = 0\n",
    "        shifts = get_shifts(true)\n",
    "        f1s = []\n",
    "        precs = []\n",
    "        recs = []\n",
    "        for i in shifts:\n",
    "            rec, prec = thirty_meter(i, pred)\n",
    "            rec = np.mean(rec)\n",
    "            prec = np.mean(prec)\n",
    "            f1_score = 2 * ((prec * rec) / (prec + rec))\n",
    "            f1s.append(f1_score)\n",
    "            precs.append(prec)\n",
    "            recs.append(rec)\n",
    "        rec = recs[np.argmax(f1s)]\n",
    "        prec = precs[np.argmax(f1s)]\n",
    "        f1s = max(f1s)\n",
    "        f1_all.append(f1s)\n",
    "        #recalls.append(rec)\n",
    "        #precisions.append(prec)\n",
    "    #recalls = [item for sublist in recalls for item in sublist]\n",
    "    #precisions = [item for sublist in precisions for item in sublist]\n",
    "    print(np.mean(f1_all))\n",
    "    #print(\"{}: Recall: {}\\t Precision: {}\".format(j*0.05, np.mean(recalls), np.mean(precisions)))\n",
    "    #TEST: 1161, 1076, 1267, 1187, 1197,  1109, 1235 TEAIN: 290, 184, 294, 890, 807\n",
    "# 135224667"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO @jombrandt top 10 worst training, test samples by IOU \n",
    "\n",
    "These should be written to a tmp/ .txt file and indexed by validate-data.ipynb to ensure that original classifications were correct, and to identify regions that need more training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_sensing",
   "language": "python",
   "name": "remote_sensing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
