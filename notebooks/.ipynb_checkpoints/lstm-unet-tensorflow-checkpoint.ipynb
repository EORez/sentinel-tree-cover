{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/john.brandt/anaconda3/envs/remote_sensing/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#tf.reset_default_graph()\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "from keras import backend as K\n",
    "K.set_session(sess)\n",
    "\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.layers import ELU\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import itertools\n",
    "from tflearn.layers.conv import global_avg_pool\n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "from tensorflow.contrib.layers import batch_norm\n",
    "from keras.regularizers import l1\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvGRUCell(tf.nn.rnn_cell.RNNCell):\n",
    "  \"\"\"A GRU cell with convolutions instead of multiplications.\"\"\"\n",
    "\n",
    "  def __init__(self, shape, filters, kernel, padding = 'SAME', activation=tf.tanh, normalize=False, data_format='channels_last', reuse=None):\n",
    "    super(ConvGRUCell, self).__init__(_reuse=reuse)\n",
    "    self._filters = filters\n",
    "    self._kernel = kernel\n",
    "    self._activation = activation\n",
    "    self._normalize = normalize\n",
    "    self._padding = padding\n",
    "    if data_format == 'channels_last':\n",
    "        self._size = tf.TensorShape(shape + [self._filters])\n",
    "        self._feature_axis = self._size.ndims\n",
    "        self._data_format = None\n",
    "    elif data_format == 'channels_first':\n",
    "        self._size = tf.TensorShape([self._filters] + shape)\n",
    "        self._feature_axis = 0\n",
    "        self._data_format = 'NC'\n",
    "    else:\n",
    "        raise ValueError('Unknown data_format')\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    return self._size\n",
    "\n",
    "  @property\n",
    "  def output_size(self):\n",
    "    return self._size\n",
    "\n",
    "  def call(self, x, h):\n",
    "    channels = x.shape[self._feature_axis].value\n",
    "\n",
    "    with tf.variable_scope('gates'):\n",
    "      inputs = tf.concat([x, h], axis=self._feature_axis)\n",
    "      n = channels + self._filters\n",
    "      m = 2 * self._filters if self._filters > 1 else 2\n",
    "      W = tf.get_variable('kernel', self._kernel + [n, m])\n",
    "      y = tf.nn.convolution(inputs, W, self._padding, data_format=self._data_format)\n",
    "      if self._normalize:\n",
    "        r, u = tf.split(y, 2, axis=self._feature_axis)\n",
    "        r = tf.contrib.layers.layer_norm(r)\n",
    "        u = tf.contrib.layers.layer_norm(u)\n",
    "      else:\n",
    "        y += tf.get_variable('bias', [m], initializer=tf.ones_initializer())\n",
    "        r, u = tf.split(y, 2, axis=self._feature_axis)\n",
    "      r, u = tf.sigmoid(r), tf.sigmoid(u)\n",
    "\n",
    "    with tf.variable_scope('candidate'):\n",
    "      inputs = tf.concat([x, r * h], axis=self._feature_axis)\n",
    "      n = channels + self._filters\n",
    "      m = self._filters\n",
    "      W = tf.get_variable('kernel', self._kernel + [n, m])\n",
    "      y = tf.nn.convolution(inputs, W, self._padding, data_format=self._data_format)\n",
    "      if self._normalize:\n",
    "        y = tf.contrib.layers.layer_norm(y)\n",
    "      else:\n",
    "        y += tf.get_variable('bias', [m], initializer=tf.zeros_initializer())\n",
    "      h = u * h + (1 - u) * self._activation(y)\n",
    "\n",
    "    return h, h\n",
    "\n",
    "def Batch_Normalization(x, training, scope):\n",
    "    with arg_scope([batch_norm],\n",
    "                   scope=scope,\n",
    "                   updates_collections=None,\n",
    "                   decay=0.9,\n",
    "                   center=True,\n",
    "                   scale=True,\n",
    "                   zero_debias_moving_mean=True) :\n",
    "        return tf.cond(training,\n",
    "                       lambda : batch_norm(inputs=x, is_training=training, reuse=None),\n",
    "                       lambda : batch_norm(inputs=x, is_training=training, reuse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()\n",
    "class ReflectionPadding2D(Layer):\n",
    "    def __init__(self, padding=(1, 1), **kwargs):\n",
    "        self.padding = tuple(padding)\n",
    "        self.input_spec = [InputSpec(ndim=4)]\n",
    "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, s):\n",
    "        \"\"\" If you are using \"channels_last\" configuration\"\"\"\n",
    "        return (s[0], s[1] + 2 * self.padding[0], s[2] + 2 * self.padding[1], s[3])\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        w_pad,h_pad = self.padding\n",
    "        return tf.pad(x, [[0,0], [h_pad,h_pad], [w_pad,w_pad], [0,0] ], 'REFLECT')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-abdd6f7d1bf2>:45: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/john.brandt/anaconda3/envs/remote_sensing/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/john.brandt/anaconda3/envs/remote_sensing/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "FIRST GRU (?, 24, 14, 14, 48)\n",
      "DOWNSAMPLE (?, 24, 7, 7, 48)\n",
      "SECOND GRU (?, 7, 7, 64)\n",
      "WARNING:tensorflow:From <ipython-input-4-abdd6f7d1bf2>:10: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "Down block conv (?, 7, 7, 98)\n",
      "Upsampling (?, 14, 14, 98)\n",
      "Up block conv 2 (?, 14, 14, 48)\n",
      "Up block conv 3 (?, 14, 14, 32)\n",
      "(?, 14, 14, 1)\n"
     ]
    }
   ],
   "source": [
    "reg = l1(0.001)\n",
    "inp = tf.placeholder(tf.float32, shape=(None, 24, 14, 14, 13))\n",
    "length = tf.placeholder(tf.int32, shape = (None, 1))\n",
    "labels = tf.placeholder(tf.float32, shape=(None, 14, 14, 1))\n",
    "length2 = tf.reshape(length, (-1,))\n",
    "is_training = tf.placeholder_with_default(False, (), 'is_training')\n",
    "\n",
    "def Fully_connected(x, units, layer_name='fully_connected') :\n",
    "    with tf.name_scope(layer_name) :\n",
    "        return tf.layers.dense(inputs=x, use_bias=True, units=units)\n",
    "\n",
    "def Relu(x):\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def Sigmoid(x):\n",
    "    return tf.nn.sigmoid(x)\n",
    "\n",
    "def Global_Average_Pooling(x):\n",
    "    return global_avg_pool(x, name='Global_avg_pooling')\n",
    "\n",
    "def Squeeze_excitation_layer(input_x, out_dim, ratio, layer_name):\n",
    "    with tf.name_scope(layer_name) :\n",
    "        squeeze = global_avg_pool(input_x)\n",
    "\n",
    "        excitation = Fully_connected(squeeze, units=out_dim / ratio, layer_name=layer_name+'_fully_connected1')\n",
    "        excitation = Relu(excitation)\n",
    "        excitation = Fully_connected(excitation, units=out_dim, layer_name=layer_name+'_fully_connected2')\n",
    "        excitation = Sigmoid(excitation)\n",
    "\n",
    "        excitation = tf.reshape(excitation, [-1,1,1,out_dim])\n",
    "\n",
    "        scale = input_x * excitation\n",
    "\n",
    "        return scale\n",
    "\n",
    "with tf.variable_scope('10'):\n",
    "    # Downsampling Block 1 (14 x 14)\n",
    "    cell_10 = ConvGRUCell(shape = [14, 14],\n",
    "                   filters = 24,\n",
    "                   kernel = [3,3],\n",
    "                   padding = 'SAME')\n",
    "\n",
    "    def convGRU(x, cell, ln):\n",
    "        output, final = tf.nn.bidirectional_dynamic_rnn(\n",
    "            cell, cell, x, ln, dtype=tf.float32)\n",
    "        output = tf.concat(output, -1)\n",
    "        final = tf.concat(final, -1)\n",
    "        return [output, final]\n",
    "\n",
    "# Return the final state and the output states\n",
    "first_conv = convGRU(inp, cell_10, length2)\n",
    "print(\"FIRST GRU {}\".format(first_conv[0].shape))\n",
    "\n",
    "\n",
    "downsampled = TimeDistributed(MaxPool2D(pool_size = (2, 2)))(first_conv[0])\n",
    "print(\"DOWNSAMPLE {}\".format(downsampled.shape))\n",
    "\n",
    "# Downsampling block 2 (7 x 7)\n",
    "with tf.variable_scope('7'):\n",
    "    cell_7 = ConvGRUCell(shape = [7, 7],\n",
    "                   filters = 32,\n",
    "                   kernel = [3,3],\n",
    "                   padding = 'SAME')\n",
    "    state_7 = convGRU(downsampled, cell_7, length2)\n",
    "print(\"SECOND GRU {}\".format(state_7[1].shape))\n",
    "\n",
    "conv_block_7_u = Conv2D(filters = 98, kernel_size = (3, 3), padding = 'same', activity_regularizer=reg)(state_7[1])\n",
    "elu_7_u = ELU()(conv_block_7_u)\n",
    "#x = Batch_Normalization(elu_7_u, training=is_training, scope = 'bn1')\n",
    "squeezed = Squeeze_excitation_layer(input_x = elu_7_u, out_dim = 98, ratio = 4, layer_name = \"squeezed\")\n",
    "print(\"Down block conv {}\".format(elu_7_u.shape))\n",
    "\n",
    "\n",
    "\n",
    "# Upsampling final block (14 x 14)\n",
    "upsampling_14 = UpSampling2D()(squeezed)\n",
    "print(\"Upsampling {}\".format(upsampling_14.shape))\n",
    "padded = ReflectionPadding2D((1, 1))(upsampling_14)\n",
    "fm = Conv2D(filters = 48,\n",
    "            kernel_size = (3, 3), \n",
    "            padding = 'valid',\n",
    "            activity_regularizer=l1(0.001),\n",
    "            )(padded)\n",
    "elu = ELU()(fm)\n",
    "#x = Batch_Normalization(elu, training=is_training, scope = 'bn2')\n",
    "\n",
    "squeezed2 = Squeeze_excitation_layer(input_x = elu, out_dim = 48, ratio = 4, layer_name = \"squeezed2\")\n",
    "print(\"Up block conv 2 {}\".format(elu.shape))\n",
    "concat = Concatenate(axis = -1)([squeezed2, first_conv[1]])\n",
    "padded = ReflectionPadding2D((1, 1))(fm)\n",
    "fm = Conv2D(filters = 32,\n",
    "            kernel_size = (3, 3), \n",
    "            padding = 'valid',\n",
    "            )(padded)\n",
    "elu = ELU()(fm)\n",
    "print(\"Up block conv 3 {}\".format(elu.shape))\n",
    "# Output layer\n",
    "fm = Conv2D(filters = 1,\n",
    "            kernel_size = (1, 1), \n",
    "            padding = 'valid',\n",
    "            activation = 'sigmoid'\n",
    "            )(elu)\n",
    "print(fm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15984\n",
      "48\n",
      "7992\n",
      "24\n",
      "46080\n",
      "64\n",
      "23040\n",
      "32\n",
      "56448\n",
      "98\n",
      "2352\n",
      "24\n",
      "2352\n",
      "98\n",
      "42336\n",
      "48\n",
      "576\n",
      "12\n",
      "576\n",
      "48\n",
      "13824\n",
      "32\n",
      "32\n",
      "1\n",
      "212121\n"
     ]
    }
   ],
   "source": [
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    # shape is an array of tf.Dimension\n",
    "    shape = variable.get_shape()\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        variable_parameters *= dim.value\n",
    "    print(variable_parameters)\n",
    "    total_parameters += variable_parameters\n",
    "print(total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lovasz_grad(gt_sorted):\n",
    "    \"\"\"\n",
    "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
    "    See Alg. 1 in paper\n",
    "    \"\"\"\n",
    "    gts = tf.reduce_sum(gt_sorted)\n",
    "    intersection = gts - tf.cumsum(gt_sorted)\n",
    "    union = gts + tf.cumsum(1. - gt_sorted)\n",
    "    jaccard = 1. - intersection / union\n",
    "    jaccard = tf.concat((jaccard[0:1], jaccard[1:] - jaccard[:-1]), 0)\n",
    "    return jaccard\n",
    "\n",
    "\n",
    "def lovasz_hinge(logits, labels):\n",
    "        \"\"\"\n",
    "        Binary Lovasz hinge loss\n",
    "          logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n",
    "          labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n",
    "          per_image: compute the loss per image instead of per batch\n",
    "          ignore: void class id\n",
    "        \"\"\"\n",
    "        def treat_image(log_lab):\n",
    "            log, lab = log_lab\n",
    "            log, lab = tf.expand_dims(log, 0), tf.expand_dims(lab, 0)\n",
    "            log = tf.reshape(log, (-1,))\n",
    "            lab = tf.reshape(lab, (-1,))\n",
    "            return lovasz_hinge_flat(log, lab)\n",
    "        losses = tf.map_fn(treat_image, (logits, labels), dtype=tf.float32)\n",
    "        losses.set_shape((None,))\n",
    "        loss = tf.reduce_mean(losses)\n",
    "        return loss\n",
    "\n",
    "\n",
    "def lovasz_hinge_flat(logits, labels):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
    "      labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
    "      ignore: label to ignore\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_loss():\n",
    "        labelsf = tf.cast(labels, logits.dtype)\n",
    "        signs = 2. * labelsf - 1.\n",
    "        errors = 1. - logits * tf.stop_gradient(signs)\n",
    "        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], name=\"descending_sort\")\n",
    "        gt_sorted = tf.gather(labelsf, perm)\n",
    "        grad = lovasz_grad(gt_sorted)\n",
    "        loss = tf.tensordot(tf.nn.relu(errors_sorted), tf.stop_gradient(grad), 1, name=\"loss_non_void\")\n",
    "        return loss\n",
    "\n",
    "    # deal with the void prediction case (only void pixels)\n",
    "    loss = tf.cond(tf.equal(tf.shape(logits)[0], 0),\n",
    "                   lambda: tf.reduce_sum(logits) * 0.,\n",
    "                   compute_loss,\n",
    "                   strict=True,\n",
    "                   name=\"loss\"\n",
    "                   )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336\n",
      "0.3673082 0.7603346\n",
      "0.3491656 0.9995814\n",
      "22\n",
      "0.3871069 0.7687279\n",
      "22\n",
      "0.33417815 0.8142235\n",
      "22\n",
      "0.002267599 0.9975062\n",
      "0.39245468 0.7822398\n",
      "22\n",
      "0.018518478 0.9987654\n",
      "22\n",
      "0.0 0.9993017\n",
      "0.3425532 0.70296013\n",
      "22\n",
      "0.4108352 0.9997895\n",
      "22\n",
      "0.34487897 0.9009186\n",
      "0.53952295 0.84337354\n",
      "0.18374261 0.9995545\n",
      "22\n",
      "0.13010052 0.60302514\n",
      "22\n",
      "0.3105899 0.8685879\n",
      "22\n",
      "0.2887975 0.9896908\n",
      "22\n",
      "0.0014124513 0.9996896\n",
      "22\n",
      "0.23668641 0.99915755\n",
      "22\n",
      "0.29181817 0.8222011\n",
      "22\n",
      "0.0045248866 0.8501582\n",
      "22\n",
      "0.2584334 0.80423415\n",
      "22\n",
      "0.38473642 0.984609\n",
      "22\n",
      "0.41035423 0.9077408\n",
      "0.38322064 0.8306217\n",
      "0.3645833 0.9701493\n",
      "22\n",
      "0.063926905 0.9997599\n",
      "22\n",
      "0.4337806 0.999689\n",
      "22\n",
      "0.1746633 0.6894075\n",
      "22\n",
      "0.31032127 0.91107\n",
      "22\n",
      "0.41572785 0.7161658\n",
      "22\n",
      "0.19358599 0.8611111\n",
      "22\n",
      "0.39320165 0.8684366\n",
      "22\n",
      "0.0 0.92077863\n",
      "0.30590612 0.78264093\n",
      "22\n",
      "0.27876106 0.89801836\n",
      "22\n",
      "0.0 0.7867664\n",
      "22\n",
      "0.40803814 0.9996698\n",
      "22\n",
      "0.35833332 0.99970436\n",
      "22\n",
      "0.3681694 0.8744757\n",
      "22\n",
      "0.096410275 0.8437039\n",
      "22\n",
      "0.2955739 0.92014974\n",
      "22\n",
      "0.50090694 0.9014919\n",
      "22\n",
      "0.27903348 0.9996344\n",
      "22\n",
      "0.45578232 0.99968636\n",
      "22\n",
      "0.35435057 0.99967337\n",
      "22\n",
      "0.0 0.7417355\n",
      "22\n",
      "0.37985706 0.95855856\n",
      "22\n",
      "0.39617723 0.6945298\n",
      "22\n",
      "0.2922636 0.99978\n",
      "22\n",
      "0.42020816 0.6305693\n",
      "22\n",
      "0.49460045 0.9997597\n",
      "22\n",
      "0.49167857 0.7548135\n",
      "22\n",
      "0.15544674 0.7680502\n",
      "22\n",
      "0.19649449 0.7865112\n",
      "22\n",
      "0.30178475 0.9996568\n",
      "0.43549785 0.79252243\n",
      "0.13424125 0.9997204\n",
      "22\n",
      "0.3191268 0.8366606\n",
      "0.41762114 0.9997759\n",
      "22\n",
      "0.40950423 0.9993364\n",
      "22\n",
      "0.3089526 0.9994612\n",
      "22\n",
      "0.4179262 0.7579204\n",
      "0.2304075 0.9995671\n",
      "22\n",
      "0.25301206 0.99950373\n",
      "22\n",
      "0.0011723638 0.8292117\n",
      "22\n",
      "0.40502408 0.78553617\n",
      "0.26287264 0.99977577\n",
      "22\n",
      "0.0016366839 0.84738374\n",
      "22\n",
      "0.42904156 0.99968046\n",
      "22\n",
      "0.35589075 0.90558076\n",
      "22\n",
      "0.31215298 0.9644521\n",
      "22\n",
      "0.23336253 0.73027825\n",
      "0.36319524 0.99256015\n",
      "0.064606756 0.90047395\n",
      "22\n",
      "0.32623512 0.8108995\n",
      "22\n",
      "0.33007336 0.95626974\n",
      "22\n",
      "0.3248009 0.99945056\n",
      "22\n",
      "0.470981 0.99978244\n",
      "22\n",
      "0.39266303 0.8648433\n",
      "22\n",
      "0.24876848 0.99965894\n",
      "22\n",
      "0.05873716 0.99972034\n",
      "22\n",
      "0.0017094314 0.77190876\n",
      "22\n",
      "0.008849531 0.9985528\n",
      "22\n",
      "0.32590413 0.7887856\n",
      "22\n",
      "0.3789693 0.928321\n",
      "22\n",
      "0.3445452 0.7546742\n",
      "22\n",
      "0.26993865 0.9655173\n",
      "22\n",
      "0.23644754 0.99941385\n",
      "22\n",
      "0.11918369 0.9379684\n",
      "22\n",
      "0.3982188 0.8724623\n",
      "22\n",
      "0.3385147 0.99976575\n",
      "22\n",
      "0.17464116 0.6804435\n",
      "22\n",
      "0.3702387 0.99930507\n",
      "22\n",
      "0.4144032 0.80874574\n",
      "22\n",
      "0.34709096 0.8510215\n",
      "22\n",
      "0.36305556 0.7025261\n",
      "22\n",
      "0.3857843 0.99971354\n",
      "22\n",
      "0.15258855 0.8004695\n",
      "22\n",
      "0.47793427 0.90938723\n",
      "22\n",
      "0.3941541 0.98254955\n",
      "22\n",
      "0.38875753 0.7676856\n",
      "0.34804833 0.8804426\n",
      "0.4285174 0.85224533\n",
      "22\n",
      "0.3588987 0.8521683\n",
      "0.32683253 0.8534653\n",
      "22\n",
      "0.03666666 0.7468355\n",
      "22\n",
      "0.3734269 0.8675023\n",
      "22\n",
      "0.30542564 0.81811106\n",
      "22\n",
      "0.4225422 0.78728604\n",
      "22\n",
      "0.22857144 0.99976945\n",
      "22\n",
      "0.393268 0.99978757\n",
      "22\n",
      "0.35229176 0.7330437\n",
      "22\n",
      "0.002127707 0.9996828\n",
      "22\n",
      "0.3752399 0.99978495\n",
      "22\n",
      "0.28844112 0.8718196\n",
      "22\n",
      "0.34723127 0.75359213\n",
      "22\n",
      "0.3707989 0.80626523\n",
      "22\n",
      "0.38114443 0.99966395\n",
      "22\n",
      "0.36584288 0.78945696\n",
      "22\n",
      "0.31178978 0.9997884\n",
      "22\n",
      "0.34814048 0.88769305\n",
      "22\n",
      "0.35336977 0.9811828\n",
      "22\n",
      "0.3356102 0.7086817\n",
      "22\n",
      "0.43370944 0.9997492\n",
      "22\n",
      "0.53612983 0.99948215\n",
      "22\n",
      "0.13341647 0.99956524\n",
      "22\n",
      "0.32003468 0.99916875\n",
      "22\n",
      "0.27673936 0.8267001\n",
      "22\n",
      "0.36858475 0.91984487\n",
      "22\n",
      "0.2327852 0.88237727\n",
      "22\n",
      "0.34304473 0.978022\n",
      "22\n",
      "0.3066333 0.99977046\n",
      "0.42553192 0.9996394\n",
      "22\n",
      "0.34462363 0.8780609\n",
      "22\n",
      "0.4043716 0.99889505\n",
      "0.35082605 0.9757576\n",
      "22\n",
      "0.42476425 0.99973106\n",
      "22\n",
      "0.3231663 0.6869217\n",
      "22\n",
      "0.3505896 0.88933957\n",
      "22\n",
      "0.21913329 0.9603509\n",
      "22\n",
      "0.35864156 0.9130095\n",
      "22\n",
      "0.0 0.7938144\n",
      "0.002123177 0.9966887\n",
      "22\n",
      "0.42097837 0.81372297\n",
      "22\n",
      "0.38715354 0.7870658\n",
      "22\n",
      "0.026957601 0.62002337\n",
      "22\n",
      "0.3943074 0.9996841\n",
      "22\n",
      "0.34513515 0.87486553\n",
      "22\n",
      "0.18023258 0.99838185\n",
      "22\n",
      "0.27330512 0.78803873\n",
      "22\n",
      "0.2874962 0.9142028\n",
      "22\n",
      "0.001980245 0.99957263\n",
      "22\n",
      "0.40070564 0.8688645\n",
      "0.011627883 0.95928335\n",
      "22\n",
      "0.35854858 0.8959835\n",
      "22\n",
      "0.51217115 0.8568773\n",
      "22\n",
      "0.35814363 0.9488506\n",
      "22\n",
      "0.45396826 0.9954028\n",
      "22\n",
      "0.2615176 0.99924695\n",
      "22\n",
      "0.21251476 0.9996551\n",
      "22\n",
      "0.29236668 0.86652124\n",
      "22\n",
      "0.08137715 0.87192625\n",
      "22\n",
      "0.35315984 0.9031699\n",
      "22\n",
      "0.3148734 0.66019416\n",
      "22\n",
      "0.35239017 0.57241994\n",
      "22\n",
      "0.29048842 0.99981976\n",
      "22\n",
      "0.08528033 0.9991511\n",
      "22\n",
      "0.1752941 0.72921866\n",
      "22\n",
      "0.35825163 0.81179065\n",
      "22\n",
      "0.4669236 0.8413612\n",
      "22\n",
      "0.5647418 0.9997684\n",
      "22\n",
      "0.26641554 0.9376266\n",
      "22\n",
      "0.3744076 0.9997944\n",
      "22\n",
      "0.2929048 0.99976695\n",
      "0.25680274 0.99977\n",
      "22\n",
      "0.31073788 0.9996785\n",
      "22\n",
      "0.44069815 0.99970376\n",
      "22\n",
      "0.20878422 0.7812871\n",
      "22\n",
      "0.5721187 0.99965405\n",
      "22\n",
      "0.3968073 0.9997408\n",
      "22\n",
      "0.3321871 0.9969826\n",
      "22\n",
      "0.24177504 0.8296616\n",
      "22\n",
      "0.0020492077 0.99957097\n",
      "22\n",
      "0.24001777 0.7419355\n",
      "22\n",
      "0.42459828 0.9831933\n",
      "22\n",
      "0.39570737 0.9997708\n",
      "22\n",
      "0.42429906 0.9997797\n",
      "22\n",
      "0.4145183 0.9997449\n",
      "22\n",
      "0.35752687 0.84250104\n",
      "0.35666835 0.6728945\n",
      "22\n",
      "0.43950617 0.99981475\n",
      "22\n",
      "0.34788305 0.88483685\n",
      "0.08108109 0.9994302\n",
      "22\n",
      "0.33849007 0.94865835\n",
      "22\n",
      "0.33417937 0.7913897\n",
      "22\n",
      "0.0 0.8269708\n",
      "0.46722028 0.99984795\n",
      "22\n",
      "0.21793637 0.9503708\n",
      "22\n",
      "0.38430113 0.89746916\n",
      "22\n",
      "0.35461912 0.8720355\n",
      "22\n",
      "0.53121245 0.9921188\n",
      "22\n",
      "0.4596725 0.9689526\n",
      "22\n",
      "0.5014375 0.9998057\n",
      "22\n",
      "0.5768194 0.9996909\n",
      "22\n",
      "0.28959274 0.86260414\n",
      "22\n",
      "0.0025252998 0.975\n",
      "22\n",
      "0.34348112 0.99973243\n",
      "22\n",
      "0.4554189 0.9997287\n",
      "22\n",
      "0.381289 0.99984336\n",
      "22\n",
      "0.00386101 0.9996521\n",
      "22\n",
      "0.0010638535 0.99984694\n",
      "22\n",
      "0.5059819 0.8530612\n",
      "22\n",
      "0.42174625 0.9997689\n",
      "22\n",
      "0.0014749467 0.7890809\n",
      "22\n",
      "0.28905523 0.88420475\n",
      "22\n",
      "0.39984453 0.8730459\n",
      "22\n",
      "0.33910224 0.9994487\n",
      "0.009708732 0.9995512\n",
      "22\n",
      "0.0047846735 0.9997314\n",
      "0.002145946 0.90574193\n",
      "22\n",
      "0.33216783 0.9995153\n",
      "0.42458868 0.9724269\n",
      "22\n",
      "0.32226324 0.9997891\n",
      "0.32784957 0.999524\n",
      "22\n",
      "0.24656084 0.7226891\n",
      "22\n",
      "0.41117877 0.87061\n",
      "22\n",
      "0.47747144 0.94482756\n",
      "22\n",
      "0.0027100742 0.9205607\n",
      "22\n",
      "0.50792474 0.9997548\n",
      "22\n",
      "0.4070434 0.9998008\n",
      "22\n",
      "0.45073792 0.99938154\n",
      "22\n",
      "0.22686026 0.830009\n",
      "22\n",
      "0.43625733 0.99981725\n",
      "0.46085933 0.99981385\n",
      "22\n",
      "0.49183977 0.938341\n",
      "22\n",
      "0.33153397 0.99973756\n",
      "22\n",
      "0.5544858 0.9574977\n",
      "22\n",
      "0.44727987 0.8468036\n",
      "22\n",
      "0.40466923 0.90336645\n",
      "22\n",
      "0.52347296 0.93067014\n",
      "22\n",
      "0.42064518 0.99968684\n",
      "22\n",
      "0.5 0.9992764\n",
      "22\n",
      "0.49716806 0.9997433\n",
      "22\n",
      "0.5427649 0.96532184\n",
      "22\n",
      "0.4256606 0.91538006\n",
      "22\n",
      "0.4820377 0.99977547\n",
      "22\n",
      "0.29907295 0.6877345\n",
      "22\n",
      "0.487089 0.88235295\n",
      "22\n",
      "0.32804877 0.7662879\n",
      "22\n",
      "0.009900987 0.99976146\n",
      "22\n",
      "0.3180548 0.9997215\n",
      "22\n",
      "0.44634312 0.9252778\n",
      "22\n",
      "0.33768266 0.81793845\n",
      "22\n",
      "0.12924528 0.99920946\n",
      "0.35132927 0.99975526\n",
      "22\n",
      "0.3859433 0.93070567\n",
      "22\n",
      "0.432341 0.99863386\n",
      "22\n",
      "0.33539093 0.9994583\n",
      "22\n",
      "0.4577417 0.9998184\n",
      "0.41964287 0.9997156\n",
      "22\n",
      "0.4183647 0.91639346\n",
      "22\n",
      "0.3240741 0.9997599\n",
      "22\n",
      "0.005405396 0.99973947\n",
      "22\n",
      "0.25765306 0.99950886\n",
      "22\n",
      "0.3118231 0.9996847\n",
      "0.43600416 0.9554777\n",
      "22\n",
      "0.42362526 0.95509136\n",
      "22\n",
      "0.4327885 0.87096775\n",
      "22\n",
      "0.45497793 0.9216663\n",
      "22\n",
      "0.4256702 0.9707638\n",
      "22\n",
      "0.17488262 0.8653321\n",
      "22\n",
      "0.23797336 0.99969316\n",
      "22\n",
      "0.49224022 0.99953073\n",
      "22\n",
      "0.5478406 0.99979424\n",
      "22\n",
      "0.33190066 0.9992764\n",
      "0.55918694 0.965381\n",
      "22\n",
      "0.4756501 0.9169069\n",
      "22\n",
      "0.370058 0.99965906\n",
      "22\n",
      "0.47975078 0.9996221\n",
      "22\n",
      "0.37918216 0.94166666\n",
      "22\n",
      "0.3 0.99966794\n",
      "0.5576683 0.999809\n",
      "22\n",
      "0.36251107 0.9997903\n",
      "0.55282474 0.999735\n",
      "22\n",
      "0.41252303 0.99964666\n",
      "22\n",
      "0.00082919 0.96428573\n",
      "22\n",
      "0.45235023 0.9997393\n",
      "22\n",
      "0.0053191483 0.72727275\n",
      "22\n",
      "0.4614105 0.92294586\n",
      "22\n",
      "0.19786617 0.998703\n",
      "22\n",
      "0.506809 0.9337925\n",
      "22\n",
      "0.4576128 0.9661617\n",
      "22\n",
      "0.0 0.99962425\n",
      "22\n",
      "0.39065912 0.9995136\n",
      "0.0030303597 0.88428646\n",
      "22\n",
      "0.47928005 0.8982512\n",
      "22\n",
      "0.25519627 0.98844224\n",
      "22\n",
      "0.36137527 0.62682295\n",
      "22\n",
      "0.5 0.9931035\n",
      "22\n",
      "0.301107 0.9993485\n",
      "22\n",
      "0.2875622 0.9582083\n",
      "22\n",
      "0.4099333 0.84266925\n",
      "22\n",
      "0.038724363 0.999354\n",
      "22\n",
      "0.5086207 0.8500353\n",
      "22\n",
      "0.38507462 0.99974775\n",
      "22\n",
      "0.5283927 0.99975103\n",
      "22\n",
      "0.5413599 0.87688696\n",
      "22\n",
      "0.4899483 0.99797976\n",
      "22\n",
      "0.34251606 0.9830111\n",
      "22\n",
      "0.37435603 0.9099526\n",
      "22\n",
      "0.40201926 0.9549372\n",
      "22\n",
      "0.44814813 0.9998212\n",
      "22\n",
      "0.5191215 0.9440648\n",
      "22\n",
      "0.48339483 0.99961615\n",
      "22\n",
      "0.002237171 0.97999996\n",
      "22\n",
      "0.44600937 0.9214127\n",
      "22\n",
      "0.31690928 0.9471947\n",
      "22\n",
      "0.30490196 0.99936867\n",
      "0.0 0.999642\n",
      "22\n",
      "0.45180398 0.9998383\n",
      "22\n",
      "0.5075188 0.9998046\n",
      "0.016949117 0.99959314\n",
      "22\n",
      "0.4047085 0.9050869\n",
      "22\n",
      "0.38065383 0.99982715\n",
      "22\n",
      "0.34870845 0.9912423\n",
      "22\n",
      "0.36630177 0.9997509\n",
      "22\n",
      "0.26318842 0.999446\n",
      "0.0 0.89827704\n",
      "22\n",
      "0.3020962 0.99978495\n",
      "22\n",
      "0.33533263 0.99975073\n",
      "0.43506235 0.71137756\n",
      "22\n",
      "0.37384742 0.99969745\n",
      "22\n",
      "0.31616765 0.9973861\n",
      "22\n",
      "0.42328042 0.7253521\n",
      "22\n",
      "0.37649277 0.8804245\n",
      "22\n",
      "0.47779834 0.94041866\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(\"../data/subplot.csv\")\n",
    "df1 = pd.read_csv(\"../data/subplot2.csv\")\n",
    "\n",
    "df = df.drop('IMAGERY_TITLE', axis = 1)\n",
    "df1 = df1.drop('IMAGERY_TITLE', axis = 1)\n",
    "df = pd.concat([df, df1], ignore_index = True)\n",
    "df = df.dropna(axis = 0)\n",
    "\n",
    "\n",
    "N_SAMPLES = int(df.shape[0]/196)\n",
    "print(N_SAMPLES)\n",
    "\n",
    "plot_ids = sorted(df['PLOT_ID'].unique())\n",
    "\n",
    "def reconstruct_images(plot_id):\n",
    "    subs = df[df['PLOT_ID'] == plot_id]\n",
    "    rows = []\n",
    "    lats = reversed(sorted(subs['LAT'].unique()))\n",
    "    for i, val in enumerate(lats):\n",
    "        subs_lat = subs[subs['LAT'] == val]\n",
    "        subs_lat = subs_lat.sort_values('LON', axis = 0)\n",
    "        rows.append(list(subs_lat['TREE']))\n",
    "    return rows\n",
    "\n",
    "data = [reconstruct_images(x) for x in plot_ids]\n",
    "\n",
    "def remove_blank_steps(array):\n",
    "    to_update = {}\n",
    "    sets = []\n",
    "    for k in range(6):\n",
    "        for i in range(array.shape[0]):\n",
    "            for k in range(array.shape[-1]):\n",
    "                mean = (np.mean(array[i, :, :, k]))\n",
    "                if mean == 0:\n",
    "                    sets.append(i)\n",
    "                    if i < array.shape[0] - 1:\n",
    "                        array[i, :, :, k] = array[i + 1, :, :, k]\n",
    "                    else:\n",
    "                        array[i, :, :, k] = array[i - 1, :, :, k]\n",
    "                if mean == 1:\n",
    "                    sets.append(i)\n",
    "                    if i < array.shape[0] - 1:\n",
    "                        array[i, :, :, k] = array[i + 1, :, :, k]\n",
    "                    else:\n",
    "                        array[i, :, :, k] = array[i - 1, :, :, k]\n",
    "    for i in range(array.shape[0]):\n",
    "        for k in range(array.shape[-1]):\n",
    "            mean = (np.mean(array[i, :, :, k]))\n",
    "            if mean == 0:\n",
    "                if i < array.shape[0] - 2:\n",
    "                    array[i, :, :, k] = array[i + 2, :, :, k]\n",
    "                else:\n",
    "                    array[i, :, :, k] = array[i - 2, :, :, k]\n",
    "            if mean == 1:\n",
    "                if i < array.shape[0] - 2:\n",
    "                    array[i, :, :, k] = array[i + 2, :, :, k]\n",
    "                else:\n",
    "                    array[i, :, :, k] = array[i - 2, :, :, k]\n",
    "    return array\n",
    "\n",
    "import os\n",
    "image_size = 14\n",
    "\n",
    "def ndvi(x):\n",
    "    # (B8 - B4)/(B8 + B4)\n",
    "    ndvis = [(im[:, :, 6] - im[:, :, 2]) / (im[:, :, 6] + im[:, :, 2]) for im in x]\n",
    "    min_ndvi = min([np.min(x) for x in ndvis])\n",
    "    max_ndvi = max([np.max(x) for x in ndvis])\n",
    "    if min_ndvi < -1 or max_ndvi > 1:\n",
    "        print(\"ERROR\")\n",
    "    ndvis = [((x + 1) / 2) for x in ndvis]\n",
    "    min_ndvi = min([np.min(x) for x in ndvis])\n",
    "    max_ndvi = max([np.max(x) for x in ndvis])\n",
    "    print(min_ndvi, max_ndvi)\n",
    "    x_padding = np.zeros((x.shape[0], image_size, image_size, 1))\n",
    "    x = np.concatenate((x, x_padding), axis = 3)\n",
    "    # Iterate over each time step and add NDVI in as the 11th channel\n",
    "    for i in range(x.shape[0]):\n",
    "        x[i, :, :, 10] = ndvis[i]\n",
    "    return x\n",
    "\n",
    "def evi(x):\n",
    "    # 2.5 x (08 - 04) / (08 + 6 * 04 - 7.5 * 02 + 1)\n",
    "    evis = [2.5 * ((im[:, :, 6] - im[:, :, 2]) / (im[:, :, 6] + 6 * im[:,:, 2] - 7.5 * im[:, :, 0] + 1)) for im in x]\n",
    "    x_padding = np.zeros((x.shape[0], image_size, image_size, 1))\n",
    "    x = np.concatenate((x, x_padding), axis = 3)\n",
    "    # Iterate over each time step and add NDVI in as the 11th channel\n",
    "    for i in range(x.shape[0]):\n",
    "        x[i, :, :, 11] = evis[i]\n",
    "    return x\n",
    "    \n",
    "def savi(x):\n",
    "    # (1.5)(08 - 04)/ (08 + 04 + 0.5)\n",
    "    savis = [(1.5 * im[:, :, 6] - im[:, :, 2]) / (im[:, :, 6] + im[:, :, 2] + 0.5) for im in x]\n",
    "    x_padding = np.zeros((x.shape[0], image_size, image_size, 1))\n",
    "    x = np.concatenate((x, x_padding), axis = 3)\n",
    "    # Iterate over each time step and add NDVI in as the 11th channel\n",
    "    for i in range(x.shape[0]):\n",
    "        x[i, :, :, 12] = savis[i]\n",
    "    return x\n",
    "\n",
    "# Initiate empty lists to store the X and Y data in\n",
    "data_x = []\n",
    "data_y = []\n",
    "binary_y = []\n",
    "data_location_x = []\n",
    "data_location_y = []\n",
    "lengths = []\n",
    "\n",
    "# Iterate over each plot\n",
    "pad = True\n",
    "flip = True\n",
    "for i in plot_ids:\n",
    "    # Load the sentinel imagery\n",
    "    x = np.load(\"../data/ids/\" + str(i) + \".npy\")\n",
    "    # Shape check\n",
    "    if x.shape[1] == image_size:\n",
    "        x = ndvi(x)                # calc NDVI\n",
    "        x = evi(x)\n",
    "        x = savi(x)\n",
    "        x = remove_blank_steps(x)\n",
    "        y = reconstruct_images(i)\n",
    "        if sum([sum(x) for x in y]) >= 1:\n",
    "            binary_y.append(1)\n",
    "        else:\n",
    "            binary_y.append(0)\n",
    "        lengths.append(x.shape[0])\n",
    "        #x = np.median(x, axis = 0) # and calculate the median over the time steps\n",
    "        if pad:\n",
    "            if x.shape[0] < 24:\n",
    "                print(x.shape[0])\n",
    "                padding = np.zeros((24 - x.shape[0], image_size, image_size, 13))\n",
    "                x = np.concatenate((x, padding), axis = 0)\n",
    "        data_x.append(x)\n",
    "        data_y.append(y)\n",
    "        if flip:\n",
    "                # FLIP HORIZONTAL\n",
    "            x1 = np.flip(x, 1)\n",
    "            data_x.append(x1)\n",
    "            data_y.append(np.flip(y, 0))\n",
    "            lengths.append(x.shape[0])\n",
    "    \n",
    "                # FLIP BOTH\n",
    "            x2 = np.flip(x, 2)\n",
    "            x2 = np.flip(x2, 1)\n",
    "            data_x.append(x2)\n",
    "            data_y.append(np.flip(y, [0, 1]))\n",
    "            lengths.append(x.shape[0])\n",
    "                # FLIP VERTICAL\n",
    "            x3 = np.flip(x, 2)\n",
    "            data_x.append(x3)\n",
    "            data_y.append(np.flip(y, 1))\n",
    "            lengths.append(x.shape[0])\n",
    "\n",
    "data_x = np.stack(data_x)\n",
    "data_y = np.stack(data_y)\n",
    "data_y = np.reshape(data_y, (N_SAMPLES*4, 14, 14, 1))\n",
    "binary_y = np.stack(binary_y)\n",
    "lengths = np.stack(lengths)\n",
    "lengths = np.reshape(lengths, (lengths.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1344"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_foc(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: A tensor of the same shape as `y_pred`\n",
    "        :param y_pred:  A tensor resulting from a sigmoid\n",
    "        :return: Output tensor.\n",
    "        \"\"\"\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "        epsilon = K.epsilon()\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n",
    "        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n",
    "\n",
    "        return -K.sum(0.5 * K.pow(1. - pt_1, 2) * K.log(pt_1)) \\\n",
    "               -K.sum((1 - 0.5) * K.pow(pt_0, 2) * K.log(1. - pt_0))\n",
    "\n",
    "def foc_lovasz(y_true, y_pred):\n",
    "    #jaccard_loss = jaccard_distance(y_true, y_pred)\n",
    "    lovasz = lovasz_hinge(y_pred, y_true)\n",
    "    #pred_reshape = tf.reshape(y_pred, (-1, 14, 14))\n",
    "    #true_reshape = tf.reshape(y_true, (-1, 14, 14))\n",
    "    focal_loss = bin_foc(y_true, y_pred)\n",
    "    summed = lovasz + np.log(focal_loss)\n",
    "    return summed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thirty_meter(true, pred):\n",
    "    indices_x = [x for x in range(0, 13, 1)]\n",
    "    indices_y = [x for x in range(0, 13, 1)]\n",
    "    perms = [(y, x) for x, y in itertools.product(indices_y, indices_x)]\n",
    "    \n",
    "    #perms = ([list(zip(indices_x, p)) for p in itertools.permutations(indices_y)])\n",
    "    #perms = [item for sublist in perms for item in sublist]\n",
    "    #perms = list(set(perms))\n",
    "    indexes = [([a, a + 1], [b, b + 1]) for a,b in perms]\n",
    "    subs_true = []\n",
    "    subs_pred = []\n",
    "    for i in indexes:\n",
    "        true_i = true[i[0][0]:i[0][1], i[1][0]:i[1][1]]\n",
    "        pred_i = pred[i[0][0]:i[0][1], i[1][0]:i[1][1]]\n",
    "        subs_true.append(true_i)\n",
    "        subs_pred.append(pred_i)\n",
    "    pred = [np.sum(x) for x in subs_pred]\n",
    "    true = [np.sum(x) for x in subs_true]\n",
    "    true_positives = []\n",
    "    false_positives = []\n",
    "    false_negatives = []\n",
    "    for p, t in zip(pred, true):\n",
    "        if p > t:\n",
    "            tp = p - (p - t)\n",
    "            fp = p - tp\n",
    "            fn = 0\n",
    "        if t >= p:\n",
    "            tp = t\n",
    "            fp = 0\n",
    "            fn = t - p\n",
    "        true_positives.append(tp)\n",
    "        false_positives.append(fp)\n",
    "        false_negatives.append(fn)\n",
    "    prec = [x / (x + y) for x,y in zip(true_positives, false_positives) if (x+y) > 0]\n",
    "    prec = [x for x in prec if not np.isnan(x)]\n",
    "    rec = [x / (x + y) for x,y in zip(true_positives, false_negatives) if (x+y) > 0]\n",
    "    rec = [x for x in rec if not np.isnan(x)]\n",
    "    \n",
    "    #recall = [min(x / y, 1) for x, y in zip(pred, true) if y > 0]\n",
    "    #precision = [(y - x) / x for x, y in zip(pred, true)]\n",
    "    #print(precision)\n",
    "    return rec, prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss 0.6505783200263977 Val: 0.5553079843521118 P 1.0 R 0.5 F1 0.6666666666666666\n",
      "Epoch 2: Loss 0.543699324131012 Val: 0.5372703671455383 P 1.0 R 0.5 F1 0.6666666666666666\n",
      "Epoch 3: Loss 0.5380793213844299 Val: 0.5383763313293457 P 1.0 R 0.5 F1 0.6666666666666666\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-7579b4ac7f23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m                                                         \u001b[0mlength\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                                                         \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                                                         \u001b[0mis_training\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                                                         })\n\u001b[1;32m     48\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/remote_sensing/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/remote_sensing/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/remote_sensing/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/remote_sensing/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/remote_sensing/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/remote_sensing/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return 1. - score\n",
    "\n",
    "def smooth_jaccard(y_true, y_pred, smooth=100):\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return (1 - jac) * smooth\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    return binary_crossentropy(y_true, y_pred) + (0.5 * dice_loss(y_true, y_pred))\n",
    "\n",
    "\n",
    "optimizer = AdaBoundOptimizer(learning_rate=5e-7, final_lr=5e-4, beta1=0.9, beta2=0.999, amsbound=False)\n",
    "loss2 = bce_dice_loss(labels, fm)\n",
    "train_op = optimizer.minimize(loss2)\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "saver = tf.train.Saver(max_to_keep = 2)\n",
    "\n",
    "# dev --> 225\n",
    "\n",
    "# Run training loop\n",
    "for i in range(100):\n",
    "    randomize = np.arange(1088)\n",
    "    np.random.shuffle(randomize)\n",
    "    test_randomize = np.arange(1088, 1344)\n",
    "    np.random.shuffle(test_randomize)\n",
    "\n",
    "    losses = []\n",
    "    val_loss = []\n",
    "\n",
    "    for k in range(34):\n",
    "        batch_ids = randomize[k*32:(k+1)*32]\n",
    "        op, tr = sess.run([train_op, loss2], feed_dict={inp: data_x[batch_ids, :, :, :],\n",
    "                                                        length: lengths[batch_ids],\n",
    "                                                        labels: data_y[batch_ids, :, :],\n",
    "                                                        is_training: True\n",
    "                                                        })\n",
    "        losses.append(tr)\n",
    "    for j in range(8):\n",
    "        batch_ids = test_randomize[j*32:(j+1)*32]\n",
    "        vl, y = sess.run([loss2, fm], feed_dict={inp: data_x[batch_ids, :, :, :],\n",
    "                                                 length: lengths[batch_ids],\n",
    "                                                 labels: data_y[batch_ids, :, :],\n",
    "                                                 is_training: False,\n",
    "                                                })\n",
    "        val_loss.append(vl)\n",
    "        \n",
    "    recalls = []\n",
    "    precisions = []\n",
    "    for m in range(1088, 1344):\n",
    "        true = data_y[m].reshape((14, 14))\n",
    "        y = sess.run([fm], feed_dict={inp: data_x[m].reshape(1, 24, 14, 14, 13),\n",
    "                                  length: lengths[m].reshape(1, 1),\n",
    "                                  is_training: False,\n",
    "                                  })[0]\n",
    "        pred = y.reshape(14, 14)\n",
    "        #pred = np.sigmoid(pred)\n",
    "        pred[np.where(pred > 0.45)] = 1\n",
    "        pred[np.where(pred < 0.45)] = 0\n",
    "        rec, prec = thirty_meter(true, pred)\n",
    "        recalls.append(rec)\n",
    "        precisions.append(prec)\n",
    "    precisions = [item for sublist in precisions for item in sublist]\n",
    "    recalls = [item for sublist in recalls for item in sublist]\n",
    "    precision = np.mean(precisions)\n",
    "    recall = np.mean(recalls)\n",
    "    f1_score = 2 * ((precision * recall) / (precision + recall))\n",
    "    save_path = saver.save(sess, \"../models/correct_ndvi/model\")\n",
    "    print(\"Epoch {}: Loss {} Val: {} P {} R {} F1 {}\".format(i + 1,\n",
    "                                                             np.mean(losses), np.mean(val_loss),\n",
    "                                                             precision, recall, f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a61491358>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = int((len(data_x))*0.8) + 21 # 105, 25, 215, 90, 140\n",
    "y = sess.run([fm], feed_dict={inp: data_x[idx].reshape(1, 24, 14, 14, 13),\n",
    "                              length: lengths[idx].reshape(1, 1),\n",
    "                              })\n",
    "\n",
    "sns.heatmap(data_y[idx][:, :, :].reshape(14, 14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a61477470>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = y[0][:, :, :].reshape(14, 14)\n",
    "#pred[np.where(pred > 0.6)] = 1\n",
    "sns.heatmap(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recalls = []\n",
    "precisions = []\n",
    "for i in range(1089, 1344):\n",
    "    true = data_y[i].reshape((14, 14))\n",
    "    y = sess.run([fm], feed_dict={inp: data_x[i].reshape(1, 24, 14, 14, 13),\n",
    "                              length: lengths[i].reshape(1, 1),\n",
    "                              })[0]\n",
    "    pred = y.reshape(14, 14)\n",
    "    pred[np.where(pred > 0.3)] = 1\n",
    "    pred[np.where(pred < 0.3)] = 0\n",
    "    rec, prec = thirty_meter(true, pred)\n",
    "    #rec, prec = half_hectare_accuracy(true, pred)\n",
    "    recalls.append(rec)\n",
    "    precisions.append(prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6755302310857867"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recalls = [item for sublist in recalls for item in sublist]\n",
    "np.mean(recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7484008528784648"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precisions = [item for sublist in precisions for item in sublist]\n",
    "np.mean(precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"AdaBound for TensorFlow.\"\"\"\n",
    "\n",
    "from tensorflow.python.eager import context\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import resource_variable_ops\n",
    "from tensorflow.python.ops import state_ops\n",
    "from tensorflow.python.ops import variable_scope\n",
    "from tensorflow.python.training import optimizer\n",
    "from tensorflow.python.ops.clip_ops import clip_by_value\n",
    "\n",
    "\"\"\"Implements AdaBound algorithm.\n",
    "    It has been proposed in `Adaptive Gradient Methods with Dynamic Bound of Learning Rate`_.\n",
    "    Arguments:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float, optional): Adam learning rate (default: 1e-3)\n",
    "        betas (Tuple[float, float], optional): coefficients used for computing\n",
    "            running averages of gradient and its square (default: (0.9, 0.999))\n",
    "        final_lr (float, optional): final (SGD) learning rate (default: 0.1)\n",
    "        gamma (float, optional): convergence speed of the bound functions (default: 1e-3)\n",
    "        eps (float, optional): term added to the denominator to improve\n",
    "            numerical stability (default: 1e-8)\n",
    "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
    "        amsbound (boolean, optional): whether to use the AMSBound variant of this algorithm\n",
    "    .. Adaptive Gradient Methods with Dynamic Bound of Learning Rate:\n",
    "        https://openreview.net/forum?id=Bkg3g2R9FX\n",
    "    \"\"\"\n",
    "\n",
    "class AdaBoundOptimizer(optimizer.Optimizer):\n",
    "    def __init__(self, learning_rate=0.001, final_lr=0.1, beta1=0.9, beta2=0.999,\n",
    "                 gamma=1e-3, epsilon=1e-8, amsbound=False,\n",
    "                 use_locking=False, name=\"AdaBound\"):\n",
    "        super(AdaBoundOptimizer, self).__init__(use_locking, name)\n",
    "        self._lr = learning_rate\n",
    "        self._final_lr = final_lr\n",
    "        self._beta1 = beta1\n",
    "        self._beta2 = beta2\n",
    "        self._epsilon = epsilon\n",
    "\n",
    "        self._gamma = gamma\n",
    "        self._amsbound = amsbound\n",
    "\n",
    "        self._lr_t = None\n",
    "        self._beta1_t = None\n",
    "        self._beta2_t = None\n",
    "        self._epsilon_t = None\n",
    "\n",
    "    def _create_slots(self, var_list):\n",
    "        first_var = min(var_list, key=lambda x: x.name)\n",
    "\n",
    "        graph = None if context.executing_eagerly() else ops.get_default_graph()\n",
    "        create_new = self._get_non_slot_variable(\"beta1_power\", graph) is None\n",
    "        if not create_new and context.in_graph_mode():\n",
    "            create_new = (self._get_non_slot_variable(\"beta1_power\", graph).graph is not first_var.graph)\n",
    "\n",
    "        if create_new:\n",
    "            self._create_non_slot_variable(initial_value=self._beta1,\n",
    "                                           name=\"beta1_power\",\n",
    "                                           colocate_with=first_var)\n",
    "            self._create_non_slot_variable(initial_value=self._beta2,\n",
    "                                           name=\"beta2_power\",\n",
    "                                           colocate_with=first_var)\n",
    "            self._create_non_slot_variable(initial_value=self._gamma,\n",
    "                                           name=\"gamma_multi\",\n",
    "                                           colocate_with=first_var)\n",
    "        # Create slots for the first and second moments.\n",
    "        for v in var_list :\n",
    "            self._zeros_slot(v, \"m\", self._name)\n",
    "            self._zeros_slot(v, \"v\", self._name)\n",
    "            self._zeros_slot(v, \"vhat\", self._name)\n",
    "\n",
    "\n",
    "    def _prepare(self):\n",
    "        self._lr_t = ops.convert_to_tensor(self._lr)\n",
    "        self._base_lr_t = ops.convert_to_tensor(self._lr)\n",
    "        self._beta1_t = ops.convert_to_tensor(self._beta1)\n",
    "        self._beta2_t = ops.convert_to_tensor(self._beta2)\n",
    "        self._epsilon_t = ops.convert_to_tensor(self._epsilon)\n",
    "        self._gamma_t = ops.convert_to_tensor(self._gamma)\n",
    "\n",
    "    def _apply_dense(self, grad, var):\n",
    "        graph = None if context.executing_eagerly() else ops.get_default_graph()\n",
    "        beta1_power = math_ops.cast(self._get_non_slot_variable(\"beta1_power\", graph=graph), var.dtype.base_dtype)\n",
    "        beta2_power = math_ops.cast(self._get_non_slot_variable(\"beta2_power\", graph=graph), var.dtype.base_dtype)\n",
    "        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n",
    "        base_lr_t = math_ops.cast(self._base_lr_t, var.dtype.base_dtype)\n",
    "        beta1_t = math_ops.cast(self._beta1_t, var.dtype.base_dtype)\n",
    "        beta2_t = math_ops.cast(self._beta2_t, var.dtype.base_dtype)\n",
    "        epsilon_t = math_ops.cast(self._epsilon_t, var.dtype.base_dtype)\n",
    "        gamma_multi = math_ops.cast(self._get_non_slot_variable(\"gamma_multi\", graph=graph), var.dtype.base_dtype)\n",
    "\n",
    "        step_size = (lr_t * math_ops.sqrt(1 - beta2_power) / (1 - beta1_power))\n",
    "        final_lr = self._final_lr * lr_t / base_lr_t\n",
    "        lower_bound = final_lr * (1. - 1. / (gamma_multi + 1.))\n",
    "        upper_bound = final_lr * (1. + 1. / (gamma_multi))\n",
    "\n",
    "        # m_t = beta1 * m + (1 - beta1) * g_t\n",
    "        m = self.get_slot(var, \"m\")\n",
    "        m_scaled_g_values = grad * (1 - beta1_t)\n",
    "        m_t = state_ops.assign(m, beta1_t * m + m_scaled_g_values, use_locking=self._use_locking)\n",
    "\n",
    "        # v_t = beta2 * v + (1 - beta2) * (g_t * g_t)\n",
    "        v = self.get_slot(var, \"v\")\n",
    "        v_scaled_g_values = (grad * grad) * (1 - beta2_t)\n",
    "        v_t = state_ops.assign(v, beta2_t * v + v_scaled_g_values, use_locking=self._use_locking)\n",
    "\n",
    "        # amsgrad\n",
    "        vhat = self.get_slot(var, \"vhat\")\n",
    "        if self._amsbound :\n",
    "            vhat_t = state_ops.assign(vhat, math_ops.maximum(v_t, vhat))\n",
    "            v_sqrt = math_ops.sqrt(vhat_t)\n",
    "        else :\n",
    "            vhat_t = state_ops.assign(vhat, vhat)\n",
    "            v_sqrt = math_ops.sqrt(v_t)\n",
    "\n",
    "\n",
    "        # Compute the bounds\n",
    "        step_size_bound = step_size / (v_sqrt + epsilon_t)\n",
    "        bounded_lr = m_t * clip_by_value(step_size_bound, lower_bound, upper_bound)\n",
    "\n",
    "        var_update = state_ops.assign_sub(var, bounded_lr, use_locking=self._use_locking)\n",
    "        return control_flow_ops.group(*[var_update, m_t, v_t, vhat_t])\n",
    "\n",
    "    def _resource_apply_dense(self, grad, var):\n",
    "        graph = None if context.executing_eagerly() else ops.get_default_graph()\n",
    "        beta1_power = math_ops.cast(self._get_non_slot_variable(\"beta1_power\", graph=graph), grad.dtype.base_dtype)\n",
    "        beta2_power = math_ops.cast(self._get_non_slot_variable(\"beta2_power\", graph=graph), grad.dtype.base_dtype)\n",
    "        lr_t = math_ops.cast(self._lr_t, grad.dtype.base_dtype)\n",
    "        base_lr_t = math_ops.cast(self._base_lr_t, var.dtype.base_dtype)\n",
    "        beta1_t = math_ops.cast(self._beta1_t, grad.dtype.base_dtype)\n",
    "        beta2_t = math_ops.cast(self._beta2_t, grad.dtype.base_dtype)\n",
    "        epsilon_t = math_ops.cast(self._epsilon_t, grad.dtype.base_dtype)\n",
    "        gamma_multi = math_ops.cast(self._get_non_slot_variable(\"gamma_multi\", graph=graph), var.dtype.base_dtype)\n",
    "\n",
    "        step_size = (lr_t * math_ops.sqrt(1 - beta2_power) / (1 - beta1_power))\n",
    "        final_lr = self._final_lr * lr_t / base_lr_t\n",
    "        lower_bound = final_lr * (1. - 1. / (gamma_multi + 1.))\n",
    "        upper_bound = final_lr * (1. + 1. / (gamma_multi))\n",
    "\n",
    "        # m_t = beta1 * m + (1 - beta1) * g_t\n",
    "        m = self.get_slot(var, \"m\")\n",
    "        m_scaled_g_values = grad * (1 - beta1_t)\n",
    "        m_t = state_ops.assign(m, beta1_t * m + m_scaled_g_values, use_locking=self._use_locking)\n",
    "\n",
    "        # v_t = beta2 * v + (1 - beta2) * (g_t * g_t)\n",
    "        v = self.get_slot(var, \"v\")\n",
    "        v_scaled_g_values = (grad * grad) * (1 - beta2_t)\n",
    "        v_t = state_ops.assign(v, beta2_t * v + v_scaled_g_values, use_locking=self._use_locking)\n",
    "\n",
    "        # amsgrad\n",
    "        vhat = self.get_slot(var, \"vhat\")\n",
    "        if self._amsbound:\n",
    "            vhat_t = state_ops.assign(vhat, math_ops.maximum(v_t, vhat))\n",
    "            v_sqrt = math_ops.sqrt(vhat_t)\n",
    "        else:\n",
    "            vhat_t = state_ops.assign(vhat, vhat)\n",
    "            v_sqrt = math_ops.sqrt(v_t)\n",
    "\n",
    "        # Compute the bounds\n",
    "        step_size_bound = step_size / (v_sqrt + epsilon_t)\n",
    "        bounded_lr = m_t * clip_by_value(step_size_bound, lower_bound, upper_bound)\n",
    "\n",
    "        var_update = state_ops.assign_sub(var, bounded_lr, use_locking=self._use_locking)\n",
    "\n",
    "        return control_flow_ops.group(*[var_update, m_t, v_t, vhat_t])\n",
    "\n",
    "    def _apply_sparse_shared(self, grad, var, indices, scatter_add):\n",
    "        graph = None if context.executing_eagerly() else ops.get_default_graph()\n",
    "        beta1_power = math_ops.cast(self._get_non_slot_variable(\"beta1_power\", graph=graph), var.dtype.base_dtype)\n",
    "        beta2_power = math_ops.cast(self._get_non_slot_variable(\"beta2_power\", graph=graph), var.dtype.base_dtype)\n",
    "        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n",
    "        base_lr_t = math_ops.cast(self._base_lr_t, var.dtype.base_dtype)\n",
    "        beta1_t = math_ops.cast(self._beta1_t, var.dtype.base_dtype)\n",
    "        beta2_t = math_ops.cast(self._beta2_t, var.dtype.base_dtype)\n",
    "        epsilon_t = math_ops.cast(self._epsilon_t, var.dtype.base_dtype)\n",
    "        gamma_t = math_ops.cast(self._gamma_t, var.dtype.base_dtype)\n",
    "\n",
    "        step_size = (lr_t * math_ops.sqrt(1 - beta2_power) / (1 - beta1_power))\n",
    "        final_lr = self._final_lr * lr_t / base_lr_t\n",
    "        lower_bound = final_lr * (1. - 1. / (gamma_t + 1.))\n",
    "        upper_bound = final_lr * (1. + 1. / (gamma_t))\n",
    "\n",
    "        # m_t = beta1 * m + (1 - beta1) * g_t\n",
    "        m = self.get_slot(var, \"m\")\n",
    "        m_scaled_g_values = grad * (1 - beta1_t)\n",
    "        m_t = state_ops.assign(m, m * beta1_t, use_locking=self._use_locking)\n",
    "        with ops.control_dependencies([m_t]):\n",
    "            m_t = scatter_add(m, indices, m_scaled_g_values)\n",
    "\n",
    "        # v_t = beta2 * v + (1 - beta2) * (g_t * g_t)\n",
    "        v = self.get_slot(var, \"v\")\n",
    "        v_scaled_g_values = (grad * grad) * (1 - beta2_t)\n",
    "        v_t = state_ops.assign(v, v * beta2_t, use_locking=self._use_locking)\n",
    "        with ops.control_dependencies([v_t]):\n",
    "            v_t = scatter_add(v, indices, v_scaled_g_values)\n",
    "\n",
    "        # amsgrad\n",
    "        vhat = self.get_slot(var, \"vhat\")\n",
    "        if self._amsbound:\n",
    "            vhat_t = state_ops.assign(vhat, math_ops.maximum(v_t, vhat))\n",
    "            v_sqrt = math_ops.sqrt(vhat_t)\n",
    "        else:\n",
    "            vhat_t = state_ops.assign(vhat, vhat)\n",
    "            v_sqrt = math_ops.sqrt(v_t)\n",
    "\n",
    "        # Compute the bounds\n",
    "        step_size_bound = step_size / (v_sqrt + epsilon_t)\n",
    "        bounded_lr = m_t * clip_by_value(step_size_bound, lower_bound, upper_bound)\n",
    "\n",
    "        var_update = state_ops.assign_sub(var, bounded_lr, use_locking=self._use_locking)\n",
    "\n",
    "        return control_flow_ops.group(*[var_update, m_t, v_t, vhat_t])\n",
    "\n",
    "    def _apply_sparse(self, grad, var):\n",
    "        return self._apply_sparse_shared(\n",
    "            grad.values, var, grad.indices,\n",
    "            lambda x, i, v: state_ops.scatter_add(  # pylint: disable=g-long-lambda\n",
    "                x, i, v, use_locking=self._use_locking))\n",
    "\n",
    "    def _resource_scatter_add(self, x, i, v):\n",
    "        with ops.control_dependencies(\n",
    "                [resource_variable_ops.resource_scatter_add(x, i, v)]):\n",
    "            return x.value()\n",
    "\n",
    "    def _resource_apply_sparse(self, grad, var, indices):\n",
    "        return self._apply_sparse_shared(\n",
    "            grad, var, indices, self._resource_scatter_add)\n",
    "\n",
    "    def _finish(self, update_ops, name_scope):\n",
    "        # Update the power accumulators.\n",
    "        with ops.control_dependencies(update_ops):\n",
    "            graph = None if context.executing_eagerly() else ops.get_default_graph()\n",
    "            beta1_power = self._get_non_slot_variable(\"beta1_power\", graph=graph)\n",
    "            beta2_power = self._get_non_slot_variable(\"beta2_power\", graph=graph)\n",
    "            gamma_multi = self._get_non_slot_variable(\"gamma_multi\", graph=graph)\n",
    "            with ops.colocate_with(beta1_power):\n",
    "                update_beta1 = beta1_power.assign(\n",
    "                    beta1_power * self._beta1_t,\n",
    "                    use_locking=self._use_locking)\n",
    "                update_beta2 = beta2_power.assign(\n",
    "                    beta2_power * self._beta2_t,\n",
    "                    use_locking=self._use_locking)\n",
    "                update_gamma = gamma_multi.assign(\n",
    "                    gamma_multi + self._gamma_t,\n",
    "                    use_locking=self._use_locking)\n",
    "        return control_flow_ops.group(*update_ops + [update_beta1, update_beta2, update_gamma],\n",
    "                                      name=name_scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_sensing",
   "language": "python",
   "name": "remote_sensing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
