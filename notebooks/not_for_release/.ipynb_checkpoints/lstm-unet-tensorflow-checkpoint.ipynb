{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#tf.reset_default_graph()\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "from keras import backend as K\n",
    "K.set_session(sess)\n",
    "\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.layers import ELU\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import itertools\n",
    "from tflearn.layers.conv import global_avg_pool\n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "from tensorflow.contrib.layers import batch_norm\n",
    "from keras.regularizers import l1\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvGRUCell(tf.nn.rnn_cell.RNNCell):\n",
    "  \"\"\"A GRU cell with convolutions instead of multiplications.\"\"\"\n",
    "\n",
    "  def __init__(self, shape, filters, kernel, padding = 'SAME', activation=tf.tanh, normalize=False, data_format='channels_last', reuse=None):\n",
    "    super(ConvGRUCell, self).__init__(_reuse=reuse)\n",
    "    self._filters = filters\n",
    "    self._kernel = kernel\n",
    "    self._activation = activation\n",
    "    self._normalize = normalize\n",
    "    self._padding = padding\n",
    "    if data_format == 'channels_last':\n",
    "        self._size = tf.TensorShape(shape + [self._filters])\n",
    "        self._feature_axis = self._size.ndims\n",
    "        self._data_format = None\n",
    "    elif data_format == 'channels_first':\n",
    "        self._size = tf.TensorShape([self._filters] + shape)\n",
    "        self._feature_axis = 0\n",
    "        self._data_format = 'NC'\n",
    "    else:\n",
    "        raise ValueError('Unknown data_format')\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    return self._size\n",
    "\n",
    "  @property\n",
    "  def output_size(self):\n",
    "    return self._size\n",
    "\n",
    "  def call(self, x, h):\n",
    "    channels = x.shape[self._feature_axis].value\n",
    "\n",
    "    with tf.variable_scope('gates'):\n",
    "      inputs = tf.concat([x, h], axis=self._feature_axis)\n",
    "      n = channels + self._filters\n",
    "      m = 2 * self._filters if self._filters > 1 else 2\n",
    "      W = tf.get_variable('kernel', self._kernel + [n, m])\n",
    "      y = tf.nn.convolution(inputs, W, self._padding, data_format=self._data_format)\n",
    "      if self._normalize:\n",
    "        r, u = tf.split(y, 2, axis=self._feature_axis)\n",
    "        r = tf.contrib.layers.layer_norm(r)\n",
    "        u = tf.contrib.layers.layer_norm(u)\n",
    "      else:\n",
    "        y += tf.get_variable('bias', [m], initializer=tf.ones_initializer())\n",
    "        r, u = tf.split(y, 2, axis=self._feature_axis)\n",
    "      r, u = tf.sigmoid(r), tf.sigmoid(u)\n",
    "\n",
    "    with tf.variable_scope('candidate'):\n",
    "      inputs = tf.concat([x, r * h], axis=self._feature_axis)\n",
    "      n = channels + self._filters\n",
    "      m = self._filters\n",
    "      W = tf.get_variable('kernel', self._kernel + [n, m])\n",
    "      y = tf.nn.convolution(inputs, W, self._padding, data_format=self._data_format)\n",
    "      if self._normalize:\n",
    "        y = tf.contrib.layers.layer_norm(y)\n",
    "      else:\n",
    "        y += tf.get_variable('bias', [m], initializer=tf.zeros_initializer())\n",
    "      h = u * h + (1 - u) * self._activation(y)\n",
    "\n",
    "    return h, h\n",
    "\n",
    "def Batch_Normalization(x, training, scope):\n",
    "    with arg_scope([batch_norm],\n",
    "                   scope=scope,\n",
    "                   updates_collections=None,\n",
    "                   decay=0.9,\n",
    "                   center=True,\n",
    "                   scale=True,\n",
    "                   zero_debias_moving_mean=True) :\n",
    "        return tf.cond(training,\n",
    "                       lambda : batch_norm(inputs=x, is_training=training, reuse=None),\n",
    "                       lambda : batch_norm(inputs=x, is_training=training, reuse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()\n",
    "class ReflectionPadding2D(Layer):\n",
    "    def __init__(self, padding=(1, 1), **kwargs):\n",
    "        self.padding = tuple(padding)\n",
    "        self.input_spec = [InputSpec(ndim=4)]\n",
    "        super(ReflectionPadding2D, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, s):\n",
    "        \"\"\" If you are using \"channels_last\" configuration\"\"\"\n",
    "        return (s[0], s[1] + 2 * self.padding[0], s[2] + 2 * self.padding[1], s[3])\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        w_pad,h_pad = self.padding\n",
    "        return tf.pad(x, [[0,0], [h_pad,h_pad], [w_pad,w_pad], [0,0] ], 'REFLECT')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-3ef345dd7e0e>:47: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "FIRST GRU (?, 24, 14, 14, 48)\n",
      "DOWNSAMPLE (?, 24, 7, 7, 48)\n",
      "SECOND GRU (?, 7, 7, 64)\n",
      "WARNING:tensorflow:From <ipython-input-4-3ef345dd7e0e>:10: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "Down block conv (?, 7, 7, 96)\n",
      "Upsampling (?, 14, 14, 96)\n",
      "Up block conv 2 (?, 14, 14, 48)\n",
      "Up block conv 3 (?, 14, 14, 24)\n",
      "(?, 14, 14, 1)\n"
     ]
    }
   ],
   "source": [
    "reg = keras.regularizers.l1(0.001)\n",
    "inp = tf.placeholder(tf.float32, shape=(None, 24, 14, 14, 13))\n",
    "length = tf.placeholder(tf.int32, shape = (None, 1))\n",
    "labels = tf.placeholder(tf.float32, shape=(None, 14, 14))#, 1))\n",
    "length2 = tf.reshape(length, (-1,))\n",
    "is_training = tf.placeholder_with_default(False, (), 'is_training')\n",
    "\n",
    "def Fully_connected(x, units, layer_name='fully_connected') :\n",
    "    with tf.name_scope(layer_name) :\n",
    "        return tf.layers.dense(inputs=x, use_bias=True, units=units)\n",
    "\n",
    "def Relu(x):\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def Sigmoid(x):\n",
    "    return tf.nn.sigmoid(x)\n",
    "\n",
    "def Global_Average_Pooling(x):\n",
    "    return global_avg_pool(x, name='Global_avg_pooling')\n",
    "\n",
    "def Squeeze_excitation_layer(input_x, out_dim, ratio, layer_name):\n",
    "    with tf.name_scope(layer_name) :\n",
    "        squeeze = global_avg_pool(input_x)\n",
    "\n",
    "        excitation = Fully_connected(squeeze, units=out_dim / ratio, layer_name=layer_name+'_fully_connected1')\n",
    "        excitation = Relu(excitation)\n",
    "        excitation = Fully_connected(excitation, units=out_dim, layer_name=layer_name+'_fully_connected2')\n",
    "        excitation = Sigmoid(excitation)\n",
    "\n",
    "        excitation = tf.reshape(excitation, [-1,1,1,out_dim])\n",
    "\n",
    "        scale = input_x * excitation\n",
    "\n",
    "        return scale\n",
    "    \n",
    "#inp_pad = TimeDistributed(tf.keras.layers.ZeroPadding2D((1, 1)))(inp)\n",
    "\n",
    "with tf.variable_scope('10'):\n",
    "    # Downsampling Block 1 (14 x 14)\n",
    "    cell_10 = ConvGRUCell(shape = [14, 14],\n",
    "                   filters = 24,\n",
    "                   kernel = [3,3],\n",
    "                   padding = 'SAME')\n",
    "\n",
    "    def convGRU(x, cell, ln):\n",
    "        output, final = tf.nn.bidirectional_dynamic_rnn(\n",
    "            cell, cell, x, ln, dtype=tf.float32)\n",
    "        output = tf.concat(output, -1)\n",
    "        final = tf.concat(final, -1)\n",
    "        return [output, final]\n",
    "\n",
    "# Return the final state and the output states\n",
    "first_conv = convGRU(inp, cell_10, length2)\n",
    "print(\"FIRST GRU {}\".format(first_conv[0].shape))\n",
    "\n",
    "\n",
    "downsampled = TimeDistributed(MaxPool2D(pool_size = (2, 2)))(first_conv[0])\n",
    "print(\"DOWNSAMPLE {}\".format(downsampled.shape))\n",
    "\n",
    "# Downsampling block 2 (7 x 7)\n",
    "with tf.variable_scope('7'):\n",
    "    cell_7 = ConvGRUCell(shape = [7, 7],\n",
    "                   filters = 32,\n",
    "                   kernel = [3,3],\n",
    "                   padding = 'SAME')\n",
    "    state_7 = convGRU(downsampled, cell_7, length2)\n",
    "print(\"SECOND GRU {}\".format(state_7[1].shape))\n",
    "\n",
    "conv_block_7_u = Conv2D(filters = 96, kernel_size = (3, 3), padding = 'same', activity_regularizer=reg)(state_7[1])\n",
    "elu_7_u = ELU()(conv_block_7_u)\n",
    "x = Batch_Normalization(elu_7_u, training=is_training, scope = 'bn1')\n",
    "squeezed = Squeeze_excitation_layer(input_x = x, out_dim = 96, ratio = 4, layer_name = \"squeezed\")\n",
    "print(\"Down block conv {}\".format(elu_7_u.shape))\n",
    "\n",
    "\n",
    "\n",
    "# Upsampling final block (14 x 14)\n",
    "upsampling_14 = UpSampling2D((2, 2))(squeezed)\n",
    "#upsampling_14 = tf.keras.layers.Conv2DTranspose(filters = 96, kernel_size = (2, 2), strides=(2, 2), padding='same')(squeezed)\n",
    "print(\"Upsampling {}\".format(upsampling_14.shape))\n",
    "padded = ReflectionPadding2D((1, 1))(upsampling_14)\n",
    "fm = Conv2D(filters = 48,\n",
    "            kernel_size = (3, 3), \n",
    "            padding = 'valid',\n",
    "            activity_regularizer=l1(0.001),\n",
    "            )(padded)\n",
    "elu = ELU()(fm)\n",
    "x = Batch_Normalization(elu, training=is_training, scope = 'bn2')\n",
    "\n",
    "squeezed2 = Squeeze_excitation_layer(input_x = x, out_dim = 48, ratio = 4, layer_name = \"squeezed2\")\n",
    "print(\"Up block conv 2 {}\".format(elu.shape))\n",
    "concat = Concatenate(axis = -1)([squeezed2, first_conv[1]])\n",
    "padded = ReflectionPadding2D((1, 1))(fm)\n",
    "fm = Conv2D(filters = 24,\n",
    "            kernel_size = (3, 3), \n",
    "            padding = 'valid',\n",
    "            )(padded)\n",
    "elu = ELU()(fm)\n",
    "print(\"Up block conv 3 {}\".format(elu.shape))\n",
    "# Output layer\n",
    "fm = Conv2D(filters = 1,\n",
    "            kernel_size = (1, 1), \n",
    "            padding = 'valid',\n",
    "            activation = 'sigmoid'\n",
    "            )(elu)\n",
    "print(fm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15984\n",
      "48\n",
      "7992\n",
      "24\n",
      "46080\n",
      "64\n",
      "23040\n",
      "32\n",
      "55296\n",
      "96\n",
      "96\n",
      "96\n",
      "2304\n",
      "24\n",
      "2304\n",
      "96\n",
      "41472\n",
      "48\n",
      "48\n",
      "48\n",
      "576\n",
      "12\n",
      "576\n",
      "48\n",
      "10368\n",
      "24\n",
      "24\n",
      "1\n",
      "206821\n"
     ]
    }
   ],
   "source": [
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    # shape is an array of tf.Dimension\n",
    "    shape = variable.get_shape()\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        variable_parameters *= dim.value\n",
    "    print(variable_parameters)\n",
    "    total_parameters += variable_parameters\n",
    "print(total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336\n",
      "0.3673082 0.7603346\n",
      "0.3491656 0.9995814\n",
      "22\n",
      "0.3871069 0.7687279\n",
      "22\n",
      "0.33417815 0.8142235\n",
      "22\n",
      "0.002267599 0.9975062\n",
      "0.39245468 0.7822398\n",
      "22\n",
      "0.018518478 0.9987654\n",
      "22\n",
      "0.0 0.9993017\n",
      "0.3425532 0.70296013\n",
      "22\n",
      "0.4108352 0.9997895\n",
      "22\n",
      "0.34487897 0.9009186\n",
      "0.53952295 0.84337354\n",
      "0.18374261 0.9995545\n",
      "22\n",
      "0.13010052 0.60302514\n",
      "22\n",
      "0.3105899 0.8685879\n",
      "22\n",
      "0.2887975 0.9896908\n",
      "22\n",
      "0.0014124513 0.9996896\n",
      "22\n",
      "0.23668641 0.99915755\n",
      "22\n",
      "0.29181817 0.8222011\n",
      "22\n",
      "0.0045248866 0.8501582\n",
      "22\n",
      "0.2584334 0.80423415\n",
      "22\n",
      "0.38473642 0.984609\n",
      "22\n",
      "0.41035423 0.9077408\n",
      "0.38322064 0.8306217\n",
      "0.3645833 0.9701493\n",
      "22\n",
      "0.063926905 0.9997599\n",
      "22\n",
      "0.4337806 0.999689\n",
      "22\n",
      "0.1746633 0.6894075\n",
      "22\n",
      "0.31032127 0.91107\n",
      "22\n",
      "0.41572785 0.7161658\n",
      "22\n",
      "0.19358599 0.8611111\n",
      "22\n",
      "0.39320165 0.8684366\n",
      "22\n",
      "0.0 0.92077863\n",
      "0.30590612 0.78264093\n",
      "22\n",
      "0.27876106 0.89801836\n",
      "22\n",
      "0.0 0.7867664\n",
      "22\n",
      "0.40803814 0.9996698\n",
      "22\n",
      "0.35833332 0.99970436\n",
      "22\n",
      "0.3681694 0.8744757\n",
      "22\n",
      "0.096410275 0.8437039\n",
      "22\n",
      "0.2955739 0.92014974\n",
      "22\n",
      "0.50090694 0.9014919\n",
      "22\n",
      "0.27903348 0.9996344\n",
      "22\n",
      "0.45578232 0.99968636\n",
      "22\n",
      "0.35435057 0.99967337\n",
      "22\n",
      "0.0 0.7417355\n",
      "22\n",
      "0.37985706 0.95855856\n",
      "22\n",
      "0.39617723 0.6945298\n",
      "22\n",
      "0.2922636 0.99978\n",
      "22\n",
      "0.42020816 0.6305693\n",
      "22\n",
      "0.49460045 0.9997597\n",
      "22\n",
      "0.49167857 0.7548135\n",
      "22\n",
      "0.15544674 0.7680502\n",
      "22\n",
      "0.19649449 0.7865112\n",
      "22\n",
      "0.30178475 0.9996568\n",
      "0.43549785 0.79252243\n",
      "0.13424125 0.9997204\n",
      "22\n",
      "0.3191268 0.8366606\n",
      "0.41762114 0.9997759\n",
      "22\n",
      "0.40950423 0.9993364\n",
      "22\n",
      "0.3089526 0.9994612\n",
      "22\n",
      "0.4179262 0.7579204\n",
      "0.2304075 0.9995671\n",
      "22\n",
      "0.25301206 0.99950373\n",
      "22\n",
      "0.0011723638 0.8292117\n",
      "22\n",
      "0.40502408 0.78553617\n",
      "0.26287264 0.99977577\n",
      "22\n",
      "0.0016366839 0.84738374\n",
      "22\n",
      "0.42904156 0.99968046\n",
      "22\n",
      "0.35589075 0.90558076\n",
      "22\n",
      "0.31215298 0.9644521\n",
      "22\n",
      "0.23336253 0.73027825\n",
      "0.36319524 0.99256015\n",
      "0.064606756 0.90047395\n",
      "22\n",
      "0.32623512 0.8108995\n",
      "22\n",
      "0.33007336 0.95626974\n",
      "22\n",
      "0.3248009 0.99945056\n",
      "22\n",
      "0.470981 0.99978244\n",
      "22\n",
      "0.39266303 0.8648433\n",
      "22\n",
      "0.24876848 0.99965894\n",
      "22\n",
      "0.05873716 0.99972034\n",
      "22\n",
      "0.0017094314 0.77190876\n",
      "22\n",
      "0.008849531 0.9985528\n",
      "22\n",
      "0.32590413 0.7887856\n",
      "22\n",
      "0.3789693 0.928321\n",
      "22\n",
      "0.3445452 0.7546742\n",
      "22\n",
      "0.26993865 0.9655173\n",
      "22\n",
      "0.23644754 0.99941385\n",
      "22\n",
      "0.11918369 0.9379684\n",
      "22\n",
      "0.3982188 0.8724623\n",
      "22\n",
      "0.3385147 0.99976575\n",
      "22\n",
      "0.17464116 0.6804435\n",
      "22\n",
      "0.3702387 0.99930507\n",
      "22\n",
      "0.4144032 0.80874574\n",
      "22\n",
      "0.34709096 0.8510215\n",
      "22\n",
      "0.36305556 0.7025261\n",
      "22\n",
      "0.3857843 0.99971354\n",
      "22\n",
      "0.15258855 0.8004695\n",
      "22\n",
      "0.47793427 0.90938723\n",
      "22\n",
      "0.3941541 0.98254955\n",
      "22\n",
      "0.38875753 0.7676856\n",
      "0.34804833 0.8804426\n",
      "0.4285174 0.85224533\n",
      "22\n",
      "0.3588987 0.8521683\n",
      "0.32683253 0.8534653\n",
      "22\n",
      "0.03666666 0.7468355\n",
      "22\n",
      "0.3734269 0.8675023\n",
      "22\n",
      "0.30542564 0.81811106\n",
      "22\n",
      "0.4225422 0.78728604\n",
      "22\n",
      "0.22857144 0.99976945\n",
      "22\n",
      "0.393268 0.99978757\n",
      "22\n",
      "0.35229176 0.7330437\n",
      "22\n",
      "0.002127707 0.9996828\n",
      "22\n",
      "0.3752399 0.99978495\n",
      "22\n",
      "0.28844112 0.8718196\n",
      "22\n",
      "0.34723127 0.75359213\n",
      "22\n",
      "0.3707989 0.80626523\n",
      "22\n",
      "0.38114443 0.99966395\n",
      "22\n",
      "0.36584288 0.78945696\n",
      "22\n",
      "0.31178978 0.9997884\n",
      "22\n",
      "0.34814048 0.88769305\n",
      "22\n",
      "0.35336977 0.9811828\n",
      "22\n",
      "0.3356102 0.7086817\n",
      "22\n",
      "0.43370944 0.9997492\n",
      "22\n",
      "0.53612983 0.99948215\n",
      "22\n",
      "0.13341647 0.99956524\n",
      "22\n",
      "0.32003468 0.99916875\n",
      "22\n",
      "0.27673936 0.8267001\n",
      "22\n",
      "0.36858475 0.91984487\n",
      "22\n",
      "0.2327852 0.88237727\n",
      "22\n",
      "0.34304473 0.978022\n",
      "22\n",
      "0.3066333 0.99977046\n",
      "0.42553192 0.9996394\n",
      "22\n",
      "0.34462363 0.8780609\n",
      "22\n",
      "0.4043716 0.99889505\n",
      "0.35082605 0.9757576\n",
      "22\n",
      "0.42476425 0.99973106\n",
      "22\n",
      "0.3231663 0.6869217\n",
      "22\n",
      "0.3505896 0.88933957\n",
      "22\n",
      "0.21913329 0.9603509\n",
      "22\n",
      "0.35864156 0.9130095\n",
      "22\n",
      "0.0 0.7938144\n",
      "0.002123177 0.9966887\n",
      "22\n",
      "0.42097837 0.81372297\n",
      "22\n",
      "0.38715354 0.7870658\n",
      "22\n",
      "0.026957601 0.62002337\n",
      "22\n",
      "0.3943074 0.9996841\n",
      "22\n",
      "0.34513515 0.87486553\n",
      "22\n",
      "0.18023258 0.99838185\n",
      "22\n",
      "0.27330512 0.78803873\n",
      "22\n",
      "0.2874962 0.9142028\n",
      "22\n",
      "0.001980245 0.99957263\n",
      "22\n",
      "0.40070564 0.8688645\n",
      "0.011627883 0.95928335\n",
      "22\n",
      "0.35854858 0.8959835\n",
      "22\n",
      "0.51217115 0.8568773\n",
      "22\n",
      "0.35814363 0.9488506\n",
      "22\n",
      "0.45396826 0.9954028\n",
      "22\n",
      "0.2615176 0.99924695\n",
      "22\n",
      "0.21251476 0.9996551\n",
      "22\n",
      "0.29236668 0.86652124\n",
      "22\n",
      "0.08137715 0.87192625\n",
      "22\n",
      "0.35315984 0.9031699\n",
      "22\n",
      "0.3148734 0.66019416\n",
      "22\n",
      "0.35239017 0.57241994\n",
      "22\n",
      "0.29048842 0.99981976\n",
      "22\n",
      "0.08528033 0.9991511\n",
      "22\n",
      "0.1752941 0.72921866\n",
      "22\n",
      "0.35825163 0.81179065\n",
      "22\n",
      "0.4669236 0.8413612\n",
      "22\n",
      "0.5647418 0.9997684\n",
      "22\n",
      "0.26641554 0.9376266\n",
      "22\n",
      "0.3744076 0.9997944\n",
      "22\n",
      "0.2929048 0.99976695\n",
      "0.25680274 0.99977\n",
      "22\n",
      "0.31073788 0.9996785\n",
      "22\n",
      "0.44069815 0.99970376\n",
      "22\n",
      "0.20878422 0.7812871\n",
      "22\n",
      "0.5721187 0.99965405\n",
      "22\n",
      "0.3968073 0.9997408\n",
      "22\n",
      "0.3321871 0.9969826\n",
      "22\n",
      "0.24177504 0.8296616\n",
      "22\n",
      "0.0020492077 0.99957097\n",
      "22\n",
      "0.24001777 0.7419355\n",
      "22\n",
      "0.42459828 0.9831933\n",
      "22\n",
      "0.39570737 0.9997708\n",
      "22\n",
      "0.42429906 0.9997797\n",
      "22\n",
      "0.4145183 0.9997449\n",
      "22\n",
      "0.35752687 0.84250104\n",
      "0.35666835 0.6728945\n",
      "22\n",
      "0.43950617 0.99981475\n",
      "22\n",
      "0.34788305 0.88483685\n",
      "0.08108109 0.9994302\n",
      "22\n",
      "0.33849007 0.94865835\n",
      "22\n",
      "0.33417937 0.7913897\n",
      "22\n",
      "0.0 0.8269708\n",
      "0.46722028 0.99984795\n",
      "22\n",
      "0.21793637 0.9503708\n",
      "22\n",
      "0.38430113 0.89746916\n",
      "22\n",
      "0.35461912 0.8720355\n",
      "22\n",
      "0.53121245 0.9921188\n",
      "22\n",
      "0.4596725 0.9689526\n",
      "22\n",
      "0.5014375 0.9998057\n",
      "22\n",
      "0.5768194 0.9996909\n",
      "22\n",
      "0.28959274 0.86260414\n",
      "22\n",
      "0.0025252998 0.975\n",
      "22\n",
      "0.34348112 0.99973243\n",
      "22\n",
      "0.4554189 0.9997287\n",
      "22\n",
      "0.381289 0.99984336\n",
      "22\n",
      "0.00386101 0.9996521\n",
      "22\n",
      "0.0010638535 0.99984694\n",
      "22\n",
      "0.5059819 0.8530612\n",
      "22\n",
      "0.42174625 0.9997689\n",
      "22\n",
      "0.0014749467 0.7890809\n",
      "22\n",
      "0.28905523 0.88420475\n",
      "22\n",
      "0.39984453 0.8730459\n",
      "22\n",
      "0.33910224 0.9994487\n",
      "0.009708732 0.9995512\n",
      "22\n",
      "0.0047846735 0.9997314\n",
      "0.002145946 0.90574193\n",
      "22\n",
      "0.33216783 0.9995153\n",
      "0.42458868 0.9724269\n",
      "22\n",
      "0.32226324 0.9997891\n",
      "0.32784957 0.999524\n",
      "22\n",
      "0.24656084 0.7226891\n",
      "22\n",
      "0.41117877 0.87061\n",
      "22\n",
      "0.47747144 0.94482756\n",
      "22\n",
      "0.0027100742 0.9205607\n",
      "22\n",
      "0.50792474 0.9997548\n",
      "22\n",
      "0.4070434 0.9998008\n",
      "22\n",
      "0.45073792 0.99938154\n",
      "22\n",
      "0.22686026 0.830009\n",
      "22\n",
      "0.43625733 0.99981725\n",
      "0.46085933 0.99981385\n",
      "22\n",
      "0.49183977 0.938341\n",
      "22\n",
      "0.33153397 0.99973756\n",
      "22\n",
      "0.5544858 0.9574977\n",
      "22\n",
      "0.44727987 0.8468036\n",
      "22\n",
      "0.40466923 0.90336645\n",
      "22\n",
      "0.52347296 0.93067014\n",
      "22\n",
      "0.42064518 0.99968684\n",
      "22\n",
      "0.5 0.9992764\n",
      "22\n",
      "0.49716806 0.9997433\n",
      "22\n",
      "0.5427649 0.96532184\n",
      "22\n",
      "0.4256606 0.91538006\n",
      "22\n",
      "0.4820377 0.99977547\n",
      "22\n",
      "0.29907295 0.6877345\n",
      "22\n",
      "0.487089 0.88235295\n",
      "22\n",
      "0.32804877 0.7662879\n",
      "22\n",
      "0.009900987 0.99976146\n",
      "22\n",
      "0.3180548 0.9997215\n",
      "22\n",
      "0.44634312 0.9252778\n",
      "22\n",
      "0.33768266 0.81793845\n",
      "22\n",
      "0.12924528 0.99920946\n",
      "0.35132927 0.99975526\n",
      "22\n",
      "0.3859433 0.93070567\n",
      "22\n",
      "0.432341 0.99863386\n",
      "22\n",
      "0.33539093 0.9994583\n",
      "22\n",
      "0.4577417 0.9998184\n",
      "0.41964287 0.9997156\n",
      "22\n",
      "0.4183647 0.91639346\n",
      "22\n",
      "0.3240741 0.9997599\n",
      "22\n",
      "0.005405396 0.99973947\n",
      "22\n",
      "0.25765306 0.99950886\n",
      "22\n",
      "0.3118231 0.9996847\n",
      "0.43600416 0.9554777\n",
      "22\n",
      "0.42362526 0.95509136\n",
      "22\n",
      "0.4327885 0.87096775\n",
      "22\n",
      "0.45497793 0.9216663\n",
      "22\n",
      "0.4256702 0.9707638\n",
      "22\n",
      "0.17488262 0.8653321\n",
      "22\n",
      "0.23797336 0.99969316\n",
      "22\n",
      "0.49224022 0.99953073\n",
      "22\n",
      "0.5478406 0.99979424\n",
      "22\n",
      "0.33190066 0.9992764\n",
      "0.55918694 0.965381\n",
      "22\n",
      "0.4756501 0.9169069\n",
      "22\n",
      "0.370058 0.99965906\n",
      "22\n",
      "0.47975078 0.9996221\n",
      "22\n",
      "0.37918216 0.94166666\n",
      "22\n",
      "0.3 0.99966794\n",
      "0.5576683 0.999809\n",
      "22\n",
      "0.36251107 0.9997903\n",
      "0.55282474 0.999735\n",
      "22\n",
      "0.41252303 0.99964666\n",
      "22\n",
      "0.00082919 0.96428573\n",
      "22\n",
      "0.45235023 0.9997393\n",
      "22\n",
      "0.0053191483 0.72727275\n",
      "22\n",
      "0.4614105 0.92294586\n",
      "22\n",
      "0.19786617 0.998703\n",
      "22\n",
      "0.506809 0.9337925\n",
      "22\n",
      "0.4576128 0.9661617\n",
      "22\n",
      "0.0 0.99962425\n",
      "22\n",
      "0.39065912 0.9995136\n",
      "0.0030303597 0.88428646\n",
      "22\n",
      "0.47928005 0.8982512\n",
      "22\n",
      "0.25519627 0.98844224\n",
      "22\n",
      "0.36137527 0.62682295\n",
      "22\n",
      "0.5 0.9931035\n",
      "22\n",
      "0.301107 0.9993485\n",
      "22\n",
      "0.2875622 0.9582083\n",
      "22\n",
      "0.4099333 0.84266925\n",
      "22\n",
      "0.038724363 0.999354\n",
      "22\n",
      "0.5086207 0.8500353\n",
      "22\n",
      "0.38507462 0.99974775\n",
      "22\n",
      "0.5283927 0.99975103\n",
      "22\n",
      "0.5413599 0.87688696\n",
      "22\n",
      "0.4899483 0.99797976\n",
      "22\n",
      "0.34251606 0.9830111\n",
      "22\n",
      "0.37435603 0.9099526\n",
      "22\n",
      "0.40201926 0.9549372\n",
      "22\n",
      "0.44814813 0.9998212\n",
      "22\n",
      "0.5191215 0.9440648\n",
      "22\n",
      "0.48339483 0.99961615\n",
      "22\n",
      "0.002237171 0.97999996\n",
      "22\n",
      "0.44600937 0.9214127\n",
      "22\n",
      "0.31690928 0.9471947\n",
      "22\n",
      "0.30490196 0.99936867\n",
      "0.0 0.999642\n",
      "22\n",
      "0.45180398 0.9998383\n",
      "22\n",
      "0.5075188 0.9998046\n",
      "0.016949117 0.99959314\n",
      "22\n",
      "0.4047085 0.9050869\n",
      "22\n",
      "0.38065383 0.99982715\n",
      "22\n",
      "0.34870845 0.9912423\n",
      "22\n",
      "0.36630177 0.9997509\n",
      "22\n",
      "0.26318842 0.999446\n",
      "0.0 0.89827704\n",
      "22\n",
      "0.3020962 0.99978495\n",
      "22\n",
      "0.33533263 0.99975073\n",
      "0.43506235 0.71137756\n",
      "22\n",
      "0.37384742 0.99969745\n",
      "22\n",
      "0.31616765 0.9973861\n",
      "22\n",
      "0.42328042 0.7253521\n",
      "22\n",
      "0.37649277 0.8804245\n",
      "22\n",
      "0.47779834 0.94041866\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(\"../data/subplot.csv\")\n",
    "df1 = pd.read_csv(\"../data/subplot2.csv\")\n",
    "\n",
    "df = df.drop('IMAGERY_TITLE', axis = 1)\n",
    "df1 = df1.drop('IMAGERY_TITLE', axis = 1)\n",
    "df = pd.concat([df, df1], ignore_index = True)\n",
    "df = df.dropna(axis = 0)\n",
    "\n",
    "\n",
    "N_SAMPLES = int(df.shape[0]/196)\n",
    "print(N_SAMPLES)\n",
    "\n",
    "plot_ids = sorted(df['PLOT_ID'].unique())\n",
    "\n",
    "def reconstruct_images(plot_id):\n",
    "    subs = df[df['PLOT_ID'] == plot_id]\n",
    "    rows = []\n",
    "    lats = reversed(sorted(subs['LAT'].unique()))\n",
    "    for i, val in enumerate(lats):\n",
    "        subs_lat = subs[subs['LAT'] == val]\n",
    "        subs_lat = subs_lat.sort_values('LON', axis = 0)\n",
    "        rows.append(list(subs_lat['TREE']))\n",
    "    return rows\n",
    "\n",
    "data = [reconstruct_images(x) for x in plot_ids]\n",
    "\n",
    "def remove_blank_steps(array):\n",
    "    to_update = {}\n",
    "    sets = []\n",
    "    for k in range(6):\n",
    "        for i in range(array.shape[0]):\n",
    "            for k in range(array.shape[-1]):\n",
    "                mean = (np.mean(array[i, :, :, k]))\n",
    "                if mean == 0:\n",
    "                    sets.append(i)\n",
    "                    if i < array.shape[0] - 1:\n",
    "                        array[i, :, :, k] = array[i + 1, :, :, k]\n",
    "                    else:\n",
    "                        array[i, :, :, k] = array[i - 1, :, :, k]\n",
    "                if mean == 1:\n",
    "                    sets.append(i)\n",
    "                    if i < array.shape[0] - 1:\n",
    "                        array[i, :, :, k] = array[i + 1, :, :, k]\n",
    "                    else:\n",
    "                        array[i, :, :, k] = array[i - 1, :, :, k]\n",
    "    for i in range(array.shape[0]):\n",
    "        for k in range(array.shape[-1]):\n",
    "            mean = (np.mean(array[i, :, :, k]))\n",
    "            if mean == 0:\n",
    "                if i < array.shape[0] - 2:\n",
    "                    array[i, :, :, k] = array[i + 2, :, :, k]\n",
    "                else:\n",
    "                    array[i, :, :, k] = array[i - 2, :, :, k]\n",
    "            if mean == 1:\n",
    "                if i < array.shape[0] - 2:\n",
    "                    array[i, :, :, k] = array[i + 2, :, :, k]\n",
    "                else:\n",
    "                    array[i, :, :, k] = array[i - 2, :, :, k]\n",
    "    return array\n",
    "\n",
    "import os\n",
    "image_size = 14\n",
    "\n",
    "def ndvi(x):\n",
    "    # (B8 - B4)/(B8 + B4)\n",
    "    ndvis = [(im[:, :, 6] - im[:, :, 2]) / (im[:, :, 6] + im[:, :, 2]) for im in x]\n",
    "    min_ndvi = min([np.min(x) for x in ndvis])\n",
    "    max_ndvi = max([np.max(x) for x in ndvis])\n",
    "    if min_ndvi < -1 or max_ndvi > 1:\n",
    "        print(\"ERROR\")\n",
    "    ndvis = [((x + 1) / 2) for x in ndvis]\n",
    "    min_ndvi = min([np.min(x) for x in ndvis])\n",
    "    max_ndvi = max([np.max(x) for x in ndvis])\n",
    "    print(min_ndvi, max_ndvi)\n",
    "    x_padding = np.zeros((x.shape[0], image_size, image_size, 1))\n",
    "    x = np.concatenate((x, x_padding), axis = 3)\n",
    "    # Iterate over each time step and add NDVI in as the 11th channel\n",
    "    for i in range(x.shape[0]):\n",
    "        x[i, :, :, 10] = ndvis[i]\n",
    "    return x\n",
    "\n",
    "def evi(x):\n",
    "    # 2.5 x (08 - 04) / (08 + 6 * 04 - 7.5 * 02 + 1)\n",
    "    evis = [2.5 * ((im[:, :, 6] - im[:, :, 2]) / (im[:, :, 6] + 6 * im[:,:, 2] - 7.5 * im[:, :, 0] + 1)) for im in x]\n",
    "    x_padding = np.zeros((x.shape[0], image_size, image_size, 1))\n",
    "    x = np.concatenate((x, x_padding), axis = 3)\n",
    "    # Iterate over each time step and add NDVI in as the 11th channel\n",
    "    for i in range(x.shape[0]):\n",
    "        x[i, :, :, 11] = evis[i]\n",
    "    return x\n",
    "    \n",
    "def savi(x):\n",
    "    # (1.5)(08 - 04)/ (08 + 04 + 0.5)\n",
    "    savis = [(1.5 * im[:, :, 6] - im[:, :, 2]) / (im[:, :, 6] + im[:, :, 2] + 0.5) for im in x]\n",
    "    x_padding = np.zeros((x.shape[0], image_size, image_size, 1))\n",
    "    x = np.concatenate((x, x_padding), axis = 3)\n",
    "    # Iterate over each time step and add NDVI in as the 11th channel\n",
    "    for i in range(x.shape[0]):\n",
    "        x[i, :, :, 12] = savis[i]\n",
    "    return x\n",
    "\n",
    "# Initiate empty lists to store the X and Y data in\n",
    "data_x = []\n",
    "data_y = []\n",
    "binary_y = []\n",
    "data_location_x = []\n",
    "data_location_y = []\n",
    "lengths = []\n",
    "\n",
    "# Iterate over each plot\n",
    "pad = True\n",
    "flip = True\n",
    "for i in plot_ids:\n",
    "    # Load the sentinel imagery\n",
    "    x = np.load(\"../data/ids/\" + str(i) + \".npy\")\n",
    "    # Shape check\n",
    "    if x.shape[1] == image_size:\n",
    "        x = ndvi(x)                # calc NDVI\n",
    "        x = evi(x)\n",
    "        x = savi(x)\n",
    "        x = remove_blank_steps(x)\n",
    "        y = reconstruct_images(i)\n",
    "        if sum([sum(x) for x in y]) >= 1:\n",
    "            binary_y.append(1)\n",
    "        else:\n",
    "            binary_y.append(0)\n",
    "        lengths.append(x.shape[0])\n",
    "        #x = np.median(x, axis = 0) # and calculate the median over the time steps\n",
    "        if pad:\n",
    "            if x.shape[0] < 24:\n",
    "                print(x.shape[0])\n",
    "                padding = np.zeros((24 - x.shape[0], image_size, image_size, 13))\n",
    "                x = np.concatenate((x, padding), axis = 0)\n",
    "        data_x.append(x)\n",
    "        data_y.append(y)\n",
    "        if flip:\n",
    "                # FLIP HORIZONTAL\n",
    "            x1 = np.flip(x, 1)\n",
    "            data_x.append(x1)\n",
    "            data_y.append(np.flip(y, 0))\n",
    "            lengths.append(x.shape[0])\n",
    "    \n",
    "                # FLIP BOTH\n",
    "            x2 = np.flip(x, 2)\n",
    "            x2 = np.flip(x2, 1)\n",
    "            data_x.append(x2)\n",
    "            data_y.append(np.flip(y, [0, 1]))\n",
    "            lengths.append(x.shape[0])\n",
    "                # FLIP VERTICAL\n",
    "            x3 = np.flip(x, 2)\n",
    "            data_x.append(x3)\n",
    "            data_y.append(np.flip(y, 1))\n",
    "            lengths.append(x.shape[0])\n",
    "\n",
    "data_x = np.stack(data_x)\n",
    "data_y = np.stack(data_y)\n",
    "data_y = np.reshape(data_y, (N_SAMPLES*4, 14, 14, 1))\n",
    "binary_y = np.stack(binary_y)\n",
    "lengths = np.stack(lengths)\n",
    "lengths = np.reshape(lengths, (lengths.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_foc(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: A tensor of the same shape as `y_pred`\n",
    "        :param y_pred:  A tensor resulting from a sigmoid\n",
    "        :return: Output tensor.\n",
    "        \"\"\"\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "        epsilon = K.epsilon()\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n",
    "        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n",
    "\n",
    "        return -K.sum(0.5 * K.pow(1. - pt_1, 2) * K.log(pt_1)) \\\n",
    "               -K.sum((1 - 0.5) * K.pow(pt_0, 2) * K.log(1. - pt_0))\n",
    "    \n",
    "def focal_loss(prediction_tensor, target_tensor, weights=None, alpha=0.25, gamma=2):\n",
    "    r\"\"\"Compute focal loss for predictions.\n",
    "        Multi-labels Focal loss formula:\n",
    "            FL = -alpha * (z-p)^gamma * log(p) -(1-alpha) * p^gamma * log(1-p)\n",
    "                 ,which alpha = 0.25, gamma = 2, p = sigmoid(x), z = target_tensor.\n",
    "    Args:\n",
    "     prediction_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "        num_classes] representing the predicted logits for each class\n",
    "     target_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "        num_classes] representing one-hot encoded classification targets\n",
    "     weights: A float tensor of shape [batch_size, num_anchors]\n",
    "     alpha: A scalar tensor for focal loss alpha hyper-parameter\n",
    "     gamma: A scalar tensor for focal loss gamma hyper-parameter\n",
    "    Returns:\n",
    "        loss: A (scalar) tensor representing the value of the loss function\n",
    "    \"\"\"\n",
    "    sigmoid_p = prediction_tensor\n",
    "    zeros = array_ops.zeros_like(sigmoid_p, dtype=sigmoid_p.dtype)\n",
    "    \n",
    "    # For poitive prediction, only need consider front part loss, back part is 0;\n",
    "    # target_tensor > zeros <=> z=1, so poitive coefficient = z - p.\n",
    "    pos_p_sub = array_ops.where(target_tensor > zeros, target_tensor - sigmoid_p, zeros)\n",
    "    \n",
    "    # For negative prediction, only need consider back part loss, front part is 0;\n",
    "    # target_tensor > zeros <=> z=1, so negative coefficient = 0.\n",
    "    neg_p_sub = array_ops.where(target_tensor > zeros, zeros, sigmoid_p)\n",
    "    per_entry_cross_ent = - alpha * (pos_p_sub ** gamma) * tf.log(tf.clip_by_value(sigmoid_p, 1e-8, 1.0)) \\\n",
    "                          - (1 - alpha) * (neg_p_sub ** gamma) * tf.log(tf.clip_by_value(1.0 - sigmoid_p, 1e-8, 1.0))\n",
    "    return tf.reduce_sum(per_entry_cross_ent)\n",
    "\n",
    "def foc_lovasz(y_true, y_pred):\n",
    "    #jaccard_loss = jaccard_distance(y_true, y_pred)\n",
    "    lovasz = lovasz_hinge(y_pred, y_true)\n",
    "    #pred_reshape = tf.reshape(y_pred, (-1, 14, 14))\n",
    "    #true_reshape = tf.reshape(y_true, (-1, 14, 14))\n",
    "    focal_loss = bin_foc(y_true, y_pred)\n",
    "    summed = lovasz + np.log(focal_loss)\n",
    "    return summed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thirty_meter(true, pred):\n",
    "    indices_x = [x for x in range(0, 13, 1)]\n",
    "    indices_y = [x for x in range(0, 13, 1)]\n",
    "    perms = [(y, x) for x, y in itertools.product(indices_y, indices_x)]\n",
    "    \n",
    "    #perms = ([list(zip(indices_x, p)) for p in itertools.permutations(indices_y)])\n",
    "    #perms = [item for sublist in perms for item in sublist]\n",
    "    #perms = list(set(perms))\n",
    "    indexes = [([a, a + 1], [b, b + 1]) for a,b in perms]\n",
    "    subs_true = []\n",
    "    subs_pred = []\n",
    "    for i in indexes:\n",
    "        true_i = true[i[0][0]:i[0][1], i[1][0]:i[1][1]]\n",
    "        pred_i = pred[i[0][0]:i[0][1], i[1][0]:i[1][1]]\n",
    "        subs_true.append(true_i)\n",
    "        subs_pred.append(pred_i)\n",
    "    pred = [np.sum(x) for x in subs_pred]\n",
    "    true = [np.sum(x) for x in subs_true]\n",
    "    true_positives = []\n",
    "    false_positives = []\n",
    "    false_negatives = []\n",
    "    for p, t in zip(pred, true):\n",
    "        if p > t:\n",
    "            tp = p - (p - t)\n",
    "            fp = p - tp\n",
    "            fn = 0\n",
    "        if t >= p:\n",
    "            tp = t\n",
    "            fp = 0\n",
    "            fn = t - p\n",
    "        true_positives.append(tp)\n",
    "        false_positives.append(fp)\n",
    "        false_negatives.append(fn)\n",
    "    prec = [x / (x + y) for x,y in zip(true_positives, false_positives) if (x+y) > 0]\n",
    "    prec = [x for x in prec if not np.isnan(x)]\n",
    "    rec = [x / (x + y) for x,y in zip(true_positives, false_negatives) if (x+y) > 0]\n",
    "    rec = [x for x in rec if not np.isnan(x)]\n",
    "    \n",
    "    #recall = [min(x / y, 1) for x, y in zip(pred, true) if y > 0]\n",
    "    #precision = [(y - x) / x for x, y in zip(pred, true)]\n",
    "    #print(precision)\n",
    "    return rec, prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def lovasz_grad(gt_sorted):\n",
    "    \"\"\"\n",
    "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
    "    See Alg. 1 in paper\n",
    "    \"\"\"\n",
    "    gts = tf.reduce_sum(gt_sorted)\n",
    "    intersection = gts - tf.cumsum(gt_sorted)\n",
    "    union = gts + tf.cumsum(1. - gt_sorted)\n",
    "    jaccard = 1. - intersection / union\n",
    "    jaccard = tf.concat((jaccard[0:1], jaccard[1:] - jaccard[:-1]), 0)\n",
    "    return jaccard\n",
    "\n",
    "\n",
    "# --------------------------- BINARY LOSSES ---------------------------\n",
    "\n",
    "\n",
    "def lovasz_hinge(logits, labels, per_image=True, ignore=None):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n",
    "      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n",
    "      per_image: compute the loss per image instead of per batch\n",
    "      ignore: void class id\n",
    "    \"\"\"\n",
    "    if per_image:\n",
    "        def treat_image(log_lab):\n",
    "            log, lab = log_lab\n",
    "            log, lab = tf.expand_dims(log, 0), tf.expand_dims(lab, 0)\n",
    "            log, lab = flatten_binary_scores(log, lab, ignore)\n",
    "            return lovasz_hinge_flat(log, lab)\n",
    "        losses = tf.map_fn(treat_image, (logits, labels), dtype=tf.float32)\n",
    "        loss = tf.reduce_mean(losses)\n",
    "    else:\n",
    "        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def lovasz_hinge_flat(logits, labels):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
    "      labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
    "      ignore: label to ignore\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_loss():\n",
    "        labelsf = tf.cast(labels, logits.dtype)\n",
    "        signs = 2. * labelsf - 1.\n",
    "        errors = 1. - logits * tf.stop_gradient(signs)\n",
    "        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], name=\"descending_sort\")\n",
    "        gt_sorted = tf.gather(labelsf, perm)\n",
    "        grad = lovasz_grad(gt_sorted)\n",
    "        loss = tf.tensordot(tf.nn.relu(errors_sorted), tf.stop_gradient(grad), 1, name=\"loss_non_void\")\n",
    "        return loss\n",
    "\n",
    "    # deal with the void prediction case (only void pixels)\n",
    "    loss = tf.cond(tf.equal(tf.shape(logits)[0], 0),\n",
    "                   lambda: tf.reduce_sum(logits) * 0.,\n",
    "                   compute_loss,\n",
    "                   strict=True,\n",
    "                   name=\"loss\"\n",
    "                   )\n",
    "    return loss\n",
    "\n",
    "\n",
    "def flatten_binary_scores(scores, labels, ignore=None):\n",
    "    \"\"\"\n",
    "    Flattens predictions in the batch (binary case)\n",
    "    Remove labels equal to 'ignore'\n",
    "    \"\"\"\n",
    "    scores = tf.reshape(scores, (-1,))\n",
    "    labels = tf.reshape(labels, (-1,))\n",
    "    if ignore is None:\n",
    "        return scores, labels\n",
    "    valid = tf.not_equal(labels, ignore)\n",
    "    vscores = tf.boolean_mask(scores, valid, name='valid_scores')\n",
    "    vlabels = tf.boolean_mask(labels, valid, name='valid_labels')\n",
    "    return vscores, vlabels\n",
    "\n",
    "\n",
    "# --------------------------- MULTICLASS LOSSES ---------------------------\n",
    "\n",
    "\n",
    "def lovasz_softmax(probas, labels, classes='present', per_image=False, ignore=None, order='BHWC'):\n",
    "    \"\"\"\n",
    "    Multi-class Lovasz-Softmax loss\n",
    "      probas: [B, H, W, C] or [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1)\n",
    "              Interpreted as binary (sigmoid) output with outputs of size [B, H, W].\n",
    "      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)\n",
    "      classes: 'all' for all, 'present' for classes present in labels, or a list of classes to average.\n",
    "      per_image: compute the loss per image instead of per batch\n",
    "      ignore: void class labels\n",
    "      order: use BHWC or BCHW\n",
    "    \"\"\"\n",
    "    if per_image:\n",
    "        def treat_image(prob_lab):\n",
    "            prob, lab = prob_lab\n",
    "            prob, lab = tf.expand_dims(prob, 0), tf.expand_dims(lab, 0)\n",
    "            prob, lab = flatten_probas(prob, lab, ignore, order)\n",
    "            return lovasz_softmax_flat(prob, lab, classes=classes)\n",
    "        losses = tf.map_fn(treat_image, (probas, labels), dtype=tf.float32)\n",
    "        loss = tf.reduce_mean(losses)\n",
    "    else:\n",
    "        loss = lovasz_softmax_flat(*flatten_probas(probas, labels, ignore, order), classes=classes)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def lovasz_softmax_flat(probas, labels, classes='present'):\n",
    "    \"\"\"\n",
    "    Multi-class Lovasz-Softmax loss\n",
    "      probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)\n",
    "      labels: [P] Tensor, ground truth labels (between 0 and C - 1)\n",
    "      classes: 'all' for all, 'present' for classes present in labels, or a list of classes to average.\n",
    "    \"\"\"\n",
    "    C = probas.shape[1]\n",
    "    losses = []\n",
    "    present = []\n",
    "    class_to_sum = list(range(C)) if classes in ['all', 'present'] else classes\n",
    "    for c in class_to_sum:\n",
    "        fg = tf.cast(tf.equal(labels, c), probas.dtype)  # foreground for class c\n",
    "        if classes == 'present':\n",
    "            present.append(tf.reduce_sum(fg) > 0)\n",
    "        if C == 1:\n",
    "            if len(classes) > 1:\n",
    "                raise ValueError('Sigmoid output possible only with 1 class')\n",
    "            class_pred = probas[:, 0]\n",
    "        else:\n",
    "            class_pred = probas[:, c]\n",
    "        errors = tf.abs(fg - class_pred)\n",
    "        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], name=\"descending_sort_{}\".format(c))\n",
    "        fg_sorted = tf.gather(fg, perm)\n",
    "        grad = lovasz_grad(fg_sorted)\n",
    "        losses.append(\n",
    "            tf.tensordot(errors_sorted, tf.stop_gradient(grad), 1, name=\"loss_class_{}\".format(c))\n",
    "                      )\n",
    "    if len(class_to_sum) == 1:  # short-circuit mean when only one class\n",
    "        return losses[0]\n",
    "    losses_tensor = tf.stack(losses)\n",
    "    if classes == 'present':\n",
    "        present = tf.stack(present)\n",
    "        losses_tensor = tf.boolean_mask(losses_tensor, present)\n",
    "    loss = tf.reduce_mean(losses_tensor)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def flatten_probas(probas, labels, ignore=None, order='BHWC'):\n",
    "    \"\"\"\n",
    "    Flattens predictions in the batch\n",
    "    \"\"\"\n",
    "    if len(probas.shape) == 3:\n",
    "        probas, order = tf.expand_dims(probas, 3), 'BHWC'\n",
    "    if order == 'BCHW':\n",
    "        probas = tf.transpose(probas, (0, 2, 3, 1), name=\"BCHW_to_BHWC\")\n",
    "        order = 'BHWC'\n",
    "    if order != 'BHWC':\n",
    "        raise NotImplementedError('Order {} unknown'.format(order))\n",
    "    C = probas.shape[3]\n",
    "    probas = tf.reshape(probas, (-1, C))\n",
    "    labels = tf.reshape(labels, (-1,))\n",
    "    if ignore is None:\n",
    "        return probas, labels\n",
    "    valid = tf.not_equal(labels, ignore)\n",
    "    vprobas = tf.boolean_mask(probas, valid, name='valid_probas')\n",
    "    vlabels = tf.boolean_mask(labels, valid, name='valid_labels')\n",
    "    return vprobas, vlabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss 577.3815307617188 Val: 484.9937438964844 P 0.7559957045698604 R 0.6149779040404041 F1 0.6782342868455424\n",
      "Epoch 2: Loss 436.594970703125 Val: 438.832275390625 P 0.9918597370068879 R 0.5336174242424242 F1 0.6939122413841022\n",
      "Epoch 3: Loss 425.5428771972656 Val: 436.32781982421875 P 0.6789541363051864 R 0.6668244949494949 F1 0.6728346527741786\n",
      "Epoch 4: Loss 429.542236328125 Val: 415.0594482421875 P 0.8225366740231079 R 0.6279987373737373 F1 0.7122225195904357\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4559a7fe85e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m                                                         \u001b[0mlength\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                                                         \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                                                         \u001b[0mis_training\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m                                                         })\n\u001b[1;32m     70\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.losses import binary_crossentropy\n",
    "from tensorflow.python.ops import array_ops\n",
    "\n",
    "def weighted_cross_entropy(y_true, y_pred):\n",
    "    y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "    y_pred = tf.log(y_pred / (1 - y_pred))\n",
    "    loss = tf.nn.weighted_cross_entropy_with_logits(logits=y_pred, targets=y_true, pos_weight=2.)\n",
    "\n",
    "    # or reduce_sum and/or axis=-1\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return 1. - score\n",
    "\n",
    "def smooth_jaccard(y_true, y_pred, smooth=100):\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return (1 - jac) * smooth\n",
    "\n",
    "def bce_jaccard(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, (-1, 14, 14, 1))\n",
    "    return weighted_cross_entropy(y_true, y_pred) + (0.5 * smooth_jaccard(y_true, y_pred))\n",
    "\n",
    "def bce_lovasz(y_true, y_pred):\n",
    "    return binary_crossentropy(tf.reshape(y_true, (-1, 14, 14, 1)), y_pred) + lovasz_softmax(y_pred, y_true, classes=[1], per_image=False)\n",
    "\n",
    "def foc_jaccard(y_true, y_pred):\n",
    "    y_true_r = tf.reshape(y_true, (-1, 14, 14, 1))\n",
    "    y_pred_r = tf.reshape(y_pred, (-1, 14, 14))\n",
    "    y_true_f = tf.reshape(y_true, (-1, 14*14, 1))\n",
    "    y_pred_f = tf.reshape(y_pred, (-1, 14*14, 1))\n",
    "    jac = smooth_jaccard(y_true_r, y_pred)\n",
    "    foc = focal_loss(y_pred_f, y_true_f, alpha=0.5, gamma=1.5)\n",
    "    return foc + tf.reduce_mean(jac)# + (0.5 * smooth_jaccard(y_true_r, y_pred))\n",
    "\n",
    "optimizer = AdaBoundOptimizer(learning_rate=5e-7, final_lr=5e-4, beta1=0.9, beta2=0.999, amsbound=False)\n",
    "loss2 = foc_jaccard(labels, fm)\n",
    "\n",
    "train_op = optimizer.minimize(loss2)\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "saver = tf.train.Saver(max_to_keep = 2)\n",
    "\n",
    "# dev --> 225\n",
    "\n",
    "# Run training loop\n",
    "for i in range(100):\n",
    "    randomize = np.arange(1088)\n",
    "    np.random.shuffle(randomize)\n",
    "    test_randomize = np.arange(1088, 1344)\n",
    "    np.random.shuffle(test_randomize)\n",
    "\n",
    "    losses = []\n",
    "    val_loss = []\n",
    "\n",
    "    for k in range(34):\n",
    "        batch_ids = randomize[k*32:(k+1)*32]\n",
    "        op, tr = sess.run([train_op, loss2], feed_dict={inp: data_x[batch_ids, :, :, :],\n",
    "                                                        length: lengths[batch_ids],\n",
    "                                                        labels: data_y[batch_ids, :, :].reshape(32, 14, 14),\n",
    "                                                        is_training: True\n",
    "                                                        })\n",
    "        losses.append(tr)\n",
    "    for j in range(8):\n",
    "        batch_ids = test_randomize[j*32:(j+1)*32]\n",
    "        vl, y = sess.run([loss2, fm], feed_dict={inp: data_x[batch_ids, :, :, :],\n",
    "                                                 length: lengths[batch_ids],\n",
    "                                                 labels: data_y[batch_ids, :, :].reshape(32, 14, 14),\n",
    "                                                 is_training: False,\n",
    "                                                })\n",
    "        val_loss.append(vl)\n",
    "        \n",
    "    recalls = []\n",
    "    precisions = []\n",
    "    for m in range(1088, 1344):\n",
    "        true = data_y[m].reshape((14, 14))\n",
    "        y = sess.run([fm], feed_dict={inp: data_x[m].reshape(1, 24, 14, 14, 13),\n",
    "                                  length: lengths[m].reshape(1, 1),\n",
    "                                  is_training: False,\n",
    "                                  })[0]\n",
    "        pred = y.reshape(14, 14)\n",
    "        #pred = np.sigmoid(pred)\n",
    "        pred[np.where(pred > 0.45)] = 1\n",
    "        pred[np.where(pred < 0.45)] = 0\n",
    "        rec, prec = thirty_meter(true, pred)\n",
    "        recalls.append(rec)\n",
    "        precisions.append(prec)\n",
    "    precisions = [item for sublist in precisions for item in sublist]\n",
    "    recalls = [item for sublist in recalls for item in sublist]\n",
    "    precision = np.mean(precisions)\n",
    "    recall = np.mean(recalls)\n",
    "    f1_score = 2 * ((precision * recall) / (precision + recall))\n",
    "    save_path = saver.save(sess, \"../models/correct_ndvi/model\")\n",
    "    print(\"Epoch {}: Loss {} Val: {} P {} R {} F1 {}\".format(i + 1,\n",
    "                                                             np.mean(losses), np.mean(val_loss),\n",
    "                                                             precision, recall, f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x13e356b38>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = int((len(data_x))*0.8) + 21# 105, 25, 215, 90, 140\n",
    "y = sess.run([fm], feed_dict={inp: data_x[idx].reshape(1, 24, 14, 14, 13),\n",
    "                              length: lengths[idx].reshape(1, 1),\n",
    "                              })\n",
    "\n",
    "sns.heatmap(data_y[idx][:, :, :].reshape(14, 14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x14211de48>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAAD8CAYAAABAWd66AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAHiJJREFUeJzt3XuUXXV99/H3ZyaZ3O8glyQSFBIFdHGJiFUpBVFsbWhrXUXsElpq+iylaq2P4sJFq1YrVenjaqmaKqj1woN4SxUBtYLWFkxQQEJAQgSSAOFqkASSzJzv88feyXMYc87e55zfzOyz+bxYe+Wcffnu38wcvvOb3/7t71ZEYGZmE29gohtgZmYZJ2Qzs4pwQjYzqwgnZDOzinBCNjOrCCdkM7OKcEI2M6sIJ2Qzs4pwQjYzq4hJY32Cly48OdmtgPMHp6cKldQRA7OSxXrlk41ksV5w1APJYk09bGqyWAB3fSddvF/umpEs1s4BJYv12GC6WMt27U4Wa/H8bclipbZ0/VU9f9N2P7yxdM6ZvN9z0v2QEnAP2cysIsa8h2xmNq4aIxPdgq45IZtZvYwMT3QLuuaEbGa1EpHuOsx4c0I2s3pp1DghS3oecDqwMF+1BVgdEevHsmFmZl3p4x5y21kWkt4NXAYI+Em+CPiypPPGvnlmZh1qjJRfKqaoh3wOcGREPG0SpKSLgHXAh/d1kKSVwEqA58xZxoEzDk7QVDOzEuraQwYawL6y6UH5tn2KiFURsTwiljsZm9l4ipHh0kvVFPWQ3w58X9KdwKZ83bOBw4Bzx7JhZmZdqetFvYi4StJS4HieflFvTURUbwDGzKyPhywKZ1lENqnv+nFoi5lZ7yp4sa4sz0M2s3qpcw/ZzKyvVPBiXVl9lZBHWk/s6NivR3Ymi/WrgWnJYj08MJQsVsqOwpO/eIrpR6Yrczlv/o5ksbben+77vyPh/xLPGk5WeZZF8x5PFuuJJ6Yki9WISlWvzNT1op7ZHimTsdlY6uf5Bk7IZlYvHkM2M6sID1mYmVWEe8hmZhUxku75g+PNCdnM6qWPhyy6fsippD9L2RAzsySiUX6pmF6eOv2+ZK0wM0ul0Si/VEzbIQtJt7TaBBzQ5jjXQzaziVHBRFtW0RjyAcCrgMdGrRfw360OiohVwCqAly48Od3tSmZmBaLGF/W+BcyMiJtGb5B07Zi0yMysFxUcGy6rqB7yOW22nZm+OWZmPerjIYteLuqZmVVPwlkWkk6TdIekDe0e7CzptZJC0vL8/WRJn5P0c0nrJb2nTNM9D9nM6iVRD1nSIHAxcCqwGVgjaXVE3DZqv1nA24Abmla/DpgSES+QNB24TdKXI+Ludud0D9nM6iVdD/l4YENEbIyIXcBlwOn72O8DwIXAU82tAGZImgRMA3YBhTVUx7yHPEi6eqkPD29PFmtHwnrIiyYdmCzW/o10V4hnvf7YZLG09AXJYgHMvPNzyWL9eutgslj3TE43KWjprmShOPDF6YJpKF15yu3rKzijYbh8gfrmKbq5VfksMcieI7qpadtm4MWjjj8WWBwR35b0v5s2XUGWvO8HpgN/HRGPFrXHQxZmVi8dzLJonqLbKUkDwEXA2fvYfDwwAhwMzAN+JOl7EbGxXUwnZDOrl3SzLLYAi5veL8rX7TELOAq4VhLAgcBqSSuAM4GrImI38KCkHwPLgbYJ2WPIZlYv6caQ1wCHSzpU0hBwBrB672kitkXEfhGxJCKWANcDKyJiLXAvcDKApBnACcDtRSd0QjazeklUyyIihoFzgauB9cDlEbFO0vvzXnA7FwMzJa0jS+yXRkSrUhR7ecjCzOol4Z16EXElcOWodRe02PekptdPkE1960hhD1nS8ySdImnmqPWndXoyM7MxNzxcfqmYtglZ0luBbwJ/BdwqqXkO3ofGsmFmZl2JKL9UTNGQxZuA4yLiCUlLgCskLYmIj0PrCcbNc/sOm7OMg2YsTNRcM7MCfVzLoighD+RjIUTE3ZJOIkvKh9AmITfP7Ttx4SnV+zVkZvXVxwm5aAx5q6Sj97zJk/NrgP2AtLdumZml0MePcCrqIb8ReNrIdz4V5I2SPjVmrTIz69ZIulvDx1tRPeTNbbb9OH1zzMx61MdDFp6HbGb14oRsZlYRFRwbLquvEvJBk2Yli3Uf6SZ/PKV0sY46amuyWAMvPjtZLIDBhc9LFmv6yT9KFmvBTenKsk6NoWSxnjd1W7JYk5ammzqq+XOSxZo+vD5ZrFSi0b8Tu/oqIdvESZmMzcaUhyzMzCqirrMszMz6jnvIZmYV4YRsZlYRFSwaVFZhQpZ0PBARsUbSEcBpwO15nVAzs2qpaw9Z0t8CrwYmSfou2RNXfwCcJ+mYiPjgOLTRzKy8Gk97+2PgaGAK8ACwKCIel/RR4AbACdnMqqWPZ1kUVXsbjoiRiNgB3BURjwNExJNAy78LJK2UtFbS2vu3b2m1m5lZctFolF6qpigh75I0PX993J6VkubQJiFHxKqIWB4Ry12c3szGVSPKLxVTNGRxYkTsBIh42g3ik4GzxqxVZmbdqmstiz3JeB/rHwYeHpMWmZn1ooI937I8D9nM6mW4fy/qOSGbWb3UdcjCzKzveMiitSca+xyG7sqCwenFO5U0VZOTxTruqWShmDy/5cO8Oxb3bUgWqzE0LVksAAYHk4V61owdyWIt3ZGuXYMz0vXUdq69J1msoaULksXacWfa4YEUlZqrOJ2tLPeQzaxe3EM2M6sIJ2Qzs4ro41unnZDNrFb8TD0zs6ro44RcVMviN0j6/Fg0xMwsiUaj/FIxRfWQV49eBfyOpLkAEbFirBpmZtaVPu4hFw1ZLAJuAz4NBFlCXg58rN1BklYCKwEWz34u+00/sPeWmpmV0ccJuWjIYjlwI3A+sC0irgWejIjrIuK6Vgc1l990Mjaz8RQjjdJL1RRVe2sA/yTpK/m/W4uOMTObUDXuIQMQEZsj4nXAd4AvjG2TzMy6F40ovRSRdJqkOyRtkHRem/1eKykkLW9a9578uDskvapM2zvq7UbEt4Fvd3KMmdm4StRDljQIXAycCmwG1khaHRG3jdpvFvA2sueM7ll3BHAGcCRwMPA9SUsjou1dKx1PezMzq7RGB0t7xwMbImJjROwCLgNO38d+HwAuBJrLjJ0OXBYROyPil8CGPF5bTshmVisx3Ci9FFgIbGp6vzlft5ekY4HF+ehBR8fuixOymdVLBz1kSSslrW1aVpY9jaQB4CLgb1I1fcxnTGza8VCyWLNnTU0W6+BJs5LFmjKc7qquBtLVQ+apdHWC4+HNyWKl9uD2dHWy75ucrh7yrCfStWvKncPJYi0YeixZrG0Ppq2TfVCCGJ3UsoiIVcCqFpu3AIub3i/K1+0xCzgKuFYSwIHAakkrShy7T+4hm1m9pBtDXgMcLulQSUNkF+n23r0cEdsiYr+IWBIRS4DrgRURsTbf7wxJUyQdChwO/KTohJ5TbGa1kqraW0QMSzoXuBoYBC6JiHWS3g+sjYjRpSWaj10n6XKyO52HgbcUzbAAJ2Qzq5uEN+BFxJXAlaPWXdBi35NGvf8g8MFOzueEbGa1EumG28ddRwlZ0svI5tLdGhHXjE2TzMy6F9UrUVFa24t6kn7S9PpNwL+QXVn823a3EZqZTZh0F/XGXVEPeXLT65XAqRHxkKSPkl1R/PCYtczMrAv93EMuSsgDkuaR9aQVEQ8BRMR2SS1HaprrIc+c+iymDs1N1V4zs7bqnJDnkNVDFhCSDoqI+yXNzNftU/Nk6/3nLOvfWnhm1ndiJOHNVeOsqB7ykhabGsAfJm+NmVmP6txD3qeI2AH8MnFbzMx6Fo2a9pDNzPrNM66HbGZWVRHuIZuZVYJ7yG3s2L0zWaxJSlcaMaWFU7cnizV52QHJYjV+/vNksTQ77SWD4Ts3Fe9U0v2D+yeLddekwvovpW0bnJIs1qMPL0gW67Abfp0s1hM7h5LFSqVR11kWZmb9xhf1zMwqwgnZzKwioo9vRXNCNrNacQ/ZzKwiajvtTdKLgfUR8bikacB5wLFkjyX5UERsG4c2mpmVNtLHsyyKHnJ6CbDn0cUfJys2dGG+7tIxbJeZWVciVHqpmsLymxF7H4iyPCKOzV//l6SbWh3UXH5zaPJ8Jk2a1XtLzcxK6Ocx5KIe8q2S/ix/fbOk5QCSlgK7Wx0UEasiYnlELHcyNrPxFFF+qZqiHvJfAB+X9F7gYeB/JG0CNuXbzMwqpZ97yEX1kLcBZ0uaDRya7785IraOR+PMzDo10ij6w7+6Sk17i4jHgZvHuC1mZj2r4lBEWZ6HbGa10qjg7ImynJDNrFaqOJ2tLCdkM6sVD1m0sXO45ey4jg20ftB1x2ZrcrJYC/Z/NFmsxoO7ksXSvBnJYm2/bnOyWACP3JuubQ8k/BRviieTxZpOwq9xcroLVfs9NTVZrIdJ9/9RKh6yMDOriNrPsjAz6xd9PGLhhGxm9eIhCzOzivAsCzOziujjh063Ly4k6a2SFo9XY8zMehWo9FI1RZcjPwDcIOlHkt4sKd3z1s3MxsBwqPRSNUUJeSOwiCwxHwfcJukqSWdJallXU9JKSWslrW00tidsrplZe3XuIUdENCLimog4BzgY+FfgNLJk3eqgvfWQBwbSTY43MyvS6GCpmqKE/LRfIRGxOyJWR8TrgUPGrllmZt1J2UOWdJqkOyRtkHTePrb/L0k/l3STpP+SdES+/lRJN+bbbpR0cpm2F82y+JNWGyJiR6ttZmYTJVXPV9IgcDFwKrAZWCNpdUTc1rTblyLik/n+K4CLyEYQHgZ+PyLuk3QUcDWwsOicRQXqf9HVV2JmNkFG0o0NHw9siIiNAJIuA04H9ibkvFb8HjPIbxSMiJ81rV8HTJM0JSJ2tjuh5yGbWa0kfILTQrLH1e2xGXjx6J0kvQV4BzAE7Gto4rXAT4uSMRSPIZuZ9ZUGKr00zwjLl5Wdni8iLo6I5wLvBt7bvE3SkcCFwF+WiTXmPeQpk9KV55uidM1N+Zvo7k3zksV6wXMeSRZrZFO6UpI/vG1RslgAt0xJF2vzwFPJYj06UtiJKe2JwWnJYi2MwWSxpgyMJIs1d6R6U8c6KS4UEauAVS02bwGab4xblK9r5TLgE3veSFoEfB14Y0TcVaY97iGbWa0knPa2Bjhc0qGShoAzgNXNO0g6vOnt7wF35uvnAt8GzouIH5dtu8eQzaxWGkrTa4+IYUnnks2QGAQuiYh1kt4PrI2I1cC5kl4B7AYeA87KDz8XOAy4QNIF+bpXRsSD7c7phGxmtZJuQAYi4krgylHrLmh6/bYWx/098Pedns8J2cxqJeEsi3HnhGxmtdKoYI2Kstom5KaB7Psi4nuSzgR+C1gPrIqIdE8wNTNLoM6PcLo032e6pLOAmcDXgFPI7mI5q82xZmbjrs5DFi+IiBdKmkQ2/+7giBiR9AXg5lYH5ZOrVwIMTZ7PpEktK3WamSVVxSpuZRXNQx7Ihy1mAdOBOfn6KUDLOz6ay286GZvZeBpR+aVqinrInwFuJ5uDdz7wFUkbgRPI7koxM6uUfu4hF1V7+ydJ/zd/fZ+kzwOvAP4tIn4yHg00M+tEbRMyZIm46fWvgCvGtEVmZj2o4KPySvM8ZDOrlVr3kM3M+knKW6fHmxOymdVKnech92z20PRksXY0diWL9ajS1Ze9ZyDd1L4Fa9N9v4amDCeLNbuRtt+xoJHuozdTU5PFmjWYrl3LhtN9xo4cSVfbesmyR5PF2rY13fc+FQ9ZmJlVhBOymVlF1LmWhZlZX/EYsplZRXiWhZlZRTT6eNCiMCFLeg7wR2RPXx0BfgF8KSIeH+O2mZl1rJ8v6rWt9ibprcAnganAi8iqvC0Grpd00pi3zsysQ9HBUjVFPeQ3AUfnNZAvAq6MiJMkfQr4JnDMvg5qroc8e9qBTB+al7LNZmYt9XMPucwY8iSyoYopZE8MISLuldS2HjKwCuCguUdU8ReRmdXUsPo35RQl5E8DayTdALwcuBBA0v5Autt9zMwS6d90XFwP+eOSvgc8H/hYRNyer38IOHEc2mdm1pFaD1lExDpg3Ti0xcysZ7We9mZm1k/6Nx07IZtZzdR6yKJKHhvZkSzWtoTlDD876alksa4dSVfK8+hfJyyNOATTEnY9Dt6dLthgpIt1SMKyrCcc9ECyWDMX7k4Wa/LB6T4XW+6akixWKiN93Efuq4RsEydlMjYbS+4hm5lVRLiHbGZWDe4hm5lVhKe9mZlVRP+mYydkM6uZ4T5OyU7IZlYr/XxRr6ge8hxJH5Z0u6RHJT0iaX2+bm6b41ZKWitp7Y5dj6VvtZlZC40Olqppm5CBy4HHgJMiYn5ELAB+J193eauDImJVRCyPiOWuhWxm4yk6+K9qihLykoi4MCL23nIUEQ9ExIXAIWPbNDOzzqXsIUs6TdIdkjZIOm8f298h6TZJt0j6vqRDRm2fLWmzpH8p0/aihHyPpHdJOqDpBAdIejewqcwJzMzG00hE6aUdSYPAxcCrgSOA10s6YtRuPwOWR8QLgSuAfxy1/QPAD8u2vSgh/wmwALguH0N+FLgWmA+8ruxJzMzGS4MovRQ4HtgQERsjYhdwGXB68w4R8YOI2FNk53pg0Z5tko4DDgCuKdv2tgk5Ih6LiHdHxPPyMeT5EfH8iHg38AdlT2JmNl46GUNunoCQLyubQi3k6SMBm/N1rZwDfAdA0gDwMeCdnbS9l2lv7wMu7eF4M7PkOpk90fz8z15I+lNgOfDb+ao3kz0UerOk0nHaJmRJt7TaRNYVNzOrlIS3Tm8BFje9X5SvexpJrwDOB347Inbmq18CvFzSm8keDj0k6YmI+I0Lg82KesgHAK8im+b2tDYA/11wLAD7T51TZrdSpg0MJYs1EulmIS4ZTFfDeD4tH+bdsdkJJ1oui3S1qAHmzU0Xb9fOdPc3TZ+xK1mslDWMYzhZKAbmzkgW65AXPpIsVioJp7OtAQ6XdChZIj4DOLN5B0nHAJ8CTouIB/e2IeINTfucTXbhr20yhuKE/C1gZkTcNHqDpGuLgpuZjbei2RNlRcSwpHOBq4FB4JKIWCfp/cDaiFgNfISsB/yVfGji3ohY0e05i546fU6bbWe22mZmNlFSVnuLiCuBK0etu6Dp9StKxPgs8Nky53MtCzOrlSreEl2WE7KZ1UoVb4kuywnZzGrFBerNzCoiEj6FfLw5IZtZrYz0cQ+5qJZFS5K+02bb3tsRH92xtdtTmJl1LGEti3FXdKfesa02AUe3Oq75dsQXHviS6n3VZlZbdR6yWANcR5aAR2v5xBAzs4lSxZ5vWUUJeT3wlxFx5+gNklwP2cwqp87T3v6O1uPMf5W2KWZmvUt16/REKLp1+oo2m/2wPDOrnH4esuh6lgVZPWQzs0qp8yyLnushbx/eWbxTSXMHpyeLNaDBZLG2Rboyi08lvBP/mJiWLNZGpjGQ8PM7+YmRZLG2705XsnRA6b7IaY+l+1wo4Td/+O7R1XS798jGdJ8xyJ4X16s6z7LouR6y1UPKZGw2lqrY8y3L9ZDNrFZqO8vC9ZDNrN+kfBrQeHMtCzOrlTqPIZuZ9ZU6jyGbmfWV2o4hm5n1m4aHLMzMqqGfe8ht79STNFvSP0j6d0lnjtr2r22O21sP+fGnHk7VVjOzQiPRKL1UTdGt05eS3QTyVeAMSV+VNCXfdkKrgyJiVUQsj4jls6ful6ipZmbFGhGll6opGrJ4bkS8Nn/9DUnnA/8pacUYt8vMrCv9PGRRlJCnSBqIyPr2EfFBSVuAHwIzx7x1ZmYdqmLPt6yiIYv/AE5uXhERnwX+Btg1Rm0yM+tadPBf1RTdOv2uFuuvkvShsWmSmVn3RiJdJcHx5nrIZlYrEVF6qZoxr4c8dTBdrdoFCeshP95IV6d5ck+/157u0XgqWaxbhqYmizUn0n2NADuHZyWLNSVhDeO1u9N9zw7dMCdZrJQe2ZjuZzmcLFJmaYIYdb512vWQzayvVLHnW5brIZtZrfTzLAvXQzazWqni7ImyXMvCzGqlirdEl+WEbGa1UucxZDOzvlLbMWQzs37Tzz3kovKbB0r6hKSLJS2Q9HeSfi7pckkHtTlub/nNx558MH2rzcxaaBCllyKSTpN0h6QNks7bx/YTJf1U0rCkPx617dmSrpG0XtJtkpYUna9ohvhngduATcAPgCeB3wV+BHyy1UHN5TfnTXtWURvMzJJJdaeepEHgYuDVwBHA6yUdMWq3e4GzgS/tI8TngY9ExPOB44HC3mnhjSER8c95494cERfm6/9ZUsspcWZmEyXhLIvjgQ0RsRFA0mXA6WSdVAAi4u5829NOmifuSRHx3Xy/J8qcsKiH3Lz986O2DZY5gZnZeEpYoH4h2ejAHpvzdWUsBX4l6WuSfibpI3mPu62ihPxNSTMBIuK9e1ZKOgy4o2TDzMzGTSdDFs3Xu/JlZaJmTAJeDrwTeBHwHLKhjcKDWoqIC1qs3yDp25230cxsbHVyp15ErAJWtdi8BVjc9H5Rvq6MzcBNTcMd3yB77N1n2h3k8ptmVisJy2+uAQ6XdKikIeAMYHXJZqwB5kraP39/Mk1jz62MeflNM7PxlOrGkIgYlnQucDXZNbNLImKdpPcDayNitaQXAV8H5gG/L+l9EXFkRIxIeifwfUkCbgT+rcxJ2/322AocDRwyalkC3NfJb6ISv6lWOlb/x6py2xyrHrHqvBQNWewpv3nPqOVu4NrCbN+ZVIPpjjWxsVLHcyzHesZw+U0zs4pI+1weMzPrWpUScqupJ47VX7FSx3Msx3rGUD7gbmZmE6xKPWQzs2e0CU/IReXtOox1iaQHJd2aoF2LJf0gL5u3TtLbeog1VdJPJN2cx+r5phpJg/k98t/qMc7deUnVmySt7THWXElXSLo9Lzn4ki7jLMvbs2d5XNLbe2jXX+ff91slfVnS1B5ivS2Ps66bNu3rMyppvqTvSroz/3deD7Fel7etIWl5j+36SP6zvEXS1yXN7SHWB/I4N+UlKQ8u27ZnlImcc0c22fousvu8h4CbgSN6iHcicCxwa4K2HQQcm7+eBfyi27aR3UgzM389GbgBOKHH9r2DrOTft3qMczewX6Kf5+eAv8hfDwFzE31GHgAO6fL4hcAvgWn5+8uBs7uMdRRwKzCdbIbS94DDOozxG59R4B+B8/LX5wEX9hDr+cAysmmpy3ts1yvJKpYBXNhju2Y3vX4r8MkUn7m6LRPdQ95b3i4idgF7ytt1JSJ+CDyaomERcX9E/DR//WtgPeUrPY2OFfH/y+9NzpeuB+8lLQJ+D/h0tzFSkzSH7H/EzwBExK6I+FWC0KcAd0XEPT3EmARMkzSJLJne12Wc5wM3RMSOiBgGrgP+qJMALT6jp5P9MiP/9w+6jRUR6yOi48JfLWJdk3+dANeT1XLoNtbjTW9n0MPnv84mOiH3Ut5u3OSV/o8h69l2G2NQ0k1kRaq/GxFdxwL+D/AuIEXh1wCukXRjj5WuDgUeAi7Nh1I+LWlGgvadAXy524MjYgvwUbJC4vcD2yLimi7D3Qq8XNnTc6aTPaxhccExZRwQEffnrx+gmmUJ/hz4Ti8BJH1Q0ibgDcA+C5c90010Qq68vPzoV4G3j/ot35GIGImIo8l6GcdLOqrL9rwGeDAibuy2LaO8LCKOJXsqwlskndhlnElkf6Z+IiKOAbaT/fndtbygywrgKz3EmEfWAz0UOBiYIelPu4kVEevJ/nS/BrgKuAkY6bZtLc4RVKz3KOl8YBj4Yi9xIuL8iFicxzk3RdvqZqITci/l7cacpMlkyfiLEfG1FDHzP+N/AJzWZYiXAisk3U02xHOypC/00J4t+b8PkhVJOb7LUJuBzU09/yvIEnQvXg38NCK29hDjFcAvI+KhiNgNfA34rW6DRcRnIuK4iDgReIzs2kKvtip/RmX+b2UeRCnpbOA1wBvyXxYpfBF4baJYtTLRCbmX8nZjKq/Q9BlgfURc1GOs/fdcoZY0DTgVuL2bWBHxnohYFBFLyL5f/xkRXfX4JM2QNGvPa7KLOF3NUImIB4BNkpblq06hRLnBAq+nh+GK3L3ACZKm5z/TU8iuB3RF0rPyf59NNn68r2epdWo1cFb++izgmwli9kzSaWRDYysiYkePsQ5vens6XX7+a2+iryqSjcP9gmy2xfk9xvoy2TjhbrIe2zk9xHoZ2Z+Ot5D9aXoT8Ltdxnoh8LM81q3ABYm+dyfRwywLstktN+fLugTf/6OBtfnX+Q1gXg+xZgCPAHMSfJ/eR5YAbgX+HZjSQ6wfkf2iuRk4pYvjf+MzCiwAvg/cSTZzY34Psf4wf72TrFrj1T3E2kB2jWfP57/UzIgWsb6af/9vAf4DWNjrz7WOi+/UMzOriIkesjAzs5wTsplZRTghm5lVhBOymVlFOCGbmVWEE7KZWUU4IZuZVYQTsplZRfw/ykwciQWzWpsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = y[0][:, :, :].reshape(14, 14)\n",
    "#pred[np.where(pred > 0.5)] = 1\n",
    "sns.heatmap(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recalls = []\n",
    "precisions = []\n",
    "for i in range(1088, 1344):\n",
    "    true = data_y[i].reshape((14, 14))\n",
    "    y = sess.run([fm], feed_dict={inp: data_x[i].reshape(1, 24, 14, 14, 13),\n",
    "                              length: lengths[i].reshape(1, 1),\n",
    "                              is_training: False,\n",
    "                              })[0]\n",
    "    pred = y.reshape(14, 14)\n",
    "    pred[np.where(pred > 0.2)] = 1\n",
    "    pred[np.where(pred < 0.2)] = 0\n",
    "    rec, prec = thirty_meter(true, pred)\n",
    "    #rec, prec = half_hectare_accuracy(true, pred)\n",
    "    recalls.append(rec)\n",
    "    precisions.append(prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9805082070707071"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recalls = [item for sublist in recalls for item in sublist]\n",
    "np.mean(recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15727938438624797"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precisions = [item for sublist in precisions for item in sublist]\n",
    "np.mean(precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"AdaBound for TensorFlow.\"\"\"\n",
    "\n",
    "from tensorflow.python.eager import context\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import resource_variable_ops\n",
    "from tensorflow.python.ops import state_ops\n",
    "from tensorflow.python.ops import variable_scope\n",
    "from tensorflow.python.training import optimizer\n",
    "from tensorflow.python.ops.clip_ops import clip_by_value\n",
    "\n",
    "\"\"\"Implements AdaBound algorithm.\n",
    "    It has been proposed in `Adaptive Gradient Methods with Dynamic Bound of Learning Rate`_.\n",
    "    Arguments:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float, optional): Adam learning rate (default: 1e-3)\n",
    "        betas (Tuple[float, float], optional): coefficients used for computing\n",
    "            running averages of gradient and its square (default: (0.9, 0.999))\n",
    "        final_lr (float, optional): final (SGD) learning rate (default: 0.1)\n",
    "        gamma (float, optional): convergence speed of the bound functions (default: 1e-3)\n",
    "        eps (float, optional): term added to the denominator to improve\n",
    "            numerical stability (default: 1e-8)\n",
    "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
    "        amsbound (boolean, optional): whether to use the AMSBound variant of this algorithm\n",
    "    .. Adaptive Gradient Methods with Dynamic Bound of Learning Rate:\n",
    "        https://openreview.net/forum?id=Bkg3g2R9FX\n",
    "    \"\"\"\n",
    "\n",
    "class AdaBoundOptimizer(optimizer.Optimizer):\n",
    "    def __init__(self, learning_rate=0.001, final_lr=0.1, beta1=0.9, beta2=0.999,\n",
    "                 gamma=1e-3, epsilon=1e-8, amsbound=False,\n",
    "                 use_locking=False, name=\"AdaBound\"):\n",
    "        super(AdaBoundOptimizer, self).__init__(use_locking, name)\n",
    "        self._lr = learning_rate\n",
    "        self._final_lr = final_lr\n",
    "        self._beta1 = beta1\n",
    "        self._beta2 = beta2\n",
    "        self._epsilon = epsilon\n",
    "\n",
    "        self._gamma = gamma\n",
    "        self._amsbound = amsbound\n",
    "\n",
    "        self._lr_t = None\n",
    "        self._beta1_t = None\n",
    "        self._beta2_t = None\n",
    "        self._epsilon_t = None\n",
    "\n",
    "    def _create_slots(self, var_list):\n",
    "        first_var = min(var_list, key=lambda x: x.name)\n",
    "\n",
    "        graph = None if context.executing_eagerly() else ops.get_default_graph()\n",
    "        create_new = self._get_non_slot_variable(\"beta1_power\", graph) is None\n",
    "        if not create_new and context.in_graph_mode():\n",
    "            create_new = (self._get_non_slot_variable(\"beta1_power\", graph).graph is not first_var.graph)\n",
    "\n",
    "        if create_new:\n",
    "            self._create_non_slot_variable(initial_value=self._beta1,\n",
    "                                           name=\"beta1_power\",\n",
    "                                           colocate_with=first_var)\n",
    "            self._create_non_slot_variable(initial_value=self._beta2,\n",
    "                                           name=\"beta2_power\",\n",
    "                                           colocate_with=first_var)\n",
    "            self._create_non_slot_variable(initial_value=self._gamma,\n",
    "                                           name=\"gamma_multi\",\n",
    "                                           colocate_with=first_var)\n",
    "        # Create slots for the first and second moments.\n",
    "        for v in var_list :\n",
    "            self._zeros_slot(v, \"m\", self._name)\n",
    "            self._zeros_slot(v, \"v\", self._name)\n",
    "            self._zeros_slot(v, \"vhat\", self._name)\n",
    "\n",
    "\n",
    "    def _prepare(self):\n",
    "        self._lr_t = ops.convert_to_tensor(self._lr)\n",
    "        self._base_lr_t = ops.convert_to_tensor(self._lr)\n",
    "        self._beta1_t = ops.convert_to_tensor(self._beta1)\n",
    "        self._beta2_t = ops.convert_to_tensor(self._beta2)\n",
    "        self._epsilon_t = ops.convert_to_tensor(self._epsilon)\n",
    "        self._gamma_t = ops.convert_to_tensor(self._gamma)\n",
    "\n",
    "    def _apply_dense(self, grad, var):\n",
    "        graph = None if context.executing_eagerly() else ops.get_default_graph()\n",
    "        beta1_power = math_ops.cast(self._get_non_slot_variable(\"beta1_power\", graph=graph), var.dtype.base_dtype)\n",
    "        beta2_power = math_ops.cast(self._get_non_slot_variable(\"beta2_power\", graph=graph), var.dtype.base_dtype)\n",
    "        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n",
    "        base_lr_t = math_ops.cast(self._base_lr_t, var.dtype.base_dtype)\n",
    "        beta1_t = math_ops.cast(self._beta1_t, var.dtype.base_dtype)\n",
    "        beta2_t = math_ops.cast(self._beta2_t, var.dtype.base_dtype)\n",
    "        epsilon_t = math_ops.cast(self._epsilon_t, var.dtype.base_dtype)\n",
    "        gamma_multi = math_ops.cast(self._get_non_slot_variable(\"gamma_multi\", graph=graph), var.dtype.base_dtype)\n",
    "\n",
    "        step_size = (lr_t * math_ops.sqrt(1 - beta2_power) / (1 - beta1_power))\n",
    "        final_lr = self._final_lr * lr_t / base_lr_t\n",
    "        lower_bound = final_lr * (1. - 1. / (gamma_multi + 1.))\n",
    "        upper_bound = final_lr * (1. + 1. / (gamma_multi))\n",
    "\n",
    "        # m_t = beta1 * m + (1 - beta1) * g_t\n",
    "        m = self.get_slot(var, \"m\")\n",
    "        m_scaled_g_values = grad * (1 - beta1_t)\n",
    "        m_t = state_ops.assign(m, beta1_t * m + m_scaled_g_values, use_locking=self._use_locking)\n",
    "\n",
    "        # v_t = beta2 * v + (1 - beta2) * (g_t * g_t)\n",
    "        v = self.get_slot(var, \"v\")\n",
    "        v_scaled_g_values = (grad * grad) * (1 - beta2_t)\n",
    "        v_t = state_ops.assign(v, beta2_t * v + v_scaled_g_values, use_locking=self._use_locking)\n",
    "\n",
    "        # amsgrad\n",
    "        vhat = self.get_slot(var, \"vhat\")\n",
    "        if self._amsbound :\n",
    "            vhat_t = state_ops.assign(vhat, math_ops.maximum(v_t, vhat))\n",
    "            v_sqrt = math_ops.sqrt(vhat_t)\n",
    "        else :\n",
    "            vhat_t = state_ops.assign(vhat, vhat)\n",
    "            v_sqrt = math_ops.sqrt(v_t)\n",
    "\n",
    "\n",
    "        # Compute the bounds\n",
    "        step_size_bound = step_size / (v_sqrt + epsilon_t)\n",
    "        bounded_lr = m_t * clip_by_value(step_size_bound, lower_bound, upper_bound)\n",
    "\n",
    "        var_update = state_ops.assign_sub(var, bounded_lr, use_locking=self._use_locking)\n",
    "        return control_flow_ops.group(*[var_update, m_t, v_t, vhat_t])\n",
    "\n",
    "    def _resource_apply_dense(self, grad, var):\n",
    "        graph = None if context.executing_eagerly() else ops.get_default_graph()\n",
    "        beta1_power = math_ops.cast(self._get_non_slot_variable(\"beta1_power\", graph=graph), grad.dtype.base_dtype)\n",
    "        beta2_power = math_ops.cast(self._get_non_slot_variable(\"beta2_power\", graph=graph), grad.dtype.base_dtype)\n",
    "        lr_t = math_ops.cast(self._lr_t, grad.dtype.base_dtype)\n",
    "        base_lr_t = math_ops.cast(self._base_lr_t, var.dtype.base_dtype)\n",
    "        beta1_t = math_ops.cast(self._beta1_t, grad.dtype.base_dtype)\n",
    "        beta2_t = math_ops.cast(self._beta2_t, grad.dtype.base_dtype)\n",
    "        epsilon_t = math_ops.cast(self._epsilon_t, grad.dtype.base_dtype)\n",
    "        gamma_multi = math_ops.cast(self._get_non_slot_variable(\"gamma_multi\", graph=graph), var.dtype.base_dtype)\n",
    "\n",
    "        step_size = (lr_t * math_ops.sqrt(1 - beta2_power) / (1 - beta1_power))\n",
    "        final_lr = self._final_lr * lr_t / base_lr_t\n",
    "        lower_bound = final_lr * (1. - 1. / (gamma_multi + 1.))\n",
    "        upper_bound = final_lr * (1. + 1. / (gamma_multi))\n",
    "\n",
    "        # m_t = beta1 * m + (1 - beta1) * g_t\n",
    "        m = self.get_slot(var, \"m\")\n",
    "        m_scaled_g_values = grad * (1 - beta1_t)\n",
    "        m_t = state_ops.assign(m, beta1_t * m + m_scaled_g_values, use_locking=self._use_locking)\n",
    "\n",
    "        # v_t = beta2 * v + (1 - beta2) * (g_t * g_t)\n",
    "        v = self.get_slot(var, \"v\")\n",
    "        v_scaled_g_values = (grad * grad) * (1 - beta2_t)\n",
    "        v_t = state_ops.assign(v, beta2_t * v + v_scaled_g_values, use_locking=self._use_locking)\n",
    "\n",
    "        # amsgrad\n",
    "        vhat = self.get_slot(var, \"vhat\")\n",
    "        if self._amsbound:\n",
    "            vhat_t = state_ops.assign(vhat, math_ops.maximum(v_t, vhat))\n",
    "            v_sqrt = math_ops.sqrt(vhat_t)\n",
    "        else:\n",
    "            vhat_t = state_ops.assign(vhat, vhat)\n",
    "            v_sqrt = math_ops.sqrt(v_t)\n",
    "\n",
    "        # Compute the bounds\n",
    "        step_size_bound = step_size / (v_sqrt + epsilon_t)\n",
    "        bounded_lr = m_t * clip_by_value(step_size_bound, lower_bound, upper_bound)\n",
    "\n",
    "        var_update = state_ops.assign_sub(var, bounded_lr, use_locking=self._use_locking)\n",
    "\n",
    "        return control_flow_ops.group(*[var_update, m_t, v_t, vhat_t])\n",
    "\n",
    "    def _apply_sparse_shared(self, grad, var, indices, scatter_add):\n",
    "        graph = None if context.executing_eagerly() else ops.get_default_graph()\n",
    "        beta1_power = math_ops.cast(self._get_non_slot_variable(\"beta1_power\", graph=graph), var.dtype.base_dtype)\n",
    "        beta2_power = math_ops.cast(self._get_non_slot_variable(\"beta2_power\", graph=graph), var.dtype.base_dtype)\n",
    "        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n",
    "        base_lr_t = math_ops.cast(self._base_lr_t, var.dtype.base_dtype)\n",
    "        beta1_t = math_ops.cast(self._beta1_t, var.dtype.base_dtype)\n",
    "        beta2_t = math_ops.cast(self._beta2_t, var.dtype.base_dtype)\n",
    "        epsilon_t = math_ops.cast(self._epsilon_t, var.dtype.base_dtype)\n",
    "        gamma_t = math_ops.cast(self._gamma_t, var.dtype.base_dtype)\n",
    "\n",
    "        step_size = (lr_t * math_ops.sqrt(1 - beta2_power) / (1 - beta1_power))\n",
    "        final_lr = self._final_lr * lr_t / base_lr_t\n",
    "        lower_bound = final_lr * (1. - 1. / (gamma_t + 1.))\n",
    "        upper_bound = final_lr * (1. + 1. / (gamma_t))\n",
    "\n",
    "        # m_t = beta1 * m + (1 - beta1) * g_t\n",
    "        m = self.get_slot(var, \"m\")\n",
    "        m_scaled_g_values = grad * (1 - beta1_t)\n",
    "        m_t = state_ops.assign(m, m * beta1_t, use_locking=self._use_locking)\n",
    "        with ops.control_dependencies([m_t]):\n",
    "            m_t = scatter_add(m, indices, m_scaled_g_values)\n",
    "\n",
    "        # v_t = beta2 * v + (1 - beta2) * (g_t * g_t)\n",
    "        v = self.get_slot(var, \"v\")\n",
    "        v_scaled_g_values = (grad * grad) * (1 - beta2_t)\n",
    "        v_t = state_ops.assign(v, v * beta2_t, use_locking=self._use_locking)\n",
    "        with ops.control_dependencies([v_t]):\n",
    "            v_t = scatter_add(v, indices, v_scaled_g_values)\n",
    "\n",
    "        # amsgrad\n",
    "        vhat = self.get_slot(var, \"vhat\")\n",
    "        if self._amsbound:\n",
    "            vhat_t = state_ops.assign(vhat, math_ops.maximum(v_t, vhat))\n",
    "            v_sqrt = math_ops.sqrt(vhat_t)\n",
    "        else:\n",
    "            vhat_t = state_ops.assign(vhat, vhat)\n",
    "            v_sqrt = math_ops.sqrt(v_t)\n",
    "\n",
    "        # Compute the bounds\n",
    "        step_size_bound = step_size / (v_sqrt + epsilon_t)\n",
    "        bounded_lr = m_t * clip_by_value(step_size_bound, lower_bound, upper_bound)\n",
    "\n",
    "        var_update = state_ops.assign_sub(var, bounded_lr, use_locking=self._use_locking)\n",
    "\n",
    "        return control_flow_ops.group(*[var_update, m_t, v_t, vhat_t])\n",
    "\n",
    "    def _apply_sparse(self, grad, var):\n",
    "        return self._apply_sparse_shared(\n",
    "            grad.values, var, grad.indices,\n",
    "            lambda x, i, v: state_ops.scatter_add(  # pylint: disable=g-long-lambda\n",
    "                x, i, v, use_locking=self._use_locking))\n",
    "\n",
    "    def _resource_scatter_add(self, x, i, v):\n",
    "        with ops.control_dependencies(\n",
    "                [resource_variable_ops.resource_scatter_add(x, i, v)]):\n",
    "            return x.value()\n",
    "\n",
    "    def _resource_apply_sparse(self, grad, var, indices):\n",
    "        return self._apply_sparse_shared(\n",
    "            grad, var, indices, self._resource_scatter_add)\n",
    "\n",
    "    def _finish(self, update_ops, name_scope):\n",
    "        # Update the power accumulators.\n",
    "        with ops.control_dependencies(update_ops):\n",
    "            graph = None if context.executing_eagerly() else ops.get_default_graph()\n",
    "            beta1_power = self._get_non_slot_variable(\"beta1_power\", graph=graph)\n",
    "            beta2_power = self._get_non_slot_variable(\"beta2_power\", graph=graph)\n",
    "            gamma_multi = self._get_non_slot_variable(\"gamma_multi\", graph=graph)\n",
    "            with ops.colocate_with(beta1_power):\n",
    "                update_beta1 = beta1_power.assign(\n",
    "                    beta1_power * self._beta1_t,\n",
    "                    use_locking=self._use_locking)\n",
    "                update_beta2 = beta2_power.assign(\n",
    "                    beta2_power * self._beta2_t,\n",
    "                    use_locking=self._use_locking)\n",
    "                update_gamma = gamma_multi.assign(\n",
    "                    gamma_multi + self._gamma_t,\n",
    "                    use_locking=self._use_locking)\n",
    "        return control_flow_ops.group(*update_ops + [update_beta1, update_beta2, update_gamma],\n",
    "                                      name=name_scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
