{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data download pipeline\n",
    "\n",
    "Downloads 16x16 training data plots from Sentinel Hub, with the following steps:\n",
    "\n",
    "*  Convert coordinates to UTM, identify bounding boxes of 160 and 180 meter borders\n",
    "*  Download all L1C steps, correct missing bands, and calculate cloud cover\n",
    "*  Select L2A imagery corresponding to the best imagery per 15 days, with missing imagery calculated as the weighted average of the nearest time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from osgeo import ogr, osr\n",
    "from sentinelhub import WmsRequest, WcsRequest, MimeType, CRS, BBox, constants, DataSource, CustomUrlParam\n",
    "from s2cloudless import S2PixelCloudDetector, CloudMaskRequest\n",
    "import logging\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "with open(\"../config.yaml\", 'r') as stream:\n",
    "        key = (yaml.safe_load(stream))\n",
    "        API_KEY = key['key'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LOCATION = '../data/kenya-test.csv'\n",
    "OUTPUT_FOLDER = '../data/test-data-nov-27/'\n",
    "EPSG = CRS.WGS84\n",
    "existing = [int(x[:-4]) for x in os.listdir(OUTPUT_FOLDER) if \".DS\" not in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../src/slope.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup function to reproject coordinates\n",
    "def convertCoords(xy, src='', targ=''):\n",
    "\n",
    "    srcproj = osr.SpatialReference()\n",
    "    srcproj.ImportFromEPSG(src)\n",
    "    targproj = osr.SpatialReference()\n",
    "    if isinstance(targ, str):\n",
    "        targproj.ImportFromProj4(targ)\n",
    "    else:\n",
    "        targproj.ImportFromEPSG(targ)\n",
    "    transform = osr.CoordinateTransformation(srcproj, targproj)\n",
    "\n",
    "    pt = ogr.Geometry(ogr.wkbPoint)\n",
    "    pt.AddPoint(xy[0], xy[1])\n",
    "    pt.Transform(transform)\n",
    "    return([pt.GetX(), pt.GetY()])\n",
    "\n",
    "def bounding_box(points, expansion = 160):\n",
    "    # LONG, LAT FOR SOME REASON\n",
    "    bl = list(points[0])\n",
    "    tr = list(points[1])\n",
    "    \n",
    "    if 48 <= bl[0] <= 54:\n",
    "        epsg = 32639 if bl[1] > 0 else 32739\n",
    "    if 42 <= bl[0] <= 48:\n",
    "        epsg = 32638 if bl[1] > 0 else 32738\n",
    "    if 36 <= bl[0] <= 42:\n",
    "        epsg = 32637 if bl[1] > 0 else 32737\n",
    "    if 30 <= bl[0] <= 36:\n",
    "        epsg = 32636 if bl[1] > 0 else 32736\n",
    "    if 24 <= bl[0] <= 30:\n",
    "        epsg = 32635 if bl[1] > 0 else 32735\n",
    "    if 18 <= bl[0] <= 24:\n",
    "        epsg = 32634 if bl[1] > 0 else 32734\n",
    "\n",
    "    bl = convertCoords(bl, 4326, epsg)\n",
    "    tr = convertCoords(tr, 4326, epsg)\n",
    "    init = [b - a for a,b in zip(bl, tr)]\n",
    "    distance1 = tr[0] - bl[0]\n",
    "    distance2 = tr[1] - bl[1]\n",
    "    EXPANSION = (expansion - np.mean([distance1, distance2]))/2 # should this be 155 or 160?\n",
    "    \n",
    "    bl = [a - EXPANSION for a in bl]\n",
    "    tr = [a + EXPANSION for a in tr]\n",
    "    \n",
    "    after = [b - a for a,b in zip(bl, tr)]    \n",
    "    if max(init) > 130:\n",
    "        print(\"ERROR: Initial field greater than 130m\")\n",
    "    if min(init) < 120:\n",
    "        print(\"ERROR: Initial field less than 130m\")\n",
    "        \n",
    "    if min(after) < (expansion - 4.5):\n",
    "        print(\"ERROR\")\n",
    "    if max(after) > (expansion + 5):\n",
    "        print(\"ERROR\")\n",
    "    diffs = [b - a for b, a in zip(after, init)]\n",
    "\n",
    "    bl = convertCoords(bl, epsg, 4326)\n",
    "    tr = convertCoords(tr, epsg, 4326)\n",
    "    return bl, tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "location = calc_bbox(val)\n",
    "location = bounding_box(location)\n",
    "box = BBox(location, crs = EPSG)\n",
    "\n",
    "image_request = WcsRequest(\n",
    "                layer='ALL_BANDS_NDVI',\n",
    "                bbox=box,\n",
    "                time = ('2018-01-01', '2018-12-31'),\n",
    "                image_format = MimeType.TIFF_d32f,\n",
    "                maxcc=1,\n",
    "                instance_id=API_KEY,\n",
    "                custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'BICUBIC'},\n",
    "                time_difference=datetime.timedelta(hours=24),\n",
    "            )\n",
    "img_bands = image_request.get_data()\n",
    "img_bands[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bbox(plot_id):\n",
    "    subs = df[df['PLOT_ID'] == plot_id]\n",
    "    # TOP, LEFT, BOTTOM, RIGHT\n",
    "    # (min x, min y), (max x, max y)\n",
    "    return [(min(subs['LON']), min(subs['LAT'])),\n",
    "            (max(subs['LON']), max(subs['LAT']))]\n",
    "\n",
    "\n",
    "df = pd.read_csv(DATA_LOCATION)\n",
    "df = df.drop('IMAGERY_TITLE', axis = 1)\n",
    "df = df.dropna(axis = 0)\n",
    "plot_ids = sorted(df['PLOT_ID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([35.5686634922896, 4.310535388619154],\n",
       " [35.570108785525406, 4.311976573448053])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounding_box(calc_bbox(plot_ids[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2016-12-15', '2018-1-15')\n"
     ]
    }
   ],
   "source": [
    "cloud_detector = S2PixelCloudDetector(threshold=0.4, average_over=4, dilation_size=2)\n",
    "\n",
    "\n",
    "year = 2018\n",
    "time = (str(year - 1) +'-12-15', str(year+1) +'-1-15')\n",
    "print(time)\n",
    "\n",
    "def calculate_proximal_steps(uniques, date, clean_steps):\n",
    "    arg_before = None\n",
    "    arg_after = None\n",
    "    uniques = np.array(uniques)\n",
    "    satisfactory = np.argwhere(uniques > 2)\n",
    "    satisfactory = np.array([x for x in satisfactory if x in clean_steps])\n",
    "    if date > 0:\n",
    "        idx_before = satisfactory - date\n",
    "        arg_before = idx_before[np.where(idx_before < 0, idx_before, -np.inf).argmax()]\n",
    "    if date < np.max(satisfactory):\n",
    "        idx_after = satisfactory - date\n",
    "        arg_after = idx_after[np.where(idx_after > 0, idx_after, np.inf).argmin()]\n",
    "    if not arg_after and not arg_before:\n",
    "        arg_after = date\n",
    "        arg_before = date\n",
    "    if not arg_after:\n",
    "        arg_after = arg_before\n",
    "    if not arg_before:\n",
    "        arg_before = arg_after\n",
    "    print(arg_before, date, arg_after)\n",
    "    return arg_before, arg_after\n",
    "\n",
    "\n",
    "def identify_clouds(bbox, epsg = EPSG, time = time):\n",
    "    try:\n",
    "        box = BBox(bbox, crs = epsg)\n",
    "        cloud_request = WmsRequest(\n",
    "            layer='CLOUD_DETECTION',\n",
    "            bbox=box,\n",
    "            time=time,\n",
    "            width=16,\n",
    "            height=16,\n",
    "            image_format = MimeType.TIFF_d32f,\n",
    "            maxcc=0.33,\n",
    "            instance_id=API_KEY,\n",
    "            custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'BICUBIC'},\n",
    "            time_difference=datetime.timedelta(hours=24),\n",
    "        )\n",
    "        \n",
    "        cloud_img = cloud_request.get_data()\n",
    "        cloud_probs = cloud_detector.get_cloud_probability_maps(np.array(cloud_img))\n",
    "        means = np.mean(cloud_probs, (1, 2))\n",
    "        cloud_steps = [i for i, val in enumerate(means) if val > 0.25]\n",
    "        return cloud_steps, means, cloud_probs\n",
    "    except Exception as e:\n",
    "        logging.fatal(e, exc_info=True)\n",
    "    \n",
    "    \n",
    "def download_dem(val, epsg = EPSG):\n",
    "    location = calc_bbox(val)\n",
    "    bbox = bounding_box(location, expansion = 180)\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    dem_request = WmsRequest(data_source=DataSource.DEM,\n",
    "                         layer='DEM',\n",
    "                         bbox=box,\n",
    "                         width=18,\n",
    "                         height=18,\n",
    "                         instance_id=API_KEY,\n",
    "                         image_format=MimeType.TIFF_d32f,\n",
    "                         custom_url_params={CustomUrlParam.SHOWLOGO: False})\n",
    "    dem_image = dem_request.get_data()[0]\n",
    "    dem_image = calcSlope(dem_image.reshape((1, 18, 18)),\n",
    "                  np.full((18, 18), 10), np.full((18, 18), 10), zScale = 1, minSlope = 0.02)\n",
    "    dem_image = dem_image.reshape((18, 18, 1))\n",
    "    dem_image = dem_image[1:17, 1:17, :]\n",
    "    return dem_image\n",
    "\n",
    "        \n",
    "    \n",
    "def download_tiles(bbox, clean_steps, epsg = EPSG, time = time):\n",
    "    try:\n",
    "        clean_steps = np.argwhere(clean_steps <= 0.2)\n",
    "        box = BBox(bbox, crs = epsg)\n",
    "        image_request = WmsRequest(\n",
    "                layer='ALL_BANDS_NDVI',\n",
    "                bbox=box,\n",
    "                time=time,\n",
    "                width=16,\n",
    "                height=16,\n",
    "                image_format = MimeType.TIFF_d32f,\n",
    "                maxcc=0.33,\n",
    "                instance_id=API_KEY,\n",
    "                custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'BICUBIC'},\n",
    "                time_difference=datetime.timedelta(hours=24),\n",
    "            )\n",
    "        img_bands = image_request.get_data()\n",
    "        img_bands = np.array(img_bands)\n",
    "        print(\"There are {}/{} clean steps\".format(len(clean_steps), len(img_bands)))\n",
    "        num_broken_steps = 0\n",
    "        for date in range(img_bands.shape[0]):\n",
    "            if date in clean_steps:\n",
    "                for band in range(10):\n",
    "                    uniques = [len(np.unique(img_bands[i, :, :, band])) for i in range(img_bands.shape[0])]\n",
    "                    maxs = np.max(img_bands[date, :, :, band])\n",
    "                    mins = np.min(img_bands[date, :, :, band])\n",
    "                    if maxs >= 1.0 or mins <= 0.0:\n",
    "                        num_broken_steps += 1\n",
    "                        before, after = calculate_proximal_steps(uniques, date, clean_steps)\n",
    "                        before = img_bands[date + int(before), :, :, band]\n",
    "                        after = img_bands[date + int(after), :, :, band]\n",
    "                        img_bands[date, :, :, band] = (before + after) / 2\n",
    "                    if len(np.unique(img_bands[date, :, :, band])) <= 3:\n",
    "                        num_broken_steps += 1\n",
    "                        before, after = calculate_proximal_steps(uniques, date, clean_steps)\n",
    "                        before = img_bands[date + int(before), :, :, band]\n",
    "                        after = img_bands[date + int(after), :, :, band]\n",
    "                        img_bands[date, :, :, band] = (before + after) / 2\n",
    "        print(\"{} broken normal steps\".format(num_broken_steps))\n",
    "        return img_bands, image_request\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.fatal(e, exc_info=True)\n",
    "    \n",
    "\n",
    "        \n",
    "def calculate_and_save_best_images(cloud_steps, img_bands, image_request, means, year = year):\n",
    "    # Identify the date of the imagery\n",
    "    image_dates = []\n",
    "    for date in image_request.get_dates():\n",
    "        if date.year == year - 1:\n",
    "            image_dates.append(-360 + (date.month-1)*30 + date.day)\n",
    "        if date.year == year:\n",
    "            image_dates.append((date.month-1)*30 + date.day)\n",
    "        if date.year == year + 1:\n",
    "            image_dates.append(365 + (date.month-1)*30+date.day)\n",
    "        #image_dates.append((date.year - 2018)*395 + date.month*30 + date.day)\n",
    "    print(image_dates)\n",
    "\n",
    "    biweekly_dates = [day for day in range(0, 360, 15)] # ideal imagery dates are every 15 days\n",
    "    \n",
    "    # Identify the dates where there is < 20% cloud cover\n",
    "    satisfactory_ids = list(np.argwhere(np.array(means) < 0.2).reshape(-1, )) \n",
    "    satisfactory_dates = [value for idx, value in enumerate(image_dates) if idx in satisfactory_ids]\n",
    "    \n",
    "    \n",
    "    selected_images = {}\n",
    "    for i in biweekly_dates:\n",
    "        distances = [abs(date - i) for date in satisfactory_dates]\n",
    "        closest = np.min(distances)\n",
    "        closest_id = np.argmin(distances)\n",
    "        # If there is imagery within 8 days, select it\n",
    "        if closest < 8:\n",
    "            date = satisfactory_dates[closest_id]\n",
    "            image_idx = int(np.argwhere(np.array(image_dates) == date)[0])\n",
    "            selected_images[i] = {'image_date': [date], 'image_ratio': [1], 'image_idx': [image_idx]}\n",
    "        # If there is not imagery within 8 days, look for the closest above and below imagery\n",
    "        else:\n",
    "            distances = np.array([(date - i) for date in satisfactory_dates])\n",
    "            # Number of days above and below the selected date of the nearest clean imagery\n",
    "            above = distances[np.where(distances < 0, distances, -np.inf).argmax()]\n",
    "            below = distances[np.where(distances > 0, distances, np.inf).argmin()]\n",
    "            if abs(above) > 100: # If date is the last date, occassionally argmax would set above to - number\n",
    "                above = below\n",
    "            if abs(below) > 100:\n",
    "                below = above\n",
    "            if above != below:\n",
    "                below_ratio = above / (above - below)\n",
    "                above_ratio = 1 - below_ratio\n",
    "            else:\n",
    "                above_ratio = below_ratio = 0.5\n",
    "                \n",
    "            # Extract the image date and imagery index for the above and below values\n",
    "            above_date = i + above\n",
    "            above_image_idx = int(np.argwhere(np.array(image_dates) == above_date)[0])\n",
    "            \n",
    "            below_date = i + below\n",
    "            below_image_idx = int(np.argwhere(np.array(image_dates) == below_date)[0])\n",
    "            \n",
    "            selected_images[i] = {'image_date': [above_date, below_date], 'image_ratio': [above_ratio, below_ratio],\n",
    "                                 'image_idx': [above_image_idx, below_image_idx]}\n",
    "                            \n",
    "    max_distance = 0\n",
    "    \n",
    "    for i in selected_images.keys():\n",
    "        print(i, selected_images[i])\n",
    "        if len(selected_images[i]['image_date']) == 2:\n",
    "            dist = selected_images[i]['image_date'][1] - selected_images[i]['image_date'][0]\n",
    "            if dist > max_distance:\n",
    "                max_distance = dist\n",
    "    \n",
    "    print(\"Maximum time distance: {}\".format(max_distance))\n",
    "        \n",
    "    # Compute the weighted average of the selected imagery for each time step\n",
    "    keep_steps = []\n",
    "    for i in selected_images.keys():\n",
    "        info = selected_images[i]\n",
    "        if len(info['image_idx']) == 1:\n",
    "            step = img_bands[info['image_idx'][0]]\n",
    "        if len(info['image_idx']) == 2:\n",
    "            step1 = img_bands[info['image_idx'][0]] * info['image_ratio'][0]\n",
    "            step2 = img_bands[info['image_idx'][1]] * info['image_ratio'][1]\n",
    "            step = step1 + step2\n",
    "        keep_steps.append(step)\n",
    "        \n",
    "    keep_steps = np.stack(keep_steps)\n",
    "    return keep_steps, max_distance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_best(tiles, cloud_probs, request, offset_x, offset_y):\n",
    "    c_probs = cloud_probs[:, offset_x:offset_x+16, offset_y:offset_y+16]\n",
    "    images = np.stack(tiles)[:, offset_x:offset_x+16, offset_y:offset_y+16]\n",
    "    means = np.mean(c_probs, (1, 2))\n",
    "    cloud_steps = [i for i, val in enumerate(means) if val > 0.20]\n",
    "    best = calculate_and_save_best_images(cloud_steps, images, request, means)\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING DOWNLOAD OF 3 plots from ../data/kenya-test.csv to ../data/test-data-nov-27/\n",
      "Downloading 135542384\n",
      "There are 10/33 clean steps\n",
      "0 broken normal steps\n",
      "[-7, 12, 31, 63, 73, 83, 92, 112, 142, 161, 171, 181, 186, 196, 201, 206, 215, 225, 230, 235, 249, 259, 264, 274, 279, 303, 328, 338, 343, 348, 353, 358, 377]\n",
      "0 {'image_date': [-7], 'image_ratio': [1], 'image_idx': [0]}\n",
      "15 {'image_date': [12], 'image_ratio': [1], 'image_idx': [1]}\n",
      "30 {'image_date': [12, 73], 'image_ratio': [0.7049180327868853, 0.29508196721311475], 'image_idx': [1, 4]}\n",
      "45 {'image_date': [12, 73], 'image_ratio': [0.4590163934426229, 0.5409836065573771], 'image_idx': [1, 4]}\n",
      "60 {'image_date': [12, 73], 'image_ratio': [0.21311475409836067, 0.7868852459016393], 'image_idx': [1, 4]}\n",
      "75 {'image_date': [73], 'image_ratio': [1], 'image_idx': [4]}\n",
      "90 {'image_date': [92], 'image_ratio': [1], 'image_idx': [6]}\n",
      "105 {'image_date': [92, 92], 'image_ratio': [0.5, 0.5], 'image_idx': [6, 6]}\n",
      "120 {'image_date': [92, 206], 'image_ratio': [0.7543859649122807, 0.24561403508771928], 'image_idx': [6, 15]}\n",
      "135 {'image_date': [92, 206], 'image_ratio': [0.6228070175438596, 0.37719298245614036], 'image_idx': [6, 15]}\n",
      "150 {'image_date': [92, 206], 'image_ratio': [0.49122807017543857, 0.5087719298245614], 'image_idx': [6, 15]}\n",
      "165 {'image_date': [92, 206], 'image_ratio': [0.35964912280701755, 0.6403508771929824], 'image_idx': [6, 15]}\n",
      "180 {'image_date': [92, 206], 'image_ratio': [0.22807017543859653, 0.7719298245614035], 'image_idx': [6, 15]}\n",
      "195 {'image_date': [206, 206], 'image_ratio': [0.5, 0.5], 'image_idx': [15, 15]}\n",
      "210 {'image_date': [206], 'image_ratio': [1], 'image_idx': [15]}\n",
      "225 {'image_date': [230], 'image_ratio': [1], 'image_idx': [18]}\n",
      "240 {'image_date': [230, 249], 'image_ratio': [0.4736842105263158, 0.5263157894736842], 'image_idx': [18, 20]}\n",
      "255 {'image_date': [249], 'image_ratio': [1], 'image_idx': [20]}\n",
      "270 {'image_date': [274], 'image_ratio': [1], 'image_idx': [23]}\n",
      "285 {'image_date': [274, 348], 'image_ratio': [0.8513513513513513, 0.14864864864864866], 'image_idx': [23, 29]}\n",
      "300 {'image_date': [274, 348], 'image_ratio': [0.6486486486486487, 0.35135135135135137], 'image_idx': [23, 29]}\n",
      "315 {'image_date': [274, 348], 'image_ratio': [0.44594594594594594, 0.5540540540540541], 'image_idx': [23, 29]}\n",
      "330 {'image_date': [274, 348], 'image_ratio': [0.2432432432432432, 0.7567567567567568], 'image_idx': [23, 29]}\n",
      "345 {'image_date': [348], 'image_ratio': [1], 'image_idx': [29]}\n",
      "Maximum time distance: 114\n",
      "Skipping 135542384 because there is a 114 distance\n",
      "Downloading 135542470\n",
      "There are 22/32 clean steps\n",
      "0 broken normal steps\n",
      "[-3, 45, 67, 96, 116, 126, 136, 146, 155, 175, 180, 190, 200, 210, 219, 229, 239, 263, 273, 278, 283, 288, 322, 327, 332, 337, 342, 352, 357, 366, 371, 376]\n",
      "0 {'image_date': [45, 45], 'image_ratio': [0.5, 0.5], 'image_idx': [1, 1]}\n",
      "15 {'image_date': [45, 45], 'image_ratio': [0.5, 0.5], 'image_idx': [1, 1]}\n",
      "30 {'image_date': [45, 45], 'image_ratio': [0.5, 0.5], 'image_idx': [1, 1]}\n",
      "45 {'image_date': [45], 'image_ratio': [1], 'image_idx': [1]}\n",
      "60 {'image_date': [67], 'image_ratio': [1], 'image_idx': [2]}\n",
      "75 {'image_date': [67, 116], 'image_ratio': [0.8367346938775511, 0.16326530612244897], 'image_idx': [2, 4]}\n",
      "90 {'image_date': [67, 116], 'image_ratio': [0.5306122448979591, 0.46938775510204084], 'image_idx': [2, 4]}\n",
      "105 {'image_date': [67, 116], 'image_ratio': [0.22448979591836737, 0.7755102040816326], 'image_idx': [2, 4]}\n",
      "120 {'image_date': [116], 'image_ratio': [1], 'image_idx': [4]}\n",
      "135 {'image_date': [136], 'image_ratio': [1], 'image_idx': [6]}\n",
      "150 {'image_date': [146], 'image_ratio': [1], 'image_idx': [7]}\n",
      "165 {'image_date': [146, 175], 'image_ratio': [0.3448275862068966, 0.6551724137931034], 'image_idx': [7, 9]}\n",
      "180 {'image_date': [175], 'image_ratio': [1], 'image_idx': [9]}\n",
      "195 {'image_date': [190], 'image_ratio': [1], 'image_idx': [11]}\n",
      "210 {'image_date': [210], 'image_ratio': [1], 'image_idx': [13]}\n",
      "225 {'image_date': [210, 263], 'image_ratio': [0.7169811320754718, 0.2830188679245283], 'image_idx': [13, 17]}\n",
      "240 {'image_date': [210, 263], 'image_ratio': [0.4339622641509434, 0.5660377358490566], 'image_idx': [13, 17]}\n",
      "255 {'image_date': [210, 263], 'image_ratio': [0.15094339622641506, 0.8490566037735849], 'image_idx': [13, 17]}\n",
      "270 {'image_date': [263], 'image_ratio': [1], 'image_idx': [17]}\n",
      "285 {'image_date': [283], 'image_ratio': [1], 'image_idx': [20]}\n",
      "300 {'image_date': [288, 327], 'image_ratio': [0.6923076923076923, 0.3076923076923077], 'image_idx': [21, 23]}\n",
      "315 {'image_date': [288, 327], 'image_ratio': [0.3076923076923077, 0.6923076923076923], 'image_idx': [21, 23]}\n",
      "330 {'image_date': [332], 'image_ratio': [1], 'image_idx': [24]}\n",
      "345 {'image_date': [342], 'image_ratio': [1], 'image_idx': [26]}\n",
      "Maximum time distance: 53\n",
      "Downloading 135542549\n",
      "There are 24/27 clean steps\n",
      "0 broken normal steps\n",
      "[-13, -3, 6, 16, 26, 67, 77, 87, 136, 146, 190, 210, 229, 239, 263, 268, 273, 283, 327, 332, 337, 342, 352, 357, 366, 371, 376]\n",
      "0 {'image_date': [-3], 'image_ratio': [1], 'image_idx': [1]}\n",
      "15 {'image_date': [16], 'image_ratio': [1], 'image_idx': [3]}\n",
      "30 {'image_date': [26], 'image_ratio': [1], 'image_idx': [4]}\n",
      "45 {'image_date': [26, 67], 'image_ratio': [0.5365853658536586, 0.4634146341463415], 'image_idx': [4, 5]}\n",
      "60 {'image_date': [67], 'image_ratio': [1], 'image_idx': [5]}\n",
      "75 {'image_date': [77], 'image_ratio': [1], 'image_idx': [6]}\n",
      "90 {'image_date': [87], 'image_ratio': [1], 'image_idx': [7]}\n",
      "105 {'image_date': [87, 136], 'image_ratio': [0.6326530612244898, 0.3673469387755102], 'image_idx': [7, 8]}\n",
      "120 {'image_date': [87, 136], 'image_ratio': [0.326530612244898, 0.673469387755102], 'image_idx': [7, 8]}\n",
      "135 {'image_date': [136], 'image_ratio': [1], 'image_idx': [8]}\n",
      "150 {'image_date': [146], 'image_ratio': [1], 'image_idx': [9]}\n",
      "165 {'image_date': [146, 210], 'image_ratio': [0.703125, 0.296875], 'image_idx': [9, 11]}\n",
      "180 {'image_date': [146, 210], 'image_ratio': [0.46875, 0.53125], 'image_idx': [9, 11]}\n",
      "195 {'image_date': [146, 210], 'image_ratio': [0.234375, 0.765625], 'image_idx': [9, 11]}\n",
      "210 {'image_date': [210], 'image_ratio': [1], 'image_idx': [11]}\n",
      "225 {'image_date': [210, 239], 'image_ratio': [0.48275862068965514, 0.5172413793103449], 'image_idx': [11, 13]}\n",
      "240 {'image_date': [239], 'image_ratio': [1], 'image_idx': [13]}\n",
      "255 {'image_date': [239, 263], 'image_ratio': [0.33333333333333337, 0.6666666666666666], 'image_idx': [13, 14]}\n",
      "270 {'image_date': [268], 'image_ratio': [1], 'image_idx': [15]}\n",
      "285 {'image_date': [283], 'image_ratio': [1], 'image_idx': [17]}\n",
      "300 {'image_date': [283, 332], 'image_ratio': [0.653061224489796, 0.3469387755102041], 'image_idx': [17, 19]}\n",
      "315 {'image_date': [283, 332], 'image_ratio': [0.34693877551020413, 0.6530612244897959], 'image_idx': [17, 19]}\n",
      "330 {'image_date': [332], 'image_ratio': [1], 'image_idx': [19]}\n",
      "345 {'image_date': [342], 'image_ratio': [1], 'image_idx': [21]}\n",
      "Maximum time distance: 64\n"
     ]
    }
   ],
   "source": [
    "to_download = [x for x in plot_ids if x not in existing]\n",
    "errors = []\n",
    "print(\"STARTING DOWNLOAD OF {} plots from {} to {}\".format(len(to_download), DATA_LOCATION, OUTPUT_FOLDER))\n",
    "for i, val in enumerate(plot_ids):\n",
    "    if val not in existing:\n",
    "        print(\"Downloading {}\".format(val))\n",
    "        location = calc_bbox(val)\n",
    "        location = bounding_box(location, expansion = 160)\n",
    "        try:\n",
    "            # Initiate hash tables\n",
    "            cloud, means, probs = identify_clouds(location)\n",
    "            dem = download_dem(val)\n",
    "            img, image_request = download_tiles(location, means)\n",
    "            tiles, max_distance = calculate_and_save_best_images(cloud, img, image_request, means) # 22, 16, 16, 10\n",
    "            dem = np.tile(dem.reshape((1, 16, 16, 1)), (tiles.shape[0], 1, 1, 1))\n",
    "            tiles = np.concatenate([tiles, dem], axis = -1)\n",
    "            if max_distance <= 90:\n",
    "                np.save(OUTPUT_FOLDER + str(val), tiles)\n",
    "            else:\n",
    "                print(\"Skipping {} because there is a {} distance\".format(val, max_distance))\n",
    "\n",
    "        except Exception as e:\n",
    "        #    print(e)\n",
    "            logging.fatal(e, exc_info=True)\n",
    "            errors.append(img)\n",
    "            #continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "policy-toolkit",
   "language": "python",
   "name": "policy-toolkit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
