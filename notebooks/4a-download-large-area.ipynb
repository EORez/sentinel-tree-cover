{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from osgeo import ogr, osr\n",
    "from sentinelhub import WmsRequest, WcsRequest, MimeType, CRS, BBox, constants\n",
    "import logging\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import os\n",
    "import yaml\n",
    "from sentinelhub import DataSource\n",
    "import scipy.sparse as sparse\n",
    "import scipy\n",
    "from scipy.sparse.linalg import splu\n",
    "from skimage.transform import resize\n",
    "from sentinelhub import CustomUrlParam\n",
    "from time import time as timer\n",
    "from time import sleep as sleep\n",
    "import multiprocessing\n",
    "import math\n",
    "import reverse_geocoder as rg\n",
    "import pycountry\n",
    "import pycountry_convert as pc\n",
    "import hickle as hkl\n",
    "from shapely.geometry import Point, Polygon\n",
    "import geopandas\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "import math\n",
    "import boto3\n",
    "from pyproj import Proj, transform\n",
    "from timeit import default_timer as timer\n",
    "from typing import Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../config.yaml\", 'r') as stream:\n",
    "    key = (yaml.safe_load(stream))\n",
    "    API_KEY = key['key']\n",
    "    AWSKEY = key['awskey']\n",
    "    AWSSECRET = key['awssecret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%run ../src/preprocessing/slope.py\n",
    "%run ../src/preprocessing/indices.py\n",
    "%run ../src/downloading/utils.py\n",
    "%run ../src/preprocessing/cloud_removal.py\n",
    "%run ../src/preprocessing/whittaker_smoother.py\n",
    "%run ../src/dsen2/utils/DSen2Net.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2019\n",
    "dates = ('{}-12-01'.format(str(year - 1)) , '{}-02-01'.format(str(year + 1)))\n",
    "dates_sentinel_1 = ('{}-01-01'.format(str(year)) , '{}-12-31'.format(str(year)))\n",
    "SIZE = 9*5\n",
    "IMSIZE = (7*2) + (SIZE * 14)+2\n",
    "\n",
    "days_per_month = [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30]\n",
    "starting_days = np.cumsum(days_per_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "646\n"
     ]
    }
   ],
   "source": [
    "print(IMSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../tile_data/elsalvador-imposible/ (-90.015579, 13.727334)\n"
     ]
    }
   ],
   "source": [
    "landscapes = {\n",
    "    'ethiopia-tigray': (13.540810, 38.177220),\n",
    "    'kenya-makueni-2': (-1.817109, 37.44563),\n",
    "    'ghana': (9.259359, -0.83375),\n",
    "    'niger-koure': (13.18158, 2.478),\n",
    "    'cameroon-farnorth': (10.596, 14.2722),\n",
    "    'mexico-campeche': (18.232495, -92.1234215),\n",
    "    'malawi-rumphi-old': (-11.044, 33.818),\n",
    "    'malawi-rumphi': (-11.15, 33.246),\n",
    "    'ghana-sisala-east': (10.385, -1.765),\n",
    "    'ghana-west-mamprusi': (10.390084, -0.846330),\n",
    "    'ghana-kwahu': (6.518909, -0.826008),\n",
    "    'senegal-16b': (15.82585, -15.34166),\n",
    "    'india-kochi': (9.909, 76.254),\n",
    "    'india-sidhi': (24.0705, 81.607),\n",
    "    'brazil-esperito-santo': (-20.147, -40.837),\n",
    "    'brazil-paraiba': (-22.559943, -44.186629),\n",
    "    'brazil-goias': (-14.905595, -48.907399),\n",
    "    'colombia-talima': (4.179529, -74.889171),\n",
    "    'drc-kafubu': (-11.749636, 27.586622),\n",
    "    'thailand-khon-kaen': (15.709725, 102.546518),\n",
    "    'indonesia-west-java': (-6.721101, 108.280949),\n",
    "    'madagascar': (-18.960152, 47.469587),\n",
    "    'tanzania': (-6.272258, 36.679824),\n",
    "    'chile': (-36.431237, -71.872030),\n",
    "    'indonesia-jakarta': (-6.352580, 106.677072),\n",
    "    'caf-baboua': (5.765917, 14.791618),   \n",
    "    'honduras': (14.096664, -88.720304),\n",
    "    'nicaragua': (12.398014, -86.963042),\n",
    "    'china': (26.673679, 107.464231),\n",
    "    'australia-west': (-32.666762, 117.411197),\n",
    "    'mexico-sonora': (29.244288, -111.243230),\n",
    "    'south-africa': (-30.981698, 28.727301),\n",
    "    'maldonado-uraguay': (-34.629250, -55.004331),\n",
    "    'dominican-rep-la-salvia': (18.872589, -70.462961),\n",
    "    'guatemala-coban': (15.3, -90.8),\n",
    "    'senegal-tucker-a': (15.350595, -15.459789),\n",
    "    'elsalvador-imposible': (13.727334, -90.015579),\n",
    "    'peru-shatoja-district': (-6.566366, -76.759752),\n",
    "    'angola-galanga': (-12.104782, 15.151222),\n",
    "    'morocco-chefchaouen': (34.942560, -4.772589),\n",
    "    'georgia-imereti': (42.223069, 42.603353),\n",
    "    'drc-mai-ndombe' : (-3.696119, 20.362077),\n",
    "    'malawi-salima': (-13.6, 34.32),\n",
    "    'brazil-para': (-2.064534, -56.578095),\n",
    "    'brazil-para-2': (-7.351687, -48.457507),\n",
    "    'pakistan-mardan': (34.355452, 71.945095),\n",
    "    'botswana-kweneng': (-24.360968, 25.176526),\n",
    "    'nicaragua-bonanza': (13.933745, -84.690842),\n",
    "    'ghana-cocoa': (7.398111, -1.269223),\n",
    "    'ghana-brong-ahafo': (7.70258, -0.70911),\n",
    "    'mexico-change-det': (21.212083, -88.993677),\n",
    "    'costa-rica-change-det': (8.47520, -82.94909),\n",
    "    'honduras-colon': (15.617889, -85.447611),\n",
    "    'mexico-campeche-change': (18.151747, -92.152278),\n",
    "    'guatemala-gain': (16.464444, -89.479170)\n",
    "}\n",
    "\n",
    "landscape = 'elsalvador-imposible'\n",
    "\n",
    "OUTPUT_FOLDER = '../tile_data/{}/'.format(landscape)\n",
    "coords = landscapes[landscape]\n",
    "coords = (coords[1], coords[0])\n",
    "print(OUTPUT_FOLDER, coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "landscape_df = pd.DataFrame({'landscape': [x for x in landscapes.keys()], \n",
    "                             'latitude': [x[0] for x in landscapes.values()],\n",
    "                             'longitude': [x[1] for x in landscapes.values()]\n",
    "})\n",
    "\n",
    "landscape_df.to_csv(\"../data/latlongs/landscapes.csv\", index=False)\n",
    "print(len(landscape_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projection functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bbx_pyproj(coord: Tuple[float, float],\n",
    "                         step_x: int, step_y: int,\n",
    "                         expansion: int, multiplier: int = 1.) -> (Tuple[float, float], 'CRS'):\n",
    "    ''' Calculates the four corners of a bounding box\n",
    "        [bottom left, top right] as well as the UTM EPSG using Pyproj\n",
    "        \n",
    "        Note: The input for this function is (x, y), not (lat, long)\n",
    "        \n",
    "        Parameters:\n",
    "         coord (tuple): Initial (long, lat) coord\n",
    "         step_x (int): X tile number of a 6300x6300 meter tile\n",
    "         step_y (int): Y tile number of a 6300x6300 meter tile\n",
    "         expansion (int): Typically 10 meters - the size of the border for the predictions\n",
    "         multiplier (int): Currently deprecated\n",
    "         \n",
    "        Returns:\n",
    "         coords (tuple):\n",
    "         CRS (int):\n",
    "    '''\n",
    "    \n",
    "    inproj = Proj('epsg:4326')\n",
    "    outproj_code = calculate_epsg(coord)\n",
    "    outproj = Proj('epsg:' + str(outproj_code))\n",
    "    \n",
    "    \n",
    "    \n",
    "    coord_utm =  transform(inproj, outproj, coord[1], coord[0])\n",
    "    coord_utm_bottom_left = (coord_utm[0] + step_x*6300 - expansion,\n",
    "                             coord_utm[1] + step_y*6300 - expansion)\n",
    "    \n",
    "    coord_utm_top_right = (coord_utm[0] + (step_x+multiplier) * 6300 + expansion,\n",
    "                           coord_utm[1] + (step_y+multiplier) * 6300 + expansion)\n",
    "\n",
    "    zone = str(outproj_code)[3:]\n",
    "    direction = 'N' if coord[1] >= 0 else 'S'\n",
    "    utm_epsg = \"UTM_\" + zone + direction\n",
    "    return (coord_utm_bottom_left, coord_utm_top_right), CRS[utm_epsg]\n",
    "\n",
    "\n",
    "def pts_in_geojson(lats: List[float], longs: List[float], geojson: 'geojson') -> bool:  \n",
    "    \"\"\" Identifies whether candidate download tile is within an input geojson\n",
    "        \n",
    "        Parameters:\n",
    "         lats (list): list of latitudes\n",
    "         longs (list): list of longitudes\n",
    "         geojson (float): path to input geojson\n",
    "    \n",
    "        Returns:\n",
    "         bool \n",
    "    \"\"\"\n",
    "    polys = geopandas.read_file(geojson)['geometry']\n",
    "    polys = geopandas.GeoSeries(polys)\n",
    "    pnts = [Point(x, y) for x, y in zip(list(lats), list(longs))]\n",
    "    \n",
    "    def _contains(pt):\n",
    "        return polys.contains(pt)[0]\n",
    "\n",
    "    if any([_contains(pt) for pt in pnts]):\n",
    "        return True\n",
    "    else: return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data download functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_clouds_new(bbox: List[Tuple[float, float]],\n",
    "                        epsg: 'CRS', dates: dict = dates) -> (np.ndarray, np.ndarray, np.ndarray):\n",
    "\n",
    "    \"\"\" Downloads and calculates cloud cover and shadow\n",
    "        \n",
    "        Parameters:\n",
    "         bbox (list): output of calc_bbox\n",
    "         epsg (float): EPSG associated with bbox \n",
    "         dates (tuple): YY-MM-DD - YY-MM-DD bounds for downloading \n",
    "    \n",
    "        Returns:\n",
    "         cloud_img (np.array):\n",
    "         shadows (np.array): \n",
    "         clean_steps (np.array):\n",
    "    \"\"\"\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    cloud_request = WcsRequest(\n",
    "        layer='CLOUD_NEW',\n",
    "        bbox=box,\n",
    "        time=dates,\n",
    "        resx='160m', \n",
    "        resy='160m',\n",
    "        image_format = MimeType.TIFF_d8,\n",
    "        maxcc=0.7,\n",
    "        instance_id=API_KEY,\n",
    "        custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "        time_difference=datetime.timedelta(hours=72),\n",
    "    )\n",
    "\n",
    "\n",
    "    shadow_request = WcsRequest(\n",
    "        layer='SHADOW',\n",
    "        bbox=box,\n",
    "        time=dates,\n",
    "        resx='10m',\n",
    "        resy='10m',\n",
    "        image_format =  MimeType.TIFF_d16,\n",
    "        maxcc=0.7,\n",
    "        instance_id=API_KEY,\n",
    "        custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "        time_difference=datetime.timedelta(hours=72))\n",
    "\n",
    "    cloud_img = cloud_request.get_data()\n",
    "    cloud_img = np.array(cloud_img)\n",
    "    print(f\"The max clouds is {np.max(cloud_img)}\")\n",
    "    print(cloud_img.shape)\n",
    "    if np.max(cloud_img > 10):\n",
    "        cloud_img = cloud_img / 255\n",
    "        \n",
    "    assert np.max(cloud_img) <= 1.\n",
    "    c_probs_pus = ((40*40)/(512*512)) *(1/3)*cloud_img.shape[0]\n",
    "    print(f\"Cloud_probs used {c_probs_pus} processing units\")\n",
    "    \n",
    "    cloud_img = resize(cloud_img, (cloud_img.shape[0], IMSIZE, IMSIZE), order = 0)\n",
    "    n_cloud_px = np.array([len(np.argwhere(cloud_img[x, :, :].reshape((IMSIZE)*(IMSIZE)) > 0.33))\n",
    "                           for x in range(cloud_img.shape[0])])\n",
    "    cloud_steps = np.argwhere(n_cloud_px > IMSIZE**2 / 5)\n",
    "    clean_steps = [x for x in range(cloud_img.shape[0]) if x not in cloud_steps]\n",
    "    print(f\"Removing {len(cloud_steps)} from S2 download, saving {7.32 * len(cloud_steps)} PU\")\n",
    "\n",
    "    shadow_img = shadow_request.get_data(data_filter = clean_steps)\n",
    "    shadow_img = np.array(shadow_img)\n",
    "    print(shadow_img.shape)\n",
    "    shadow_img = resize(shadow_img, (shadow_img.shape[0], IMSIZE, IMSIZE, shadow_img.shape[-1]), order = 0)\n",
    "    print(shadow_img.shape)\n",
    "    shadow_pus = ((IMSIZE*IMSIZE)/(512*512)) * shadow_img.shape[0]\n",
    "    print(f\"Shadows used: {shadow_pus} processing units\")\n",
    "    print(f\"The max shadows is {np.max(shadow_img)}\")\n",
    "    if np.max(shadow_img > 10):\n",
    "        shadow_img = shadow_img / 65535\n",
    "    print(np.max(shadow_img))\n",
    " \n",
    "    cloud_img = np.delete(cloud_img, cloud_steps, 0)\n",
    "    shadows = mcm_shadow_mask(np.array(shadow_img), cloud_img)\n",
    "\n",
    "    print(f\"Cloud probs: {cloud_img.shape}\")\n",
    "    print(f\"Shadow shape {shadows.shape}\")\n",
    "    return cloud_img, shadows, clean_steps\n",
    "\n",
    "    \n",
    "    \n",
    "def download_dem(bbox: List[Tuple[float, float]], epsg: 'CRS') -> np.ndarray:\n",
    "    \"\"\" Downloads the DEM layer from Sentinel hub\n",
    "        \n",
    "        Parameters:\n",
    "         bbox (list): output of calc_bbox\n",
    "         epsg (float): EPSG associated with bbox \n",
    "    \n",
    "        Returns:\n",
    "         dem_image (arr):\n",
    "    \"\"\"\n",
    "\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    dem_s = (630)+4+8+8\n",
    "    dem_request = WmsRequest(data_source=DataSource.DEM,\n",
    "                         layer='DEM',\n",
    "                         bbox=box,\n",
    "                         width=dem_s,\n",
    "                         height=dem_s,\n",
    "                         instance_id=API_KEY,\n",
    "                         image_format=MimeType.TIFF_d32f,\n",
    "                         custom_url_params={CustomUrlParam.SHOWLOGO: False})\n",
    "    dem_image = dem_request.get_data()[0]\n",
    "    dem_image = calcSlope(dem_image.reshape((1, dem_s, dem_s)),\n",
    "                  np.full((dem_s, dem_s), 10), np.full((dem_s, dem_s), 10), zScale = 1, minSlope = 0.02)\n",
    "    dem_image = dem_image.reshape((dem_s,dem_s, 1))\n",
    "    dem_image = dem_image[1:dem_s-1, 1:dem_s-1, :]\n",
    "    print(f\"DEM used {((IMSIZE*IMSIZE)/(512*512))*2} processing units\")\n",
    "    return dem_image\n",
    " \n",
    "\n",
    "def download_layer(bbox: List[Tuple[float, float]],\n",
    "                   clean_steps: np.ndarray, epsg: 'CRS',\n",
    "                   dates: dict = dates, year: int = year) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\" Downloads the L2A sentinel layer with 10 and 20 meter bands\n",
    "        \n",
    "        Parameters:\n",
    "         bbox (list): output of calc_bbox\n",
    "         clean_steps (list): list of steps to filter download request\n",
    "         epsg (float): EPSG associated with bbox \n",
    "         time (tuple): YY-MM-DD - YY-MM-DD bounds for downloading \n",
    "    \n",
    "        Returns:\n",
    "         img (arr):\n",
    "         img_request (obj): \n",
    "    \"\"\"\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    image_request = WcsRequest(\n",
    "            layer='L2A20',\n",
    "            bbox=box,\n",
    "            time=dates,\n",
    "            image_format = MimeType.TIFF_d16,\n",
    "            maxcc=0.7,\n",
    "            resx='20m', resy='20m',\n",
    "            instance_id=API_KEY,\n",
    "            custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'NEAREST',\n",
    "                                constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "            time_difference=datetime.timedelta(hours=72),\n",
    "        )\n",
    "    print(\"Downloading L2A 20m layer\")\n",
    "    img_bands = image_request.get_data(data_filter = clean_steps)\n",
    "    img_20 = np.stack(img_bands)\n",
    "    print(f\"The max 20m is {np.max(img_20)}\")\n",
    "    if np.max(img_20) >= 10:\n",
    "        img_20 = img_20 / 65535\n",
    "    assert np.max(img_20) <= 2.\n",
    "    \n",
    "    s2_20_usage = (img_20.shape[1]*img_20.shape[2])/(512*512) * (6/3) * img_20.shape[0]\n",
    "    print(\"Original 20 meter bands size: {}, using {} PU\".format(img_20.shape, s2_20_usage))\n",
    "    img_20 = resize(img_20, (img_20.shape[0], IMSIZE, IMSIZE, img_20.shape[-1]), order = 0)\n",
    "\n",
    "    image_request = WcsRequest(\n",
    "            layer='L2A10',\n",
    "            bbox=box,\n",
    "            time=dates,\n",
    "            image_format = MimeType.TIFF_d16,\n",
    "            maxcc=0.7,\n",
    "            resx='10m', resy='10m',\n",
    "            instance_id=API_KEY,\n",
    "            custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'BICUBIC',\n",
    "                                constants.CustomUrlParam.UPSAMPLING: 'BICUBIC'},\n",
    "            time_difference=datetime.timedelta(hours=72),\n",
    "    )\n",
    "    \n",
    "    print(\"Downloading L2A 10m layer\")\n",
    "    img_bands = image_request.get_data(data_filter = clean_steps)\n",
    "    img_10 = np.stack(img_bands)\n",
    "    print(f\"The max 10m is {np.max(img_10)}\")\n",
    "    if np.max(img_10) >= 10:\n",
    "        img_10 = img_10 / 65535\n",
    "    assert np.max(img_10) <= 2.\n",
    "    \n",
    "    s2_10_usage = (img_10.shape[1]*img_10.shape[2])/(512*512) * (4/3) * img_10.shape[0]\n",
    "    print(\"Original 20 meter bands size: {}, using {} PU\".format(img_10.shape, s2_10_usage))\n",
    "    img_10 = resize(img_10, (img_10.shape[0], IMSIZE, IMSIZE, img_10.shape[-1]), order = 0)\n",
    "    img = np.concatenate([img_10, img_20], axis = -1)\n",
    "    print(f\"Sentinel 2 used {s2_20_usage + s2_10_usage} PU\")\n",
    "\n",
    "    image_dates = []\n",
    "    for date in image_request.get_dates():\n",
    "        if date.year == year - 1:\n",
    "            image_dates.append(-365 + starting_days[(date.month-1)] + date.day)\n",
    "        if date.year == year:\n",
    "            image_dates.append(starting_days[(date.month-1)] + date.day)\n",
    "        if date.year == year + 1:\n",
    "            image_dates.append(365 + starting_days[(date.month-1)]+date.day)\n",
    "    image_dates = [val for idx, val in enumerate(image_dates) if idx in clean_steps]\n",
    "    image_dates = np.array(image_dates)\n",
    "    return img, image_dates\n",
    "\n",
    "        \n",
    "def download_sentinel_1(bbox: List[Tuple[float, float]],\n",
    "                        epsg: 'CRS', imsize: int = IMSIZE, \n",
    "                        dates: dict = dates_sentinel_1, layer: str = \"SENT\",\n",
    "                        year: int = year) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\" Downloads the GRD Sentinel 1 VV-VH layer from Sentinel Hub\n",
    "        \n",
    "        Parameters:\n",
    "         bbox (list): output of calc_bbox\n",
    "         epsg (float): EPSG associated with bbox \n",
    "         imsize (int):\n",
    "         dates (tuple): YY-MM-DD - YY-MM-DD bounds for downloading \n",
    "         layer (str):\n",
    "         year (int): \n",
    "    \n",
    "        Returns:\n",
    "         s1 (arr):\n",
    "         image_dates (arr): \n",
    "    \"\"\"\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    image_request = WcsRequest(\n",
    "            layer=layer,\n",
    "            bbox=box,\n",
    "            time=dates,\n",
    "            image_format = MimeType.TIFF_d16,\n",
    "            data_source=DataSource.SENTINEL1_IW,\n",
    "            maxcc=1.0,\n",
    "            resx='10m', resy='5m',\n",
    "            instance_id=API_KEY,\n",
    "            custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'NEAREST',\n",
    "                                constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "            time_difference=datetime.timedelta(hours=96),\n",
    "        )\n",
    "    data_filter = None\n",
    "    if len(image_request.download_list) > 50:\n",
    "        data_filter = [x for x in range(len(image_request.download_list)) if x % 2 == 0]\n",
    "    img_bands = image_request.get_data(data_filter = data_filter)\n",
    "    s1 = np.stack(img_bands)\n",
    "    if np.max(s1) >= 1000:\n",
    "            s1 = s1 / 65535.\n",
    "    \n",
    "    print(f\"The max s1 is {np.max(s1)}\")\n",
    "    print(f\"Sentinel 1 used {(2/3)*s1.shape[0] * (s1.shape[1]*s1.shape[2])/(512*512)} PU for \\\n",
    "          {s1.shape[0]} out of {len(image_request.download_list)} images\")\n",
    "    s1 = resize(s1, (s1.shape[0], imsize*2, imsize*2, s1.shape[-1]), order = 0)\n",
    "    s1 = np.reshape(s1, (s1.shape[0], s1.shape[1]//2, 2, s1.shape[2] // 2, 2, s1.shape[-1]))\n",
    "    s1 = np.mean(s1, (2, 4))\n",
    "\n",
    "    image_dates = []\n",
    "    for date in image_request.get_dates():\n",
    "        if date.year == year - 1:\n",
    "            image_dates.append(-365 + starting_days[(date.month-1)] + date.day)\n",
    "        if date.year == year:\n",
    "            image_dates.append(starting_days[(date.month-1)] + date.day)\n",
    "        if date.year == year + 1:\n",
    "            image_dates.append(365 + starting_days[(date.month-1)]+date.day)\n",
    "    image_dates = np.array(image_dates)\n",
    "    s1c = np.copy(s1)\n",
    "    s1c[np.where(s1c < 1.)] = 0\n",
    "    n_pix_oob = np.sum(s1c, axis = (1, 2, 3))\n",
    "    to_remove = np.argwhere(n_pix_oob > (imsize*2*imsize*2)/50)\n",
    "    s1 = np.delete(s1, to_remove, 0)\n",
    "    image_dates = np.delete(image_dates, to_remove)\n",
    "    return s1, image_dates\n",
    "\n",
    "\n",
    "def identify_s1_layer(coords: Tuple[float, float]) -> str:\n",
    "    \"\"\" Identifies whether to download ascending or descending \n",
    "        sentinel 1 orbit based upon predetermined geographic coverage\n",
    "        \n",
    "        Reference: https://sentinel.esa.int/web/sentinel/missions/\n",
    "                   sentinel-1/satellite-description/geographical-coverage\n",
    "        \n",
    "        Parameters:\n",
    "         coords (tuple): \n",
    "    \n",
    "        Returns:\n",
    "         layer (str): either of SENT, SENT_DESC \n",
    "    \"\"\"\n",
    "    results = rg.search(coords)\n",
    "    country = results[-1]['cc']\n",
    "    continent_name = pc.country_alpha2_to_continent_code(country)\n",
    "    if continent_name in ['AF', 'OC']:\n",
    "        layer = \"SENT\"\n",
    "    if continent_name in ['SA']:\n",
    "        if coords[0] > -7.11:\n",
    "            layer = \"SENT\"\n",
    "        else:\n",
    "            layer = \"SENT_DESC\"\n",
    "    if continent_name in ['AS']:\n",
    "        if coords[0] > 23.3:\n",
    "            layer = \"SENT\"\n",
    "        else:\n",
    "            layer = \"SENT_DESC\"\n",
    "    if continent_name in ['NA']:\n",
    "        layer = \"SENT_DESC\"\n",
    "    print(continent_name)\n",
    "    print(layer)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud and shadow removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_missed_clouds(img: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" Removes clouds that may have been missed by s2cloudless\n",
    "        by looking at a temporal change outside of IQR\n",
    "        \n",
    "        Parameters:\n",
    "         img (arr): \n",
    "    \n",
    "        Returns:\n",
    "         to_remove (arr): \n",
    "    \"\"\"\n",
    "    \n",
    "    iqr = np.percentile(img[:, :, :, 3].flatten(), 75) - np.percentile(img[:, :, :, 3].flatten(), 25)\n",
    "    thresh_t = np.percentile(img[:, :, :, 3].flatten(), 75) + iqr*2\n",
    "    thresh_b = np.percentile(img[:, :, :, 3].flatten(), 25) - iqr*2\n",
    "    diffs_fw = np.diff(img, 1, axis = 0)\n",
    "    diffs_fw = np.mean(diffs_fw, axis = (1, 2, 3))\n",
    "    diffs_fw = np.array([0] + list(diffs_fw))\n",
    "    diffs_bw = np.diff(np.flip(img, 0), 1, axis = 0)\n",
    "    diffs_bw = np.flip(np.mean(diffs_bw, axis = (1, 2, 3)))\n",
    "    diffs_bw = np.array(list(diffs_bw) + [0])\n",
    "    diffs = abs(diffs_fw - diffs_bw) * 100\n",
    "    outlier_percs = []\n",
    "    for step in range(img.shape[0]):\n",
    "        bottom = len(np.argwhere(img[step, :, :, 3].flatten() > thresh_t))\n",
    "        top = len(np.argwhere(img[step, :, :, 3].flatten() < thresh_b))\n",
    "        p = 100* ((bottom + top) / (IMSIZE*IMSIZE))\n",
    "        outlier_percs.append(p)\n",
    "    to_remove = np.argwhere(np.array(outlier_percs) > 20)\n",
    "    return to_remove\n",
    "\n",
    "def calculate_bad_steps(sentinel2: np.ndarray, clouds: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" Calculates the timesteps to remove based upon cloud cover and missing data\n",
    "        \n",
    "        Parameters:\n",
    "         sentinel2 (arr): \n",
    "         clouds (arr):\n",
    "    \n",
    "        Returns:\n",
    "         to_remove (arr): \n",
    "    \"\"\"\n",
    "    n_cloud_px = np.array([len(np.argwhere(clouds[x, :, :].reshape((IMSIZE)*(IMSIZE)) > 0.30)) for x in range(clouds.shape[0])])\n",
    "    cloud_steps = np.argwhere(n_cloud_px > IMSIZE**2 / 7)\n",
    "    print(f'The percent cloud cover is {n_cloud_px/(IMSIZE**2)}')\n",
    "    missing_images = [np.argwhere(sentinel2[x, :, : :10].flatten() == 0.0) for x in range(sentinel2.shape[0])]\n",
    "    missing_images = np.array([len(x) for x in missing_images])\n",
    "    print(f'The number of missing 0 is {missing_images/(IMSIZE**2)}')\n",
    "    missing_images_p = [np.argwhere(sentinel2[x, :, : :10].flatten() >= 1) for x in range(sentinel2.shape[0])]\n",
    "    missing_images_p = np.array([len(x) for x in missing_images_p])\n",
    "    print(f'The number of missing 1 is {missing_images_p/(IMSIZE**2)}')\n",
    "    missing_images += missing_images_p\n",
    "    missing_images = np.argwhere(missing_images >= (IMSIZE**2) / 20)\n",
    "    to_remove = np.unique(np.concatenate([cloud_steps.flatten(), missing_images.flatten()]))\n",
    "    return to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def mcm_shadow_mask(arr: np.ndarray, c_probs: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" Calculates the multitemporal shadow mask for Sentinel-2 using\n",
    "        the methods from Candra et al. 2020 on L1C images and matching\n",
    "        outputs to the s2cloudless cloud probabilities\n",
    "\n",
    "        Parameters:\n",
    "         arr (arr): (Time, X, Y, Band) array of L1C data scaled from [0, 1]\n",
    "         c_probs (arr): (Time, X, Y) array of S2cloudless cloud probabilities\n",
    "    \n",
    "        Returns:\n",
    "         shadows_new (arr): cloud mask after Candra et al. 2020 and cloud matching \n",
    "         shadows_original (arr): cloud mask after Candra et al. 2020\n",
    "    \"\"\"\n",
    "    def _rank_array(arr):\n",
    "        order = arr.argsort()\n",
    "        ranks = order.argsort()\n",
    "        return ranks\n",
    "    \n",
    "    size = 648\n",
    "    \n",
    "    arr = resize(arr, (arr.shape[0], size, size, arr.shape[-1]), order = 0)\n",
    "    c_probs = resize(c_probs, (c_probs.shape[0], size, size), order = 0)\n",
    "    \n",
    "    mean_c_probs = np.mean(c_probs, axis = (1, 2))\n",
    "    cloudy_steps = np.argwhere(mean_c_probs > 0.25)\n",
    "    images_clean = np.delete(arr, cloudy_steps, 0)\n",
    "    cloud_ranks = _rank_array(mean_c_probs)\n",
    "    diffs = abs(np.sum(arr - np.mean(images_clean, axis = 0), axis = (1, 2, 3)))\n",
    "    diff_ranks = _rank_array(diffs)\n",
    "    overall_rank = diff_ranks + cloud_ranks\n",
    "    reference_idx = np.argmin(overall_rank)\n",
    "    ri = arr[reference_idx]\n",
    "    shadows = np.zeros((arr.shape[0], size, size))  \n",
    "    \n",
    "    # Candra et al. 2020\n",
    "    for time in tnrange(arr.shape[0]):\n",
    "        for x in range(arr.shape[1]):\n",
    "            for y in range(arr.shape[2]):\n",
    "                ti_slice = arr[time, x, y]\n",
    "                ri_slice = ri[x, y]\n",
    "                deltab2 = ti_slice[0] - ri_slice[0]\n",
    "                deltab8a = ti_slice[1] - ri_slice[1]\n",
    "                deltab11 = ti_slice[2] - ri_slice[2]\n",
    "                if deltab2 < 0.10: #(1000/65535):\n",
    "                    if deltab8a < -0.04: #(-400/65535):\n",
    "                        if deltab11 < -0.04: #(-400/65535):\n",
    "                            if ti_slice[0] < 0.095: #(950/65535):\n",
    "                                shadows[time, x, y] = 1.\n",
    "                                                       \n",
    "                            \n",
    "    # Remove shadows if cannot coreference a cloud\n",
    "    shadow_large = np.reshape(shadows, (shadows.shape[0], size // 8, 8, size // 8, 8))\n",
    "    shadow_large = np.sum(shadow_large, axis = (2, 4))\n",
    "\n",
    "    cloud_large = np.copy(c_probs)\n",
    "    cloud_large[np.where(c_probs > 0.33)] = 1.\n",
    "    cloud_large[np.where(c_probs < 0.33)] = 0.\n",
    "    cloud_large = np.reshape(cloud_large, (shadows.shape[0], size // 8, 8, size // 8, 8))\n",
    "    cloud_large = np.sum(cloud_large, axis = (2, 4))\n",
    "    print(shadow_large.shape)\n",
    "    print(cloud_large.shape)\n",
    "    for time in tnrange(shadow_large.shape[0]):\n",
    "        for x in range(shadow_large.shape[1]):\n",
    "            x_low = np.max([x - 8, 0])\n",
    "            x_high = np.min([x + 8, shadow_large.shape[1] - 2])\n",
    "            for y in range(shadow_large.shape[2]):\n",
    "                y_low = np.max([y - 8, 0])\n",
    "                y_high = np.min([y + 8, shadow_large.shape[1] - 2])\n",
    "                if shadow_large[time, x, y] < 8:\n",
    "                    shadow_large[time, x, y] = 0.\n",
    "                if shadow_large[time, x, y] >= 8:\n",
    "                    shadow_large[time, x, y] = 1.\n",
    "                c_prob_window = cloud_large[time, x_low:x_high, y_low:y_high]\n",
    "                if np.max(c_prob_window) < 16:\n",
    "                    shadow_large[time, x, y] = 0.\n",
    "                    \n",
    "    shadow_large = resize(shadow_large, (shadow_large.shape[0], size, size), order = 0)\n",
    "    shadows *= shadow_large\n",
    "    \n",
    "    # Go through and aggregate the shadow map to an 80m grid\n",
    "    # and extend it one grid size around any positive ID\n",
    "    shadows = np.reshape(shadows, (shadows.shape[0], size // 8, 8, size // 8, 8))\n",
    "    shadows = np.sum(shadows, axis = (2, 4))\n",
    "    shadows[np.where(shadows < 12)] = 0.\n",
    "    shadows[np.where(shadows >= 12)] = 1.\n",
    "    shadows = resize(shadows, (shadows.shape[0], size, size), order = 0)\n",
    "    shadows = np.reshape(shadows, (shadows.shape[0], size//4, 4, size//4, 4))\n",
    "    shadows = np.max(shadows, (2, 4))\n",
    "    \n",
    "    shadows_new = np.zeros_like(shadows)\n",
    "    for time in range(shadows.shape[0]):\n",
    "        for x in range(shadows.shape[1]):\n",
    "            for y in range(shadows.shape[2]):\n",
    "                if shadows[time, x, y] == 1:\n",
    "                    min_x = np.max([x - 1, 0])\n",
    "                    max_x = np.min([x + 2, 157])\n",
    "                    min_y = np.max([y - 1, 0])\n",
    "                    max_y = np.min([y + 2, 157])\n",
    "                    for x_idx in range(min_x, max_x):\n",
    "                        for y_idx in range(min_y, max_y):\n",
    "                            shadows_new[time, x_idx, y_idx] = 1.\n",
    "    shadows_new = resize(shadows_new, (shadows.shape[0], IMSIZE, IMSIZE), order = 0)\n",
    "    return shadows_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data interpolation\n",
    "\n",
    "Because the `smooth` function is called 5.6 million times on each 4000 hectare array, most of the computations are done outside of the function with the predefined `lmbd` and `d`, and then passed in as the `coefmat`, to avoid needless recomputation. This saves 141 CPU minutes per 4000 hectare array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15355894900858402\n"
     ]
    }
   ],
   "source": [
    "coefmat = intialize_smoother()\n",
    "test = np.ones((72, 10000))\n",
    "s = timer()\n",
    "x = parallel_apply_along_axis(smooth, 0, test)\n",
    "e = timer()\n",
    "print(e - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18665867403615266\n"
     ]
    }
   ],
   "source": [
    "test = np.ones((72, 10000))\n",
    "s = timer()\n",
    "x = np.apply_along_axis(smooth, 0, test)\n",
    "e = timer()\n",
    "print(e - s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Superresolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/john.brandt/anaconda3/envs/remote_sensing/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Symbolic Model Created.\n"
     ]
    }
   ],
   "source": [
    "MDL_PATH = \"../src/dsen2/models/\"\n",
    "\n",
    "input_shape = ((4, None, None), (6, None, None))\n",
    "model = s2model(input_shape, num_layers=6, feature_size=128)\n",
    "predict_file = MDL_PATH+'s2_032_lr_1e-04.hdf5'\n",
    "print('Symbolic Model Created.')\n",
    "\n",
    "model.load_weights(predict_file)\n",
    "\n",
    "def DSen2(d10: np.ndarray, d20: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Super resolves 20 meter bans using the DSen2 convolutional\n",
    "       neural network, as specified in Lanaras et al. 2018\n",
    "       https://github.com/lanha/DSen2\n",
    "\n",
    "        Parameters:\n",
    "         d10 (arr): (4, X, Y) shape array with 10 meter resolution\n",
    "         d20 (arr): (6, X, Y) shape array with 20 meter resolution\n",
    "\n",
    "        Returns:\n",
    "         prediction (arr): (6, X, Y) shape array with 10 meter superresolved\n",
    "                          output of DSen2 on d20 array\n",
    "    \"\"\"\n",
    "    test = [d10, d20]\n",
    "    input_shape = ((4, None, None), (6, None, None))\n",
    "    prediction = _predict(test, input_shape, deep=False)\n",
    "    return prediction\n",
    "\n",
    "def _predict(test: np.ndarray, input_shape: Tuple, model: 'model' = model,\n",
    "             deep: bool = False, run_60: bool = False) -> np.ndarray:\n",
    "    \n",
    "    prediction = model.predict(test, verbose=0, batch_size = 8)\n",
    "    return prediction\n",
    "\n",
    "def superresolve(sentinel2: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Worker function to deal with types and shapes\n",
    "       to superresolve a 10-band input array\n",
    "\n",
    "        Parameters:\n",
    "         sentinel2 (arr): (:, X, Y, 10) shape array with 10 meter resolution\n",
    "                          bands in indexes 0-4, and 20 meter in 4- 10\n",
    "\n",
    "        Returns:\n",
    "         superresolved (arr): (:, X, Y, 10) shape array with 10 meter \n",
    "                              superresolved output of DSen2\n",
    "    \"\"\"\n",
    "    d10 = sentinel2[:, :, :, 0:4]\n",
    "    d20 = sentinel2[:, :, :, 4:10]\n",
    "\n",
    "    d10 = np.swapaxes(d10, 1, -1)\n",
    "    d10 = np.swapaxes(d10, 2, 3)\n",
    "    d20 = np.swapaxes(d20, 1, -1)\n",
    "    d20 = np.swapaxes(d20, 2, 3)\n",
    "    superresolved = DSen2(d10, d20)\n",
    "    superresolved = np.swapaxes(superresolved, 1, -1)\n",
    "    superresolved = np.swapaxes(superresolved, 1, 2)\n",
    "    sentinel2[:, :, :, 4:10] = superresolved\n",
    "    return sentinel2 # returns band IDXs 3, 4, 5, 7, 8, 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiling and folder management functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_output_and_temp_folders(idx, output_folder = OUTPUT_FOLDER):\n",
    "    \n",
    "    def _find_and_make_dirs(dirs):\n",
    "        if not os.path.exists(os.path.realpath(dirs)):\n",
    "            os.makedirs(os.path.realpath(dirs))\n",
    "            \n",
    "    _find_and_make_dirs(output_folder + \"raw/\")\n",
    "    _find_and_make_dirs(output_folder + \"raw/clouds/\")\n",
    "    _find_and_make_dirs(output_folder + \"raw/s1/\")\n",
    "    _find_and_make_dirs(output_folder + \"raw/s2/\")\n",
    "    _find_and_make_dirs(output_folder + \"raw/misc/\")\n",
    "    _find_and_make_dirs(output_folder + \"processed/\")\n",
    "    \n",
    "\n",
    "def check_contains(coord, step_x, step_y, folder = OUTPUT_FOLDER):\n",
    "    contains = False\n",
    "    bbx, epsg = calculate_bbx_pyproj(coord, step_x, step_y, expansion = 80)\n",
    "    inproj = Proj('epsg:' + str(str(epsg)[5:]))\n",
    "    outproj = Proj('epsg:4326')\n",
    "    bottomleft = transform(inproj, outproj, bbx[0][0], bbx[0][1])\n",
    "    topright = transform(inproj, outproj, bbx[1][0], bbx[1][1])\n",
    "    \n",
    "    if os.path.exists(folder):\n",
    "            if any([x.endswith(\".geojson\") for x in os.listdir(folder)]):\n",
    "                geojson_path = folder + [x for x in os.listdir(folder) if x.endswith(\".geojson\")][0]\n",
    "    \n",
    "                bool_contains = pts_in_geojson(lats = [bottomleft[1], topright[1]], \n",
    "                                                       longs = [bottomleft[0], topright[0]],\n",
    "                                                       geojson = geojson_path)\n",
    "                contains = bool_contains\n",
    "    return contains\n",
    "\n",
    "def download_large_tile(coord, step_x, step_y, folder = OUTPUT_FOLDER, year = year, s1_layer = \"SENT\"):\n",
    "    \n",
    "    bbx, epsg = calculate_bbx_pyproj(coord, step_x, step_y, expansion = 80)\n",
    "    dem_bbx, _ = calculate_bbx_pyproj(coord, step_x, step_y, expansion = 90)\n",
    "    idx = str(step_y) + \"_\" + str(step_x)\n",
    "    idx = str(idx)\n",
    "    make_output_and_temp_folders(idx)\n",
    "\n",
    "    print(\"Calculating cloud cover\")\n",
    "    if not os.path.exists(folder + \"output/\" + str(((step_y+1)*5)-5) + \"/\" + str(((step_x+1)*5)-5) + \".npy\"):\n",
    "        if not os.path.exists(folder + \"processed/\" + str(((step_y+1)*5)-5) + \"/\" + str(((step_x+1)*5)-5) + \".hkl\"):\n",
    "            if not os.path.exists(folder + \"raw/clouds/clouds_{}.hkl\".format(idx)):\n",
    "                cloud_probs, shadows, clean_steps = identify_clouds_new(bbx, epsg = epsg)\n",
    "                hkl.dump(cloud_probs, folder + \"raw/clouds/clouds_{}.hkl\".format(idx), mode='w', compression='gzip')\n",
    "                hkl.dump(shadows, folder + \"raw/clouds/shadows_{}.hkl\".format(idx), mode='w', compression='gzip')\n",
    "                hkl.dump(clean_steps, folder + \"raw/clouds/clean_steps_{}.hkl\".format(idx), mode='w', compression='gzip')\n",
    "            \n",
    "            if not os.path.exists(folder + \"raw/s1/{}.hkl\".format(idx)):\n",
    "                print(\"Downloading S1\")\n",
    "                s1_layer = identify_s1_layer((coord[1], coord[0]))\n",
    "                s1, s1_dates = download_sentinel_1(bbx, layer = s1_layer, epsg = epsg)\n",
    "                if s1.shape[0] == 0:\n",
    "                    s1_layer = \"SENT_DESC\" if s1_layer == \"SENT\" else \"SENT\"\n",
    "                    print(f'Switching to {s1_layer}')\n",
    "                    s1, s1_dates = download_sentinel_1(bbx, layer = s1_layer, epsg = epsg)\n",
    "                s1 = process_sentinel_1_tile(s1, s1_dates)\n",
    "                hkl.dump(s1, folder + \"raw/s1/{}.hkl\".format(idx), mode='w', compression='gzip')\n",
    "                hkl.dump(s1_dates, folder + \"raw/misc/s1_dates_{}.hkl\".format(idx), mode='w', compression='gzip')\n",
    "\n",
    "            if not os.path.exists(folder + \"raw/s2/{}.hkl\".format(idx)):\n",
    "                print(\"Downloading S2\")\n",
    "                if 'clean_steps' not in globals() or locals():\n",
    "                    clean_steps = hkl.load(folder + \"raw/clouds/clean_steps_{}.hkl\".format(idx))\n",
    "                s2, s2_dates = download_layer(bbx, clean_steps = clean_steps, epsg = epsg)\n",
    "                hkl.dump(s2, folder + \"raw/s2/{}.hkl\".format(idx), mode='w', compression='gzip')\n",
    "                hkl.dump(s2_dates, folder + \"raw/misc/s2_dates_{}.hkl\".format(idx), mode='w', compression='gzip')\n",
    "\n",
    "            if not os.path.exists(folder + \"raw/misc/dem_{}.hkl\".format(idx)):\n",
    "                print(\"Downloading DEM\")\n",
    "                dem = download_dem(dem_bbx, epsg = epsg) # get the DEM BBOX\n",
    "                hkl.dump(dem, folder + \"raw/misc/dem_{}.hkl\".format(idx), mode='w', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def tile(h, w, tile_width=None, tile_height=None, window_size=100):\n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "    if not tile_width:\n",
    "        tile_width = window_size\n",
    "\n",
    "    if not tile_height:\n",
    "        tile_height = window_size\n",
    "\n",
    "    wTile = tile_width\n",
    "    hTile = tile_height\n",
    "\n",
    "    if tile_width > w or tile_height > h:\n",
    "        raise ValueError(\"tile dimensions cannot be larger than origin dimensions\")\n",
    "\n",
    "    # Number of tiles\n",
    "    nTilesX = np.uint8(np.ceil(w / wTile))\n",
    "    nTilesY = np.uint8(np.ceil(h / hTile))\n",
    "\n",
    "    # Total remainders\n",
    "    remainderX = nTilesX * wTile - w\n",
    "    remainderY = nTilesY * hTile - h\n",
    "\n",
    "    # Set up remainders per tile\n",
    "    remaindersX = np.ones((nTilesX-1, 1)) * np.uint16(np.floor(remainderX / (nTilesX-1)))\n",
    "    remaindersY = np.ones((nTilesY-1, 1)) * np.uint16(np.floor(remainderY / (nTilesY-1)))\n",
    "    remaindersX[0:np.remainder(remainderX, np.uint16(nTilesX-1))] += 1\n",
    "    remaindersY[0:np.remainder(remainderY, np.uint16(nTilesY-1))] += 1\n",
    "\n",
    "    # Initialize array of tile boxes\n",
    "    tiles = np.zeros((nTilesX * nTilesY, 4), np.uint16)\n",
    "\n",
    "    k = 0\n",
    "    x = 0\n",
    "    for i in range(nTilesX):\n",
    "        y = 0\n",
    "        for j in range(nTilesY):\n",
    "            tiles[k, :] = (x, y, hTile, wTile)\n",
    "            k += 1\n",
    "            if j < (nTilesY-1):\n",
    "                y = y + hTile - remaindersY[j]\n",
    "        if i < (nTilesX-1):\n",
    "            x = x + wTile - remaindersX[i]\n",
    "\n",
    "    return tiles\n",
    "\n",
    "def superresolve_tile(arr):\n",
    "    print(f\"The input array to superresolve is {arr.shape}\")\n",
    "    superresolved = np.copy(arr)\n",
    "    tiles = tile(646, 646, 56, 56)\n",
    "    for i in tnrange(len(tiles)):\n",
    "        subtile = tiles[i]\n",
    "        to_resolve = arr[:, subtile[0]:subtile[0]+56, subtile[1]:subtile[1]+56, :]\n",
    "        to_resolve = np.pad(to_resolve, ((0, 0), (4, 4), (4, 4), (0, 0)), 'reflect')\n",
    "        resolved = superresolve(to_resolve)\n",
    "        resolved = resolved[:, 4:-4, 4:-4, :]\n",
    "        superresolved[:, subtile[0]:subtile[0]+56, subtile[1]:subtile[1]+56] = resolved\n",
    "    return superresolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentinel_1_tile(sentinel1, dates):\n",
    "    s1, _ = calculate_and_save_best_images(sentinel1, dates)\n",
    "    biweekly_dates = np.array([day for day in range(0, 360, 5)])\n",
    "    to_remove = np.argwhere(biweekly_dates % 15 != 0)\n",
    "    s1 = np.delete(s1, to_remove, 0)\n",
    "    return s1\n",
    "\n",
    "def convert_to_int16(array):\n",
    "    return np.trunc(array * 65535).astype(int)\n",
    "\n",
    "\n",
    "def process_large_tile(coord, step_x, step_y, folder = OUTPUT_FOLDER):\n",
    "    idx = str(step_y) + \"_\" + str(step_x)\n",
    "    x_vals = []\n",
    "    y_vals = []\n",
    "    for i in range(25):\n",
    "        y_val = (24 - i) // 5\n",
    "        x_val = 5 - ((25 - i) % 5)\n",
    "        x_val = 0 if x_val == 5 else x_val\n",
    "        x_vals.append(x_val)\n",
    "        y_vals.append(y_val)\n",
    "    y_vals = [i + (5*step_y) for i in y_vals]\n",
    "    x_vals = [i + (5*step_x) for i in x_vals]\n",
    "\n",
    "    processed = True\n",
    "    for x, y in zip(x_vals, y_vals):\n",
    "        folder_path = f\"{str(y)}/{str(x)}\"\n",
    "        processed_exists = os.path.exists(folder + \"processed/\" + folder_path + \".hkl\")\n",
    "        output_exists = os.path.exists(folder + \"output/\" + folder_path + \".npy\")\n",
    "        print(output_exists)\n",
    "        if not (processed_exists or output_exists):\n",
    "            processed = False\n",
    "    if not processed:\n",
    "        clouds = hkl.load(folder + \"raw/clouds/clouds_{}.hkl\".format(idx))\n",
    "        sentinel1 = hkl.load(folder + \"raw/s1/{}.hkl\".format(idx))\n",
    "        radar_dates = hkl.load(folder + \"raw/misc/s1_dates_{}.hkl\".format(idx))\n",
    "        sentinel2 = hkl.load(folder + \"raw/s2/{}.hkl\".format(idx))\n",
    "        dem = hkl.load(folder + \"raw/misc/dem_{}.hkl\".format(idx))\n",
    "        image_dates = hkl.load(folder + \"raw/misc/s2_dates_{}.hkl\".format(idx))\n",
    "        if os.path.exists(folder + \"raw/clouds/shadows_{}.hkl\".format(idx)):\n",
    "            shadows = hkl.load(folder + \"raw/clouds/shadows_{}.hkl\".format(idx))\n",
    "        else:\n",
    "            print(\"No shadows file, so calculating shadows with L2A\")\n",
    "            shadows = mcm_shadow_mask(sentinel2, clouds)\n",
    "        print(\"The files have been loaded\")\n",
    "\n",
    "        to_remove = calculate_bad_steps(sentinel2, clouds)\n",
    "        sentinel2 = np.delete(sentinel2, to_remove, axis = 0)\n",
    "        clouds = np.delete(clouds, to_remove, axis = 0)\n",
    "        shadows = np.delete(shadows, to_remove, axis = 0)\n",
    "        image_dates = np.delete(image_dates, to_remove)\n",
    "        print(f\"{len(to_remove)} Cloudy and missing images removed, radar processed\")\n",
    "        to_remove = remove_missed_clouds(sentinel2)\n",
    "        print(f\"{len(to_remove)} missed cloudy images should have been removed\")\n",
    "\n",
    "        x = remove_cloud_and_shadows(sentinel2, clouds, shadows, image_dates)\n",
    "        print(\"Clouds and shadows interpolated\")       \n",
    "        \n",
    "        index = 0\n",
    "        print(\"Super resolving tile\")\n",
    "        x = superresolve_tile(x)\n",
    "        print(f\"The superresolved shape is: {x.shape}\")\n",
    "        \n",
    "        tiles = tile(IMSIZE, IMSIZE, window_size = 142)\n",
    "        for t in tiles:\n",
    "            start_x = t[0]\n",
    "            start_y = t[1]\n",
    "            end_x = start_x + t[2]\n",
    "            end_y = start_y + t[3]\n",
    "            print(index)\n",
    "            if not os.path.exists(folder + \"processed/{}/{}.hkl\".format(str(y_vals[index]), str(x_vals[index]))):\n",
    "                subtile = x[:, start_x:end_x, start_y:end_y, :]\n",
    "                dem_i = np.tile(dem[np.newaxis, start_x:end_x, start_y:end_y, :], (x.shape[0], 1, 1, 1))\n",
    "                subtile = np.concatenate([subtile, dem_i / 90], axis = -1)\n",
    "                subtile = evi(subtile, verbose = True)\n",
    "                subtile = bi(subtile, verbose = True)\n",
    "                subtile = msavi2(subtile, verbose = True)\n",
    "                subtile = si(subtile, verbose = True)\n",
    "                t3 = timer()\n",
    "\n",
    "                subtile, _ = calculate_and_save_best_images(subtile, image_dates)\n",
    "                subtile = interpolate_array(subtile, dim = 142)\n",
    "                t5 = timer()\n",
    "                print(\"Interpolate: {}\".format(t5 - t3))\n",
    "                subtile = np.concatenate([subtile, sentinel1[:, start_x:end_x,\n",
    "                                                            start_y:end_y, :]], axis = -1)\n",
    "\n",
    "                out_y_folder = folder + \"processed/{}/\".format(str(y_vals[index]))\n",
    "                if not os.path.exists(os.path.realpath(out_y_folder)):\n",
    "                    os.makedirs(os.path.realpath(out_y_folder))\n",
    "                subtile = convert_to_int16(subtile)\n",
    "                assert subtile.shape[1] == 142, f\"subtile shape is {subtile.shape}\"\n",
    "                hkl.dump(subtile,\n",
    "                         folder + \"processed/{}/{}.hkl\".format(str(y_vals[index]), str(x_vals[index])),\n",
    "                         mode='w', compression='gzip')\n",
    "            index += 1\n",
    "            \n",
    "def clean_up_folders():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-90.015579 13.727334\n",
      "-90.015579 13.727334\n",
      "15850\n"
     ]
    }
   ],
   "source": [
    "expansion = -10\n",
    "multiplier = 1\n",
    "step_x = 1\n",
    "coords_init = offset_x(coords, 0)\n",
    "coords_init[1] += 0\n",
    "\n",
    "coord1 = offset_x(coords, 6300*(step_x + multiplier) + expansion)\n",
    "coord1[1] += 6300*(step_x + multiplier) + expansion\n",
    "\n",
    "\n",
    "calculate_area([coords_init, coord1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "X: 0 Y:0\n",
      "Downloaded 1\n",
      "-90.015579 13.727334\n",
      "-90.015579 13.727334\n",
      "Calculating cloud cover\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "\n",
      "\n",
      "True\n",
      "X: 0 Y:1\n",
      "Downloaded 2\n",
      "-90.015579 13.727334\n",
      "-90.015579 13.727334\n",
      "Calculating cloud cover\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "\n",
      "\n",
      "True\n",
      "X: 0 Y:2\n",
      "Downloaded 3\n",
      "-90.015579 13.727334\n",
      "-90.015579 13.727334\n",
      "Calculating cloud cover\n",
      "The max clouds is 255\n",
      "(70, 40, 40)\n",
      "Cloud_probs used 0.14241536458333331 processing units\n",
      "Removing 29 from S2 download, saving 212.28 PU\n",
      "(41, 646, 646, 3)\n",
      "(41, 646, 646, 3)\n",
      "Shadows used: 65.26930236816406 processing units\n",
      "The max shadows is 1.0\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c84276deda3a43daa25dd49653fca764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=41), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(41, 81, 81)\n",
      "(41, 81, 81)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d507ea92a46740eda5fe60c7b1e677ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=41), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cloud probs: (41, 646, 646)\n",
      "Shadow shape (41, 646, 646)\n",
      "Downloading S1\n",
      "Loading formatted geocoded file...\n",
      "NA\n",
      "SENT_DESC\n",
      "The max s1 is 1.0\n",
      "Sentinel 1 used 65.79994710286458 PU for           31 out of 61 images\n",
      "Maximum time distance: 18\n",
      "Downloading S2\n",
      "Downloading L2A 20m layer\n",
      "The max 20m is 65535\n",
      "Original 20 meter bands size: (41, 323, 323, 6), using 32.63465118408203 PU\n",
      "Downloading L2A 10m layer\n",
      "The max 10m is 65535\n",
      "Original 20 meter bands size: (41, 646, 646, 4), using 87.02573649088541 PU\n",
      "Sentinel 2 used 119.66038767496744 PU\n",
      "Downloading DEM\n",
      "DEM used 3.183868408203125 processing units\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "The files have been loaded\n",
      "The percent cloud cover is [0.05747443 0.00122689 0.         0.         0.         0.\n",
      " 0.         0.00306722 0.         0.08818977 0.00682456 0.\n",
      " 0.01368987 0.         0.01694879 0.13178742 0.00828389 0.00510165\n",
      " 0.         0.06203453 0.0220097  0.         0.08094346 0.\n",
      " 0.06786704 0.04739334 0.01200289 0.         0.00755543 0.\n",
      " 0.05191989 0.01503177 0.         0.04244505 0.         0.00187867\n",
      " 0.         0.00805145 0.         0.         0.        ]\n",
      "The number of missing 0 is [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 4.02476780e-01 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 4.79253132e-06 0.00000000e+00 0.00000000e+00 4.79253132e-06\n",
      " 0.00000000e+00]\n",
      "The number of missing 1 is [0.00000000e+00 1.91701253e-05 0.00000000e+00 5.03215789e-05\n",
      " 6.70954385e-05 6.46991728e-05 5.75103758e-05 1.07831955e-04\n",
      " 4.55290475e-05 1.48089218e-03 1.24605814e-04 5.27178445e-05\n",
      " 2.63589223e-05 4.31327819e-05 1.91701253e-05 4.09521801e-03\n",
      " 7.18879698e-06 4.79253132e-06 6.03715170e-01 2.39626566e-06\n",
      " 1.67738596e-05 0.00000000e+00 1.19813283e-05 0.00000000e+00\n",
      " 2.87551879e-05 0.00000000e+00 1.43775940e-05 7.18879698e-06\n",
      " 1.43775940e-05 2.87551879e-05 1.43057060e-03 5.99066415e-05\n",
      " 5.03215789e-05 6.70954385e-05 6.94917041e-05 7.18879698e-05\n",
      " 5.99066415e-05 9.10580951e-05 5.51141102e-05 6.23029071e-05\n",
      " 5.75103758e-05]\n",
      "1 Cloudy and missing images removed, radar processed\n",
      "0 missed cloudy images should have been removed\n",
      "Interpolated 830764 px\n",
      "Clouds and shadows interpolated\n",
      "Super resolving tile\n",
      "The input array to superresolve is (40, 646, 646, 10)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a1a9097e97743308270bddb5ab93523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=144), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The superresolved shape is: (40, 646, 646, 10)\n",
      "0\n",
      "Maximum time distance: 80\n",
      "Interpolate: 4.471273519040551\n",
      "1\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.908862014010083\n",
      "2\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.8718532840139233\n",
      "3\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.995955638005398\n",
      "4\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.8192595510045066\n",
      "5\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.53988534596283\n",
      "6\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.870343841961585\n",
      "7\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.955218563962262\n",
      "8\n",
      "Maximum time distance: 80\n",
      "Interpolate: 4.19808948400896\n",
      "9\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.932360610982869\n",
      "10\n",
      "Maximum time distance: 80\n",
      "Interpolate: 4.327278662007302\n",
      "11\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.636234050965868\n",
      "12\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.6665253719547763\n",
      "13\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.996995337016415\n",
      "14\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.9051004810025916\n",
      "15\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.65870560600888\n",
      "16\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.67689248197712\n",
      "17\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.700568117958028\n",
      "18\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.795068937004544\n",
      "19\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.4277229380095378\n",
      "20\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.548893214960117\n",
      "21\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.6804424619767815\n",
      "22\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.9170613450114615\n",
      "23\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.7721412530518137\n",
      "24\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.484289203013759\n",
      "\n",
      "\n",
      "True\n",
      "X: 1 Y:0\n",
      "Downloaded 4\n",
      "-90.015579 13.727334\n",
      "-90.015579 13.727334\n",
      "Calculating cloud cover\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "\n",
      "\n",
      "True\n",
      "X: 1 Y:1\n",
      "Downloaded 5\n",
      "-90.015579 13.727334\n",
      "-90.015579 13.727334\n",
      "Calculating cloud cover\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "\n",
      "\n",
      "True\n",
      "X: 1 Y:2\n",
      "Downloaded 6\n",
      "-90.015579 13.727334\n",
      "-90.015579 13.727334\n",
      "Calculating cloud cover\n",
      "The max clouds is 255\n",
      "(70, 40, 40)\n",
      "Cloud_probs used 0.14241536458333331 processing units\n",
      "Removing 32 from S2 download, saving 234.24 PU\n",
      "(38, 646, 646, 3)\n",
      "(38, 646, 646, 3)\n",
      "Shadows used: 60.493499755859375 processing units\n",
      "The max shadows is 1.0\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac4e6a16fec4790b574ff0734d7918e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=38), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(38, 81, 81)\n",
      "(38, 81, 81)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0ddd41f75084ae6967ffc79f6f49a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=38), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cloud probs: (38, 646, 646)\n",
      "Shadow shape (38, 646, 646)\n",
      "Downloading S1\n",
      "NA\n",
      "SENT_DESC\n",
      "The max s1 is 1.0\n",
      "Sentinel 1 used 65.79994710286458 PU for           31 out of 61 images\n",
      "Maximum time distance: 48\n",
      "Downloading S2\n",
      "Downloading L2A 20m layer\n",
      "The max 20m is 65535\n",
      "Original 20 meter bands size: (38, 323, 323, 6), using 30.246749877929688 PU\n",
      "Downloading L2A 10m layer\n",
      "The max 10m is 65535\n",
      "Original 20 meter bands size: (38, 646, 646, 4), using 80.65799967447916 PU\n",
      "Sentinel 2 used 110.90474955240884 PU\n",
      "Downloading DEM\n",
      "DEM used 3.183868408203125 processing units\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "The files have been loaded\n",
      "The percent cloud cover is [0.0412589  0.00253046 0.         0.         0.01752149 0.\n",
      " 0.03443673 0.         0.05195583 0.         0.0713584  0.\n",
      " 0.         0.07166272 0.         0.00191701 0.17032177 0.01997767\n",
      " 0.         0.00061344 0.00678622 0.1436705  0.09424752 0.\n",
      " 0.00306722 0.01936183 0.         0.03270663 0.01556614 0.\n",
      " 0.12013438 0.         0.00954912 0.         0.0202101  0.00184033\n",
      " 0.00429411 0.00632614]\n",
      "The number of missing 0 is [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 4.79253132e-06\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 4.02476780e-01\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n",
      "The number of missing 1 is [3.11514536e-05 0.00000000e+00 4.55290475e-05 4.79253132e-05\n",
      " 1.79001045e-03 4.55290475e-05 3.11514536e-05 3.35477192e-05\n",
      " 2.07516606e-03 1.91701253e-05 5.63122430e-04 2.39626566e-06\n",
      " 7.18879698e-06 1.65581957e-03 0.00000000e+00 6.03715170e-01\n",
      " 4.45705413e-04 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 8.12334059e-04 4.55290475e-05\n",
      " 7.18879698e-06 0.00000000e+00 4.79253132e-06 7.30861026e-04\n",
      " 5.99066415e-05 3.11514536e-05 1.07113075e-02 3.83402506e-05\n",
      " 4.79253132e-05 3.35477192e-05 5.34367242e-04 3.11514536e-05\n",
      " 4.31327819e-05 0.00000000e+00]\n",
      "3 Cloudy and missing images removed, radar processed\n",
      "0 missed cloudy images should have been removed\n",
      "Interpolated 815278 px\n",
      "Clouds and shadows interpolated\n",
      "Super resolving tile\n",
      "The input array to superresolve is (35, 646, 646, 10)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5777eef5670c48cb922e49770cafe9f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=144), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The superresolved shape is: (35, 646, 646, 10)\n",
      "0\n",
      "Maximum time distance: 80\n",
      "Interpolate: 4.725006013002712\n",
      "1\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.62583385896869\n",
      "2\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.6554492409923114\n",
      "3\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.885673893033527\n",
      "4\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.808842757018283\n",
      "5\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.7305487309931777\n",
      "6\n",
      "Maximum time distance: 80\n",
      "Interpolate: 4.047335086972453\n",
      "7\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.7333681469899602\n",
      "8\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.7131123160361312\n",
      "9\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.547887587978039\n",
      "10\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.7256765870261006\n",
      "11\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.8841587170027196\n",
      "12\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.8881485969759524\n",
      "13\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.861227068991866\n",
      "14\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.767009272996802\n",
      "15\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.884912050969433\n",
      "16\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.8032415700145066\n",
      "17\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.663082270009909\n",
      "18\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.8293812589836307\n",
      "19\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.77699407300679\n",
      "20\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.7840754179633223\n",
      "21\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.6607823909725994\n",
      "22\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.7485860799788497\n",
      "23\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.614892806042917\n",
      "24\n",
      "Maximum time distance: 80\n",
      "Interpolate: 3.6775199419935234\n",
      "\n",
      "\n",
      "True\n",
      "X: 2 Y:0\n",
      "Downloaded 7\n",
      "-90.015579 13.727334\n",
      "-90.015579 13.727334\n",
      "Calculating cloud cover\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "\n",
      "\n",
      "True\n",
      "X: 2 Y:1\n",
      "Downloaded 8\n",
      "-90.015579 13.727334\n",
      "-90.015579 13.727334\n",
      "Calculating cloud cover\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "\n",
      "\n",
      "True\n",
      "X: 2 Y:2\n",
      "Downloaded 9\n",
      "-90.015579 13.727334\n",
      "-90.015579 13.727334\n",
      "Calculating cloud cover\n",
      "The max clouds is 255\n",
      "(70, 40, 40)\n",
      "Cloud_probs used 0.14241536458333331 processing units\n",
      "Removing 33 from S2 download, saving 241.56 PU\n",
      "(37, 646, 646, 3)\n",
      "(37, 646, 646, 3)\n",
      "Shadows used: 58.90156555175781 processing units\n",
      "The max shadows is 1.0\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6d79ecdc3949dabee5759e8e11d15f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=37), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(37, 81, 81)\n",
      "(37, 81, 81)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e02baee60d2648fc8d3ec6f073ade0a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=37), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cloud probs: (37, 646, 646)\n",
      "Shadow shape (37, 646, 646)\n",
      "Downloading S1\n",
      "NA\n",
      "SENT_DESC\n",
      "The max s1 is 1.0\n",
      "Sentinel 1 used 65.79994710286458 PU for           31 out of 61 images\n",
      "Switching to SENT\n",
      "The max s1 is 1.0\n",
      "Sentinel 1 used 65.79994710286458 PU for           31 out of 61 images\n",
      "Maximum time distance: 0\n",
      "Downloading S2\n",
      "Downloading L2A 20m layer\n",
      "The max 20m is 65535\n",
      "Original 20 meter bands size: (37, 323, 323, 6), using 29.450782775878906 PU\n",
      "Downloading L2A 10m layer\n",
      "The max 10m is 65535\n",
      "Original 20 meter bands size: (37, 646, 646, 4), using 78.53542073567708 PU\n",
      "Sentinel 2 used 107.98620351155598 PU\n",
      "Downloading DEM\n",
      "DEM used 3.183868408203125 processing units\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "The files have been loaded\n",
      "The percent cloud cover is [0.08730794 0.01188787 0.00736133 0.00061344 0.06303616 0.00061344\n",
      " 0.08339963 0.00490755 0.0373434  0.00061344 0.00061344 0.00122689\n",
      " 0.07833632 0.11962637 0.04926962 0.00122689 0.03823242 0.00122689\n",
      " 0.1444421  0.00559768 0.08389805 0.13784758 0.13427954 0.01560448\n",
      " 0.03631301 0.04451543 0.06997815 0.03619559 0.02818248 0.\n",
      " 0.06510893 0.00061344 0.00245378 0.00061344 0.00061344 0.00513999\n",
      " 0.10697888]\n",
      "The number of missing 0 is [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.40247678 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.        ]\n",
      "The number of missing 1 is [8.38692981e-05 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 4.31327819e-05 0.00000000e+00 4.07365162e-05 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 4.69668069e-04 5.50422222e-03 0.00000000e+00 0.00000000e+00\n",
      " 6.03715170e-01 0.00000000e+00 2.39626566e-06 0.00000000e+00\n",
      " 1.36587143e-04 0.00000000e+00 3.35477192e-05 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 1.91940879e-03 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 4.52894210e-03 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 3.61836115e-04]\n",
      "2 Cloudy and missing images removed, radar processed\n",
      "0 missed cloudy images should have been removed\n"
     ]
    }
   ],
   "source": [
    "downloaded = 0\n",
    "\n",
    "if not os.path.exists(os.path.realpath(OUTPUT_FOLDER)):\n",
    "            os.makedirs(os.path.realpath(OUTPUT_FOLDER))\n",
    "        \n",
    "for x_tile in range(0, 3):\n",
    "    for y_tile in range(0, 3):\n",
    "        contains = True\n",
    "        #contains = check_contains(coords, x_tile, y_tile, OUTPUT_FOLDER)\n",
    "        print(contains)\n",
    "        if contains:\n",
    "            print(\"X: {} Y:{}\".format(x_tile, y_tile))\n",
    "            downloaded += 1\n",
    "            print(f\"Downloaded {downloaded}\")\n",
    "            download_large_tile(coord = coords, step_x = x_tile, step_y = y_tile)\n",
    "            process_large_tile(coords, x_tile, y_tile)\n",
    "            print(\"\\n\")\n",
    "            #clean_up_folders(x_tile, y_tile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_sensing",
   "language": "python",
   "name": "remote_sensing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
