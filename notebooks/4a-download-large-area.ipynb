{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/john.brandt/anaconda3/envs/remote_sensing/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from osgeo import ogr, osr\n",
    "from sentinelhub import WmsRequest, WcsRequest, MimeType, CRS, BBox, constants\n",
    "from s2cloudless import S2PixelCloudDetector, CloudMaskRequest\n",
    "import logging\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import os\n",
    "import yaml\n",
    "from sentinelhub import DataSource\n",
    "import scipy.sparse as sparse\n",
    "import scipy\n",
    "from scipy.sparse.linalg import splu\n",
    "from skimage.transform import resize\n",
    "from sentinelhub import CustomUrlParam\n",
    "from time import time as timer\n",
    "from time import sleep as sleep\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/john.brandt/anaconda3/envs/remote_sensing/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%run ../src/slope.py\n",
    "%run ../src/utils-bilinear.py\n",
    "%run ../src/dsen2/utils/DSen2Net.py\n",
    "#!source ~/.bash_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ('2018-12-15', '2020-01-15')\n",
    "SIZE = 9*5\n",
    "IMSIZE = (SIZE * 14)+2\n",
    "\n",
    "cloud_detector = S2PixelCloudDetector(threshold=0.4, average_over=4, dilation_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions (to be moved to a utils file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSG = CRS.WGS84\n",
    "GRID_SIZE_X = 1\n",
    "GRID_SIZE_Y = 1\n",
    "\n",
    "IMAGE_X = 14*GRID_SIZE_X\n",
    "IMAGE_Y = 14*GRID_SIZE_Y\n",
    "\n",
    "TEST_X = 5\n",
    "TEST_Y = 5\n",
    "\n",
    "with open(\"../config.yaml\", 'r') as stream:\n",
    "    key = (yaml.safe_load(stream))\n",
    "    API_KEY = key['key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These arrays are for smoothly overlapping the cloud and shadow interpolation\n",
    "c_arr = np.array([[1, 1, 1, 1, 1,],\n",
    "                  [1, 2, 2, 2, 1,],\n",
    "                  [1, 2, 3, 2, 1,],\n",
    "                  [1, 2, 2, 2, 1,],\n",
    "                  [1, 1, 1, 1, 1,],])\n",
    "                  \n",
    "c_arr = c_arr / 3\n",
    "o_arr = 1 - c_arr\n",
    "c_arr = np.tile(c_arr[:, :, np.newaxis], (1, 1, 10))\n",
    "o_arr = np.tile(o_arr[:, :, np.newaxis], (1, 1, 10))\n",
    "\n",
    "def convertCoords(xy, src='', targ=''):\n",
    "\n",
    "    srcproj = osr.SpatialReference()\n",
    "    srcproj.ImportFromEPSG(src)\n",
    "    targproj = osr.SpatialReference()\n",
    "    if isinstance(targ, str):\n",
    "        targproj.ImportFromProj4(targ)\n",
    "    else:\n",
    "        targproj.ImportFromEPSG(targ)\n",
    "    transform = osr.CoordinateTransformation(srcproj, targproj)\n",
    "\n",
    "    pt = ogr.Geometry(ogr.wkbPoint)\n",
    "    pt.AddPoint(xy[0], xy[1])\n",
    "    pt.Transform(transform)\n",
    "    return([pt.GetX(), pt.GetY()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_epsg(points):\n",
    "    lon, lat = points[0], points[1]\n",
    "    print(lon, lat)\n",
    "    utm_band = str((math.floor((lon + 180) / 6 ) % 60) + 1)\n",
    "    if len(utm_band) == 1:\n",
    "        utm_band = '0'+utm_band\n",
    "    if lat >= 0:\n",
    "        epsg_code = '326' + utm_band\n",
    "    else:\n",
    "        epsg_code = '327' + utm_band\n",
    "    return int(epsg_code)\n",
    "    \n",
    "def offset_x(coord, offset):\n",
    "    ''' Converts a WGS 84 to UTM, adds meters, and converts back'''\n",
    "    epsg = calculate_epsg(coord)\n",
    "    coord = convertCoords(coord, 4326, epsg)\n",
    "    coord[0] += offset\n",
    "    coord = convertCoords(coord, epsg, 4326)\n",
    "    return coord\n",
    "    \n",
    "def offset_y(coord, offset):\n",
    "    ''' Converts a WGS 84 to UTM, adds meters, and converts back'''\n",
    "    epsg = calculate_epsg(coord)\n",
    "    coord = convertCoords(coord, 4326, epsg)\n",
    "    coord[1] += offset\n",
    "    coord = convertCoords(coord, epsg, 4326)\n",
    "    return coord\n",
    "\n",
    "def calculate_bbx(coord, step_x, step_y, expansion):\n",
    "    ''' Calculates the four corners of a bounding box of step_x * step_y offset from coord'''\n",
    "    coord_bl = np.copy(coord)\n",
    "    coord1 = offset_x(coord_bl, 6300*step_x - expansion)\n",
    "    coord1 = offset_y(coord1 , 6300*step_y - expansion)\n",
    "    \n",
    "    coord_tr = np.copy(coord)\n",
    "    coord2 = offset_x(coord_tr, 6300*(step_x + 1) + expansion)\n",
    "    coord2 = offset_y(coord2, 6300*(step_y + 1) + expansion)\n",
    "    bbx = (coord2, coord1)\n",
    "    return bbx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2018-12-15', '2020-01-15')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_function(func, n_tries = 5, *args, **kwargs):\n",
    "    for try_ in range(0, n_tries):\n",
    "        try:\n",
    "            returns = func(*args, **kwargs)\n",
    "        except Exception:\n",
    "            print(\"Trying again in 20 seconds\")\n",
    "            sleep(20)\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_clouds(bbox, epsg = EPSG, dates = dates):\n",
    "    '''\n",
    "    - Download the CLOUD_DETECTION layer from sentinel-hub\n",
    "    - Process cloud probabiiliity\n",
    "    - Return cloud probability map\n",
    "    '''\n",
    "    for try_ in range(0, 5):\n",
    "        try:\n",
    "            box = BBox(bbox, crs = epsg)\n",
    "            cloud_request = WmsRequest(\n",
    "                layer='CLOUD_DETECTION',\n",
    "                bbox=box,\n",
    "                time=dates,\n",
    "                width=(5*9*14)+2,\n",
    "                height=(5*9*14)+2,\n",
    "                image_format = MimeType.TIFF_d32f,\n",
    "                maxcc=0.7,\n",
    "                instance_id=API_KEY,\n",
    "                custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "                time_difference=datetime.timedelta(hours=48),\n",
    "            )\n",
    "\n",
    "            cloud_img = cloud_request.get_data()\n",
    "            cloud_probs = cloud_detector.get_cloud_probability_maps(np.array(cloud_img))\n",
    "            shadows = mcm_shadow_mask(np.array(cloud_img), cloud_probs)\n",
    "            print(\"Cloud probs: {}\".format(cloud_probs.shape))\n",
    "            return cloud_img, cloud_probs, shadows\n",
    "        except Exception as e:\n",
    "            #logging.fatal(e, exc_info=True)\n",
    "            sleep(30)\n",
    "    \n",
    "    \n",
    "def download_dem(bbox, epsg = EPSG):\n",
    "    #@valid\n",
    "    for try_ in range(0, 5):\n",
    "        try:\n",
    "            box = BBox(bbox, crs = epsg)\n",
    "            dem_s = (630)+4\n",
    "            dem_request = WmsRequest(data_source=DataSource.DEM,\n",
    "                                 layer='DEM',\n",
    "                                 bbox=box,\n",
    "                                 width=dem_s,\n",
    "                                 height=dem_s,\n",
    "                                 instance_id=API_KEY,\n",
    "                                 image_format=MimeType.TIFF_d32f,\n",
    "                                 custom_url_params={CustomUrlParam.SHOWLOGO: False})\n",
    "            dem_image = dem_request.get_data()[0]\n",
    "            dem_image = calcSlope(dem_image.reshape((1, dem_s, dem_s)),\n",
    "                          np.full((dem_s, dem_s), 10), np.full((dem_s, dem_s), 10), zScale = 1, minSlope = 0.02)\n",
    "            dem_image = dem_image.reshape((dem_s,dem_s, 1))\n",
    "            dem_image = dem_image[1:dem_s-1, 1:dem_s-1, :]\n",
    "            return dem_image #/ np.max(dem_image)\n",
    "        except Exception as e:\n",
    "            #logging.fatal(e, exc_info=True)\n",
    "            sleep((try_+1)*30)\n",
    "            pass\n",
    "\n",
    "def download_layer(bbox, epsg = EPSG, dates = dates, year = 2019):\n",
    "    for try_ in range(5):\n",
    "        try:\n",
    "            box = BBox(bbox, crs = epsg)\n",
    "            image_request = WcsRequest(\n",
    "                    layer='L2A20',\n",
    "                    bbox=box,\n",
    "                    time=dates,\n",
    "                    image_format = MimeType.TIFF_d32f,\n",
    "                    maxcc=0.7,\n",
    "                    resx='10m', resy='10m',\n",
    "                    instance_id=API_KEY,\n",
    "                    custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'NEAREST',\n",
    "                                        constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "                    time_difference=datetime.timedelta(hours=48),\n",
    "                )\n",
    "            img_bands = image_request.get_data()\n",
    "            img_20 = np.stack(img_bands)\n",
    "            print(\"Original size: {}\".format(img_20.shape))\n",
    "            img_20 = resize(img_20, (img_20.shape[0], 632, 632, img_20.shape[-1]), order = 0)\n",
    "\n",
    "            image_request = WcsRequest(\n",
    "                    layer='L2A10',\n",
    "                    bbox=box,\n",
    "                    time=dates,\n",
    "                    image_format = MimeType.TIFF_d32f,\n",
    "                    maxcc=0.7,\n",
    "                    resx='10m', resy='10m',\n",
    "                    instance_id=API_KEY,\n",
    "                    custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'BICUBIC',\n",
    "                                        constants.CustomUrlParam.UPSAMPLING: 'BICUBIC'},\n",
    "                    time_difference=datetime.timedelta(hours=48),\n",
    "            )\n",
    "\n",
    "            img_bands = image_request.get_data()\n",
    "            img_10 = np.stack(img_bands)\n",
    "            print(\"Original 10 size: {}\".format(img_10.shape))\n",
    "            img_10 = resize(img_10, (img_10.shape[0], 632, 632, img_10.shape[-1]), order = 0)\n",
    "            #shadows = img_10[:, :, :, -1]\n",
    "            #img_10 = img_10[:, :, :, :-1]\n",
    "\n",
    "            #shadows[np.where(shadows != 3)] = 0\n",
    "            #shadows[np.where(shadows == 3)] = 1\n",
    "            #print(\"Data shape: {}\".format(shadows.shape))\n",
    "            #shadow_sum = np.sum(shadows, axis = (1, 2))\n",
    "            #shadow_steps = np.argwhere(shadow_sum > (IMSIZE*IMSIZE) / 4)\n",
    "\n",
    "            img = np.concatenate([img_10, img_20], axis = -1)\n",
    "\n",
    "            image_dates = []\n",
    "            for date in image_request.get_dates():\n",
    "                if date.year == year - 1:\n",
    "                    image_dates.append(-365 + starting_days[(date.month-1)] + date.day)\n",
    "                if date.year == year:\n",
    "                    image_dates.append(starting_days[(date.month-1)] + date.day)\n",
    "                if date.year == year + 1:\n",
    "                    image_dates.append(365 + starting_days[(date.month-1)]+date.day)\n",
    "            image_dates = np.array(image_dates)\n",
    "\n",
    "            return img, image_dates#, shadows, shadow_steps\n",
    "\n",
    "        except Exception as e:\n",
    "            #logging.fatal(e, exc_info=True)\n",
    "            sleep((try_+1)*30)\n",
    "        \n",
    "        \n",
    "        \n",
    "def download_sentinel_1(bbox, epsg = EPSG, dates = dates, layer = \"SENT\", year = 2019):\n",
    "    for try_ in range(5):\n",
    "        try:\n",
    "            box = BBox(bbox, crs = epsg)\n",
    "            image_request = WcsRequest(\n",
    "                    layer=layer,\n",
    "                    bbox=box,\n",
    "                    time=dates,\n",
    "                    image_format = MimeType.TIFF_d32f,\n",
    "                    maxcc=1.0,\n",
    "                    resx='5m', resy='5m',\n",
    "                    instance_id=API_KEY,\n",
    "                    custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'NEAREST',\n",
    "                                        constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "                    time_difference=datetime.timedelta(hours=48),\n",
    "                )\n",
    "            img_bands = image_request.get_data()\n",
    "            s1 = np.stack(img_bands)\n",
    "            s1 = resize(s1, (s1.shape[0], 632*2, 632*2, s1.shape[-1]), order = 0)\n",
    "            s1 = np.reshape(s1, (s1.shape[0], s1.shape[1]//2, 2, s1.shape[2] // 2, 2, s1.shape[-1]))\n",
    "            s1 = np.mean(s1, (2, 4))\n",
    "            #s1 = s1[:, 8:24, 8:24, :]\n",
    "\n",
    "            image_dates = []\n",
    "            for date in image_request.get_dates():\n",
    "                if date.year == year - 1:\n",
    "                    image_dates.append(-365 + starting_days[(date.month-1)] + date.day)\n",
    "                if date.year == year:\n",
    "                    image_dates.append(starting_days[(date.month-1)] + date.day)\n",
    "                if date.year == year + 1:\n",
    "                    image_dates.append(365 + starting_days[(date.month-1)]+date.day)\n",
    "            image_dates = np.array(image_dates)\n",
    "            to_remove = np.argwhere(np.max(s1, (1, 2, 3)) == 1.).flatten()\n",
    "            s1 = np.delete(s1, to_remove, 0)\n",
    "            #print(np.max(s1, (1, 2, 3)))\n",
    "            image_dates = np.delete(image_dates, to_remove)\n",
    "            return s1, image_dates\n",
    "\n",
    "        except Exception as e:\n",
    "            #logging.fatal(e, exc_info=True)\n",
    "            sleep((try_+1)*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud and shadow removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_cloud_and_shadows(tiles, c_probs, shadows, image_dates):\n",
    "    '''\n",
    "    - Iterate through a window size over each time step\n",
    "    - Identify whether or not there are clouds and shadows there\n",
    "    - If there are, interpolate the 5*5 window with clean imagery for all bands\n",
    "    '''\n",
    "    wsize = 5\n",
    "    c_probs = c_probs - np.min(c_probs, axis = 0)\n",
    "    c_probs[np.where(c_probs > 0.33)] = 1.\n",
    "    c_probs[np.where(c_probs < 0.33)] = 0.\n",
    "    c_probs = np.reshape(c_probs, (c_probs.shape[0], 632//8, 8, 632//8, 8))\n",
    "    c_probs = np.sum(c_probs, (2, 4))\n",
    "    c_probs = resize(c_probs, (c_probs.shape[0], 632, 632), 0)\n",
    "    c_probs[np.where(c_probs < 16)] = 0\n",
    "    c_probs[np.where(c_probs >= 16)] = 1\n",
    "    secondary_c_probs = np.copy(c_probs)\n",
    "    c_probs += shadows\n",
    "    c_probs[np.where(c_probs >= 1.)] = 1.\n",
    "    number_interpolated = 0\n",
    "    for cval in tnrange(0, IMSIZE - 4, 2):\n",
    "        for rval in range(0, IMSIZE - 4, 2):\n",
    "            subs = c_probs[:, cval:cval + wsize, rval:rval+wsize]\n",
    "            sums = np.sum(subs, axis = (1, 2))\n",
    "            satisfactory = [x for x in range(c_probs.shape[0]) if sums[x] < 8]\n",
    "            satisfactory = np.array(satisfactory)\n",
    "            for date in range(0, tiles.shape[0]):\n",
    "                if np.sum(subs[date, :, :]) > 8:\n",
    "                    number_interpolated += 1\n",
    "                    before, after = calculate_proximal_steps(date, satisfactory)\n",
    "                    before = date + before\n",
    "                    after = date + after\n",
    "                    bef = tiles[before, cval:cval+wsize, rval:rval+wsize, : ]\n",
    "                    aft = tiles[after, cval:cval+wsize, rval:rval+wsize, : ]\n",
    "                    before = image_dates[before]\n",
    "                    after = image_dates[after]\n",
    "                    before_diff = abs(image_dates[date] - before)\n",
    "                    after_diff = abs(image_dates[date] - after)\n",
    "                    bef_wt = 1 - before_diff / (before_diff + after_diff)\n",
    "                    aft_wt = 1 - bef_wt\n",
    "                    candidate = bef_wt*bef + aft_wt*aft\n",
    "                    candidate = candidate*c_arr + tiles[date, cval:cval+wsize, rval:rval+wsize, : ]*o_arr\n",
    "                    tiles[date, cval:cval+wsize, rval:rval+wsize, : ] = candidate \n",
    "    print(\"A total of {} pixels were interpolated\".format(number_interpolated))\n",
    "    return tiles, c_probs, secondary_c_probs\n",
    "\n",
    "def remove_missed_clouds(img):\n",
    "    iqr = np.percentile(img[:, :, :, 3].flatten(), 75) - np.percentile(img[:, :, :, 3].flatten(), 25)\n",
    "    thresh_t = np.percentile(img[:, :, :, 3].flatten(), 75) + iqr*2\n",
    "    thresh_b = np.percentile(img[:, :, :, 3].flatten(), 25) - iqr*2\n",
    "    diffs_fw = np.diff(img, 1, axis = 0)\n",
    "    diffs_fw = np.mean(diffs_fw, axis = (1, 2, 3))\n",
    "    diffs_fw = np.array([0] + list(diffs_fw))\n",
    "    diffs_bw = np.diff(np.flip(img, 0), 1, axis = 0)\n",
    "    diffs_bw = np.flip(np.mean(diffs_bw, axis = (1, 2, 3)))\n",
    "    diffs_bw = np.array(list(diffs_bw) + [0])\n",
    "    diffs = abs(diffs_fw - diffs_bw) * 100 # 3, -3 -> 6, -3, 3 -> 6, -3, -3\n",
    "    #diffs = [int(x) for x in diffs]\n",
    "    outlier_percs = []\n",
    "    for step in range(img.shape[0]):\n",
    "        bottom = len(np.argwhere(img[step, :, :, 3].flatten() > thresh_t))\n",
    "        top = len(np.argwhere(img[step, :, :, 3].flatten() < thresh_b))\n",
    "        p = 100* ((bottom + top) / (IMSIZE*IMSIZE))\n",
    "        outlier_percs.append(p)\n",
    "    to_remove = np.argwhere(np.array(outlier_percs) > 20)\n",
    "    return to_remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagonals = np.zeros(2*2+1)\n",
    "diagonals[2] = 1.\n",
    "for i in range(2):\n",
    "    diff = diagonals[:-1] - diagonals[1:]\n",
    "    diagonals = diff\n",
    "offsets = np.arange(2+1)\n",
    "shape = (70, 72)\n",
    "\n",
    "def smooth(y, lmbd = 800, diagonals = diagonals, offsets = offsets, shape = shape, d = 2):\n",
    "    ''' \n",
    "    Apply whittaker smoothing to a 1-dimensional array, returning a 1-dimensional array\n",
    "    '''\n",
    "    E = sparse.eye(72, format = 'csc')\n",
    "    D = scipy.sparse.diags(diagonals, offsets, shape)\n",
    "    coefmat = E + lmbd * D.conj().T.dot(D)\n",
    "    z = splu(coefmat).solve(y)\n",
    "    return z\n",
    "\n",
    "def calculate_and_save_best_images(img_bands, image_dates):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "\n",
    "    biweekly_dates = [day for day in range(0, 360, 5)] # ideal imagery dates are every 15 days\n",
    "    \n",
    "    # Identify the dates where there is < 20% cloud cover\n",
    "    #satisfactory_ids = list(np.argwhere(np.array(means) < 4.).reshape(-1, )) \n",
    "    satisfactory_ids = [x for x in range(0, img_bands.shape[0])]\n",
    "    satisfactory_dates = [value for idx, value in enumerate(image_dates) if idx in satisfactory_ids]\n",
    "    \n",
    "    \n",
    "    selected_images = {}\n",
    "    for i in biweekly_dates:\n",
    "        distances = [abs(date - i) for date in satisfactory_dates]\n",
    "        closest = np.min(distances)\n",
    "        closest_id = np.argmin(distances)\n",
    "        # If there is imagery within 8 days, select it\n",
    "        if closest < 8:\n",
    "            date = satisfactory_dates[closest_id]\n",
    "            image_idx = int(np.argwhere(np.array(image_dates) == date)[0])\n",
    "            selected_images[i] = {'image_date': [date], 'image_ratio': [1], 'image_idx': [image_idx]}\n",
    "        # If there is not imagery within 8 days, look for the closest above and below imagery\n",
    "        else:\n",
    "            distances = np.array([(date - i) for date in satisfactory_dates])\n",
    "            # Number of days above and below the selected date of the nearest clean imagery\n",
    "            above = distances[np.where(distances < 0, distances, -np.inf).argmax()]\n",
    "            below = distances[np.where(distances > 0, distances, np.inf).argmin()]\n",
    "            if abs(above) > 240: # If date is the last date, occassionally argmax would set above to - number\n",
    "                above = below\n",
    "            if abs(below) > 240:\n",
    "                below = above\n",
    "            if above != below:\n",
    "                below_ratio = above / (above - below)\n",
    "                above_ratio = 1 - below_ratio\n",
    "            else:\n",
    "                above_ratio = below_ratio = 0.5\n",
    "                \n",
    "            # Extract the image date and imagery index for the above and below values\n",
    "            above_date = i + above\n",
    "            above_image_idx = int(np.argwhere(np.array(image_dates) == above_date)[0])\n",
    "            \n",
    "            below_date = i + below\n",
    "            below_image_idx = int(np.argwhere(np.array(image_dates) == below_date)[0])\n",
    "            \n",
    "            selected_images[i] = {'image_date': [above_date, below_date], 'image_ratio': [above_ratio, below_ratio],\n",
    "                                 'image_idx': [above_image_idx, below_image_idx]}\n",
    "                            \n",
    "    max_distance = 0\n",
    "    \n",
    "    for i in selected_images.keys():\n",
    "        #print(i, selected_images[i])\n",
    "        if len(selected_images[i]['image_date']) == 2:\n",
    "            dist = selected_images[i]['image_date'][1] - selected_images[i]['image_date'][0]\n",
    "            if dist > max_distance:\n",
    "                max_distance = dist\n",
    "    \n",
    "    print(\"Maximum time distance: {}\".format(max_distance))\n",
    "        \n",
    "    keep_steps = []\n",
    "    for i in selected_images.keys():\n",
    "        info = selected_images[i]\n",
    "        if len(info['image_idx']) == 1:\n",
    "            step = img_bands[info['image_idx'][0]]\n",
    "        if len(info['image_idx']) == 2:\n",
    "            step1 = img_bands[info['image_idx'][0]] * 0.5#info['image_ratio'][0]\n",
    "            step2 = img_bands[info['image_idx'][1]] * 0.5 #info['image_ratio'][1]\n",
    "            step = step1 + step2\n",
    "        keep_steps.append(step)\n",
    "        \n",
    "    keep_steps = np.stack(keep_steps)\n",
    "    return keep_steps\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiling and coordinate selection functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coords = (13.540810, 38.177220) # tigray\n",
    "#coords = (-1.817109, 37.477563) # makueni-2\n",
    "#coords = (-2.575694, 37.949516) # makueni-3\n",
    "#coords = (-2.561161, 38.096274) # makueni\n",
    "#coords = (9.259359, -0.833750) # ghana\n",
    "#coords = (-1.515869, 29.952997) # rwanda - useable\n",
    "#coords = (-1.455224, 30.323259) # rwanda2\n",
    "#coords = (13.316919, 2.581680) # niger\n",
    "#coords = (13.18158333, 2.47805556) # niger - koure salima\n",
    "#coords = (10.596, 14.2722) # cameroon\n",
    "#coords = (18.232495, -92.134215) # campeche\n",
    "#coords = (14.231732, -89.418679) # el salvador\n",
    "#coords = (-11.044091, 33.818034) # malawi\n",
    "#coords = (10.385811, -1.764760) # sisala east, ghana\n",
    "#coords = (10.390084, -0.846330) # weest mamprusi, ghana\n",
    "#coords = (7.702058, -0.709011) # brong ahafo, bono east\n",
    "#coords = (10.097017, -2.439068)# close to wa, has been done\n",
    "#coords = (24.070469, 81.606926) # sidhi, india\n",
    "coords = (7.398111, -1.269223) # cocoa\n",
    "#coords = (44.865106, -123.093435) # salem, oregon\n",
    "#coords = (-20.147326, -40.837780) # Esperito santo, BR\n",
    "#coords = (-20.147320, -40.837770) BR 2\n",
    "#coords = (-22.559943, -44.186629) # Vale do Paraiba, Brazil\n",
    "#coords = (6.622101, -0.704616) # kwahu\n",
    "#coords = (6.518909, -0.826008) # kwahu large\n",
    "#coords = (-6.352580, 106.677072) # jakarta\n",
    "#coords = (6.167177, -75.693226) # medellin, colombia\n",
    "#coords = (4.179529, -74.889171) # colombia\n",
    "#coords = (6.518909, -0.826008) # kwahu large\n",
    "#coords = (5.765917, 14.791618) # baboua, CAF\n",
    "#coords = (-18.960152, 47.469587) # madagascar\n",
    "#coords = (9.909083, 76.253594) # Kochi, india\n",
    "#coords = (16.032170, -90.144511) # Guatemala\n",
    "#coords = (13.757749, -90.004949) # elsalvador imposible\n",
    "#coords = (13.727334, -90.015579) # elsalvador imposible2\n",
    "#coords = (-11.749636, 27.586622) # Kafubu, DRC\n",
    "#coords = (-6.272258, 36.679824) # Tanzania\n",
    "#coords = (-36.431237, -71.872030) # Chile\n",
    "#coords = (12.398014, -86.963042) # Nicaragua\n",
    "#coords = (13.933745, -84.690842) # Bonanza, Nicaragua\n",
    "coords = (14.096664, -88.720304) # Honduras\n",
    "coords = (15.825856, -15.341662) # senegal\n",
    "coords = (coords[1], coords[0])\n",
    "OUTPUT_FOLDER = '../tile_data/senegal2/'\n",
    "#13.567962754335872\n",
    "\n",
    "#borders = bounding_box(coords, 10*(SIZE*14), 10*(SIZE*14), expansion = 0)\n",
    "#print(borders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbolic Model Created.\n"
     ]
    }
   ],
   "source": [
    "MDL_PATH = \"../src/dsen2/models/\"\n",
    "\n",
    "input_shape = ((4, None, None), (6, None, None))\n",
    "model = s2model(input_shape, num_layers=6, feature_size=128)\n",
    "predict_file = MDL_PATH+'s2_032_lr_1e-04.hdf5'\n",
    "print('Symbolic Model Created.')\n",
    "\n",
    "model.load_weights(predict_file)\n",
    "\n",
    "def DSen2(d10, d20):\n",
    "    test = [d10, d20]\n",
    "    input_shape = ((4, None, None), (6, None, None))\n",
    "    prediction = _predict(test, input_shape, deep=False)\n",
    "    #prediction *= 5\n",
    "    return prediction\n",
    "\n",
    "def _predict(test, input_shape, model = model, deep=False, run_60=False):\n",
    "    \n",
    "    print(\"Predicting using file: {}\".format(predict_file))\n",
    "    prediction = model.predict(test, verbose=1)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_per_month = [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30]\n",
    "starting_days = np.cumsum(days_per_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_proximal_steps(date, satisfactory):\n",
    "    arg_before = None\n",
    "    arg_after = None\n",
    "    if date > 0:\n",
    "        idx_before = satisfactory - date\n",
    "        arg_before = idx_before[np.where(idx_before < 0, idx_before, -np.inf).argmax()]\n",
    "    if date < np.max(satisfactory):\n",
    "        idx_after = satisfactory - date\n",
    "        arg_after = idx_after[np.where(idx_after > 0, idx_after, np.inf).argmin()]\n",
    "    if not arg_after and not arg_before:\n",
    "        arg_after = date\n",
    "        arg_before = date\n",
    "    if not arg_after:\n",
    "        arg_after = arg_before\n",
    "    if not arg_before:\n",
    "        arg_before = arg_after\n",
    "    #print(arg_before, date, arg_after)\n",
    "    return arg_before, arg_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_array(arr):\n",
    "    order = arr.argsort()\n",
    "    ranks = order.argsort()\n",
    "    return ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def mcm_shadow_mask(arr, c_probs):\n",
    "    #' From \"Cloud and cloud shadow masking for Sentinel-2 using multi-\n",
    "    #         temporal images in global area. Candra et al. 2020.\"\n",
    "    #'[B02,B03,B04, B08, B05,B06,B07, B8A,B11,B12]\n",
    "    #'[ 0 , 1 , 2 , 3  , 4  , 5,  6   7  , 8  , 9]'\n",
    "    mean_c_probs = np.mean(c_probs, axis = (1, 2))\n",
    "    cloudy_steps = np.argwhere(mean_c_probs > 0.25)\n",
    "    images_clean = np.delete(arr, cloudy_steps, 0)\n",
    "    cloud_ranks = rank_array(mean_c_probs)\n",
    "    diffs = abs(np.sum(arr - np.mean(images_clean, axis = 0), axis = (1, 2, 3)))\n",
    "    diff_ranks = rank_array(diffs)\n",
    "    overall_rank = diff_ranks + cloud_ranks\n",
    "    reference_idx = np.argmin(overall_rank)\n",
    "    ri = arr[reference_idx]\n",
    "    print(reference_idx)\n",
    "    \n",
    "    nir_means = np.mean(arr[:, :, :, 4], axis = (0))\n",
    "    \n",
    "    shadows = np.zeros((arr.shape[0], 632, 632))    \n",
    "    # Candra et al. 2020\n",
    "    \n",
    "    for time in tnrange(arr.shape[0]):\n",
    "        for x in range(arr.shape[1]):\n",
    "            for y in range(arr.shape[2]):\n",
    "                ti_slice = arr[time, x, y]\n",
    "                ri_slice = ri[x, y]\n",
    "                deltab2 = ti_slice[1] - ri_slice[1]\n",
    "                deltab3 = ti_slice[2] - ri_slice[2]\n",
    "                deltab4 = ti_slice[3] - ri_slice[3]\n",
    "                deltab8a = ti_slice[6] - ri_slice[6]\n",
    "                deltab11 = ti_slice[8] - ri_slice[8]\n",
    "\n",
    "                if deltab2 < 0.1:\n",
    "                    if deltab3 < 0.08:\n",
    "                        if deltab4 < 0.08:\n",
    "                            if deltab8a < -0.04:\n",
    "                                if deltab11 < -0.04:\n",
    "                                    if ti_slice[1] < 0.0950:\n",
    "                                        shadows[time, x, y] = 1.\n",
    "                                        \n",
    "    # Additional 3 time-window NIR thresholding\n",
    "    '''\n",
    "    for time in tnrange(shadows.shape[0]):\n",
    "        for x in range(shadows.shape[1]):\n",
    "            for y in range(shadows.shape[2]):\n",
    "                if shadows[time, x, y] == 1:\n",
    "                    time_adjusted = np.max([time, 1])\n",
    "                    time_adjusted = np.min([time_adjusted, shadows.shape[0] - 2])\n",
    "                    # check before and after\n",
    "                    if shadows[time_adjusted + 1, x, y] == 1 or shadows[time_adjusted-1, x, y] == 1:\n",
    "                        nir_mean = nir_means[x, y]\n",
    "                        nir_before = arr[time_adjusted - 1, x, y, 4] - nir_mean\n",
    "                        nir_now = arr[time, x, y, 4] - nir_mean\n",
    "                        nir_next = arr[time_adjusted + 1, x, y, 4] - nir_mean\n",
    "                        if nir_now > nir_mean * 0.75:\n",
    "                            shadows[time, x, y] = 0\n",
    "                        if nir_next or nir_before <= nir_now:\n",
    "                            shadows[time, x, y] = 0\n",
    "    '''                 \n",
    "                            \n",
    "    # Remove shadows if cannot coreference a cloud\n",
    "    shadow_large = np.reshape(shadows, (shadows.shape[0], 79, 8, 79, 8))\n",
    "    shadow_large = np.sum(shadow_large, axis = (2, 4))\n",
    "    \n",
    "    cloud_large = np.copy(c_probs)\n",
    "    cloud_large[np.where(c_probs > 0.33)] = 1.\n",
    "    cloud_large[np.where(c_probs < 0.33)] = 0.\n",
    "    cloud_large = np.reshape(cloud_large, (shadows.shape[0], 79, 8, 79, 8))\n",
    "    cloud_large = np.sum(cloud_large, axis = (2, 4))\n",
    "    for time in tnrange(shadow_large.shape[0]):\n",
    "        for x in range(shadow_large.shape[1]):\n",
    "            x_low = np.max([x - 8, 0])\n",
    "            x_high = np.min([x + 8, shadow_large.shape[1] - 1])\n",
    "            for y in range(shadow_large.shape[2]):\n",
    "                y_low = np.max([y - 8, 0])\n",
    "                y_high = np.min([y + 8, shadow_large.shape[1] - 1])\n",
    "                if shadow_large[time, x, y] < 8:\n",
    "                    shadow_large[time, x, y] = 0.\n",
    "                if shadow_large[time, x, y] >= 8:\n",
    "                    shadow_large[time, x, y] = 1.\n",
    "                c_prob_window = cloud_large[time, x_low:x_high, y_low:y_high]\n",
    "                if np.max(c_prob_window) < 16:\n",
    "                    shadow_large[time, x, y] = 0.\n",
    "                    \n",
    "    shadow_large = resize(shadow_large, (shadow_large.shape[0], 632, 632), order = 0)\n",
    "    shadows *= shadow_large\n",
    "    \n",
    "    # Go through and aggregate the shadow map to an 80m grid, and extend it one grid size around\n",
    "    # any positive ID\n",
    "    \n",
    "    shadows = np.reshape(shadows, (shadows.shape[0], 79, 8, 79, 8))\n",
    "    shadows = np.sum(shadows, axis = (2, 4))\n",
    "    shadows[np.where(shadows < 12)] = 0.\n",
    "    shadows[np.where(shadows >= 12)] = 1.\n",
    "    \n",
    "    shadows = resize(shadows, (shadows.shape[0], 632, 632), order = 0)\n",
    "    shadows = np.reshape(shadows, (shadows.shape[0], 632//4, 4, 632//4, 4))\n",
    "    shadows = np.max(shadows, (2, 4))\n",
    "    \n",
    "    shadows_new = np.zeros_like(shadows)\n",
    "    for time in range(shadows.shape[0]):\n",
    "        for x in range(shadows.shape[1]):\n",
    "            for y in range(shadows.shape[2]):\n",
    "                if shadows[time, x, y] == 1:\n",
    "                    min_x = np.max([x - 1, 0])\n",
    "                    max_x = np.min([x + 2, 157])\n",
    "                    min_y = np.max([y - 1, 0])\n",
    "                    max_y = np.min([y + 2, 157])\n",
    "                    for x_idx in range(min_x, max_x):\n",
    "                        for y_idx in range(min_y, max_y):\n",
    "                            shadows_new[time, x_idx, y_idx] = 1.\n",
    "    shadows_new = resize(shadows_new, (shadows.shape[0], 632, 632), order = 0)\n",
    "    return shadows_new\n",
    "    \n",
    "#c_probs = np.load(OUTPUT_FOLDER + \"raw/clouds/clouds_0_0.npy\")\n",
    "#arr = x\n",
    "#mask2 = mcm_shadow_mask(x, c_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clouds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-8f99e2607486>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#clouds = np.load(OUTPUT_FOLDER + \"raw/clouds/clouds_0_2.npy\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclouds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m38\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'clouds' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x1080 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "\n",
    "#clouds = np.load(OUTPUT_FOLDER + \"raw/clouds/clouds_0_2.npy\")\n",
    "sns.heatmap(clouds[38, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shadows' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-0ec17efe9fd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#shadows = np.load(OUTPUT_FOLDER + \"raw/clouds/shadows_0_2.npy\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshadows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m38\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'shadows' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x1080 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "\n",
    "#shadows = np.load(OUTPUT_FOLDER + \"raw/clouds/shadows_0_2.npy\")\n",
    "sns.heatmap(shadows[38, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-90a0ccf363db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#x = np.load(OUTPUT_FOLDER + \"raw/clouds/l1c_0_2.npy\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x1080 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "\n",
    "#x = np.load(OUTPUT_FOLDER + \"raw/clouds/l1c_0_2.npy\")\n",
    "sns.heatmap(x[25, :, :, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_and_make_dirs(dirs):\n",
    "    if not os.path.exists(os.path.realpath(dirs)):\n",
    "        os.makedirs(os.path.realpath(dirs))\n",
    "\n",
    "def make_output_and_temp_folders(idx, output_folder = OUTPUT_FOLDER):\n",
    "    find_and_make_dirs(output_folder + \"raw/\")\n",
    "    find_and_make_dirs(output_folder + \"raw/clouds/\")\n",
    "    find_and_make_dirs(output_folder + \"raw/s1/\")\n",
    "    find_and_make_dirs(output_folder + \"raw/s2/\")\n",
    "    find_and_make_dirs(output_folder + \"raw/misc/\")\n",
    "    find_and_make_dirs(output_folder + \"processed/\")\n",
    "\n",
    "def download_large_tile(coord, step_x, step_y, folder = OUTPUT_FOLDER, year = 2019):\n",
    "    '''\n",
    "    Ideally the folder structure should be as follows:\n",
    "     - Raw\n",
    "         - Cloud\n",
    "              - Y*5, X*5\n",
    "         - Sentinel 1\n",
    "         - Sentinel 2\n",
    "         - DEM\n",
    "         - Dates\n",
    "     - Processed\n",
    "         - Y, X\n",
    "         \n",
    "    Methodological questions to address:\n",
    "        - Should we download all images for a region first, and then process?     - Seems unnecessary\n",
    "          This would allow for matching up the borders\n",
    "        - Remove dirty steps for the whole tile or by subtiles \n",
    "        - Is this the proper time to address the interpolation of clouds (?)\n",
    "        - Make sure that cloud shadows line up with clouds                        - X\n",
    "        - Make sure that sentinel 1 is properly matched up and items are removed  - X\n",
    "         \n",
    "    The strategy is as follows:\n",
    "        - Download 5*9 by 5*9 cloud, S1, S2, DEM, dates, save to folders          - X\n",
    "        - Load each start:start+128 tile                                          - X\n",
    "        - Remove dirty steps, missing images, missed clouds, ratios               - X\n",
    "        - Superresolve                                                            - X\n",
    "        - Indices                                                                 - X\n",
    "        - Calculate_and_save_best_images                                          - X\n",
    "        - Whittaker smooth                                                        - X\n",
    "        - Fuse S1\n",
    "        - Save to processed / y / x\n",
    "        - Iterate for each of the large sub-tiles\n",
    "    \n",
    "    '''\n",
    "    print(coord)\n",
    "    bbx = calculate_bbx(coord, step_x, step_y, expansion = 10)\n",
    "    print(bbx)\n",
    "    dem_bbx = calculate_bbx(coord, step_x, step_y, expansion = 20)\n",
    "    idx = str(step_y) + \"_\" + str(step_x)\n",
    "    print(idx)\n",
    "    idx = str(idx)\n",
    "    #if 1 != 0:\n",
    "    print(\"Making raw folders\")\n",
    "    make_output_and_temp_folders(idx)\n",
    "\n",
    "    print(\"Calculating cloud cover\")\n",
    "    if not os.path.exists(folder + \"raw/clouds/clouds_{}.npy\".format(idx)):\n",
    "        l1c, cloud_probs, shadows = identify_clouds(bbx) # integrate cloud shadow here\n",
    "        np.save(folder + \"raw/clouds/clouds_{}.npy\".format(idx), cloud_probs)\n",
    "        np.save(folder + \"raw/clouds/shadows_{}.npy\".format(idx), shadows)\n",
    "\n",
    "    if not os.path.exists(folder + \"raw/s1/{}.npy\".format(idx)):\n",
    "        print(\"Downloading S1\")\n",
    "        s1, s1_dates = download_sentinel_1(bbx, layer = \"SENT\")\n",
    "        s1 = process_sentinel_1_tile(s1, s1_dates)\n",
    "        np.save(folder + \"raw/s1/{}.npy\".format(idx), s1)\n",
    "        np.save(folder + \"raw/misc/s1_dates_{}\".format(idx), np.array(s1_dates))\n",
    "\n",
    "    if not os.path.exists(folder + \"raw/s2/{}.npy\".format(idx)):\n",
    "        print(\"Downloading S2\")\n",
    "        s2, s2_dates = download_layer(bbx)\n",
    "        np.save(folder + \"raw/s2/{}.npy\".format(idx), s2)\n",
    "        np.save(folder + \"raw/misc/s2_dates_{}\".format(idx), np.array(s2_dates))\n",
    "        #np.save(folder + \"raw/clouds/shadows_{}\".format(idx), np.array(shadows))\n",
    "\n",
    "    if not os.path.exists(folder + \"raw/misc/dem_{}.npy\".format(idx)):\n",
    "        print(\"Downloading DEM\")\n",
    "        dem = download_dem(dem_bbx) # get the DEM BBOX\n",
    "        np.save(folder + \"raw/misc/dem_{}.npy\".format(idx), dem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bad_steps(sentinel2, clouds):\n",
    "    n_cloud_px = np.array([len(np.argwhere(clouds[x, :, :].reshape((632)*(632)) > 0.3)) for x in range(clouds.shape[0])])\n",
    "    cloud_steps = np.argwhere(n_cloud_px > 632**2 / 5)\n",
    "    missing_images = [np.argwhere(sentinel2[x, :, : :].flatten() == 0.0) for x in range(sentinel2.shape[0])]\n",
    "    missing_images = np.array([len(x) for x in missing_images])\n",
    "    missing_images_p = [np.argwhere(sentinel2[x, :, : :10].flatten() >= 1) for x in range(sentinel2.shape[0])]\n",
    "    missing_images_p = np.array([len(x) for x in missing_images_p])\n",
    "    missing_images += missing_images_p\n",
    "    missing_images = np.argwhere(missing_images >= 100)\n",
    "    print(cloud_steps)\n",
    "    print(missing_images)\n",
    "    #perc_shadow_px = np.sum(shadows, axis = (1, 2) / (79**2))\n",
    "    #shadow_steps = np.argwhere(perc_shadow_px > 20)\n",
    "    to_remove = np.unique(np.concatenate([cloud_steps.flatten(), missing_images.flatten()]))#, shadow_steps]))\n",
    "    return to_remove\n",
    "\n",
    "def superresolve(sentinel2):\n",
    "    d10 = sentinel2[:, :, :, 0:4]\n",
    "    d20 = sentinel2[:, :, :, 4:10]\n",
    "\n",
    "    d10 = np.swapaxes(d10, 1, -1)\n",
    "    d10 = np.swapaxes(d10, 2, 3)\n",
    "    d20 = np.swapaxes(d20, 1, -1)\n",
    "    d20 = np.swapaxes(d20, 2, 3)\n",
    "    superresolved = DSen2(d10, d20)\n",
    "    superresolved = np.swapaxes(superresolved, 1, -1)\n",
    "    superresolved = np.swapaxes(superresolved, 1, 2)\n",
    "\n",
    "    # returns band IDXs 3, 4, 5, 7, 8, 9\n",
    "    return superresolved\n",
    "\n",
    "def process_sentinel_1_tile(sentinel1, dates):\n",
    "    s1 = calculate_and_save_best_images(sentinel1, dates)\n",
    "    # Retain only iamgery every 15 days\n",
    "    biweekly_dates = np.array([day for day in range(0, 360, 5)])\n",
    "    to_remove = np.argwhere(biweekly_dates % 15 != 0)\n",
    "    s1 = np.delete(s1, to_remove, 0)\n",
    "    return s1\n",
    "\n",
    "\n",
    "def interpolate_array(x):\n",
    "    no_dem = np.delete(x, 10, -1)\n",
    "    no_dem = np.reshape(no_dem, (72, 128*128*14))\n",
    "    no_dem = np.swapaxes(no_dem, 0, 1)\n",
    "\n",
    "    pool = multiprocessing.Pool(6)\n",
    "    no_dem = pool.map(smooth, no_dem)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    no_dem = np.swapaxes(no_dem, 0, 1)\n",
    "    no_dem = np.reshape(no_dem, (72, 128, 128, 14))\n",
    "    x[:, :, :, :10] = no_dem[:, :, :, :10]\n",
    "    x[:, :, :, 11:] = no_dem[:, :, :, 10:]\n",
    "\n",
    "    biweekly_dates = np.array([day for day in range(0, 360, 5)])\n",
    "    to_remove = np.argwhere(biweekly_dates % 15 != 0)\n",
    "    x = np.delete(x, to_remove, 0)\n",
    "    return x\n",
    "\n",
    "def process_large_tile(step_x, step_y, folder = OUTPUT_FOLDER):\n",
    "    idx = str(step_y) + \"_\" + str(step_x)\n",
    "    x_vals = []\n",
    "    y_vals = []\n",
    "    # save to disk\n",
    "    for i in range(25):\n",
    "        y_val = (24 - i) // 5\n",
    "        x_val = 5 - ((25 - i) % 5)\n",
    "        x_val = 0 if x_val == 5 else x_val\n",
    "        x_vals.append(x_val)\n",
    "        y_vals.append(y_val)\n",
    "        \n",
    "    y_vals = [i + (5*step_y) for i in y_vals]\n",
    "    x_vals = [i + (5*step_x) for i in x_vals]\n",
    "    print(y_vals, x_vals)\n",
    "    \n",
    "    \n",
    "    processed = True\n",
    "    for x, y in zip(x_vals, y_vals):\n",
    "        if not os.path.exists(folder + \"processed/{}/{}.npy\".format(str(y), str(x))):\n",
    "            processed = False\n",
    "    if not processed:\n",
    "        \n",
    "        clouds = np.load(folder + \"raw/clouds/clouds_{}.npy\".format(idx))\n",
    "        sentinel1 = np.load(folder + \"raw/s1/{}.npy\".format(idx))\n",
    "        radar_dates = np.load(folder + \"raw/misc/s1_dates_{}.npy\".format(idx))\n",
    "        sentinel2 = np.load(folder + \"raw/s2/{}.npy\".format(idx))\n",
    "        dem = np.load(folder + \"raw/misc/dem_{}.npy\".format(idx))\n",
    "        image_dates = np.load(folder + \"raw/misc/s2_dates_{}.npy\".format(idx))\n",
    "        if os.path.exists(folder + \"raw/clouds/shadows_{}.npy\".format(idx)):\n",
    "            shadows = np.load(folder + \"raw/clouds/shadows_{}.npy\".format(idx))\n",
    "        else:\n",
    "            print(\"No shadows file, so calculating shadows with L2A\")\n",
    "            shadows = mcm_shadow_mask(sentinel2, clouds)\n",
    "        print(\"The files have been loaded\")\n",
    "\n",
    "        #sentinel1 = process_sentinel_1_tile(sentinel1, radar_dates)\n",
    "        to_remove = calculate_bad_steps(sentinel2, clouds)\n",
    "        sentinel2 = np.delete(sentinel2, to_remove, axis = 0)\n",
    "        clouds = np.delete(clouds, to_remove, axis = 0)\n",
    "        shadows = np.delete(shadows, to_remove, axis = 0)\n",
    "        image_dates = np.delete(image_dates, to_remove)\n",
    "        print(\"Cloudy and missing images removed, radar processed\")\n",
    "\n",
    "        to_remove = remove_missed_clouds(sentinel2)\n",
    "        sentinel2 = np.delete(sentinel2, to_remove, axis = 0)\n",
    "        clouds = np.delete(clouds, to_remove, axis = 0)\n",
    "        image_dates = np.delete(image_dates, to_remove)\n",
    "        shadows = np.delete(shadows, to_remove, axis = 0)\n",
    "        print(\"Missed cloudy images removed\")\n",
    "\n",
    "        x, _, _ = remove_cloud_and_shadows(sentinel2, clouds, shadows, image_dates)\n",
    "        print(\"Clouds and shadows interpolated\")\n",
    "\n",
    "\n",
    "        index = 0\n",
    "        for start_x, end_x in zip(range(0, 633, 126), range(128, 633, 126)):\n",
    "            for start_y, end_y in zip(range(0, 633, 126), range(128, 633, 126)):\n",
    "                print(index)\n",
    "                if not os.path.exists(folder + \"processed/{}/{}.npy\".format(str(y_vals[index]), str(x_vals[index]))):\n",
    "                    subtile = x[:, start_x:end_x, start_y:end_y, :]\n",
    "                    resolved = superresolve(subtile)\n",
    "                    subtile[:, :, :, 4:10] = resolved\n",
    "                    dem_i = np.tile(dem[np.newaxis, start_x:end_x, start_y:end_y, :], (x.shape[0], 1, 1, 1))\n",
    "                    subtile = np.concatenate([subtile, dem_i / 90], axis = -1)\n",
    "                    subtile, amin = evi(subtile, verbose = True)\n",
    "                    subtile = bi(subtile, verbose = True)\n",
    "                    subtile = msavi2(subtile, verbose = True)\n",
    "                    subtile = si(subtile, verbose = True)\n",
    "\n",
    "                    subtile = calculate_and_save_best_images(subtile, image_dates)\n",
    "                    subtile = interpolate_array(subtile)\n",
    "                    subtile = np.concatenate([subtile, sentinel1[:, start_x:end_x,\n",
    "                                                                start_y:end_y, :]], axis = -1)\n",
    "\n",
    "\n",
    "                    out_y_folder = folder + \"processed/{}/\".format(str(y_vals[index]))\n",
    "                    if not os.path.exists(os.path.realpath(out_y_folder)):\n",
    "                        os.makedirs(os.path.realpath(out_y_folder))\n",
    "                    np.save(folder + \"processed/{}/{}.npy\".format(str(y_vals[index]), str(x_vals[index])), subtile)\n",
    "                index += 1\n",
    "            \n",
    "def clean_up_folders():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: 0 Y:0\n",
      "(-15.341662, 15.825856)\n",
      "-15.341662 15.825856\n",
      "-15.341755383181154 15.82585585296043\n",
      "-15.341662 15.825856\n",
      "-15.282737028435815 15.825940769838544\n",
      "([-15.282816490292019, 15.88298584041455], [-15.341755231275698, 15.825765448999656])\n",
      "-15.341662 15.825856\n",
      "-15.341848766361323 15.825855705880672\n",
      "-15.341662 15.825856\n",
      "-15.282643644688509 15.82594089148297\n",
      "0_0\n",
      "Making raw folders\n",
      "Calculating cloud cover\n",
      "13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6fabd241ac843df80ca73fc664a6398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=60), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3c3270884564a8891de2a639d7129e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=60), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cloud probs: (60, 632, 632)\n",
      "Downloading S1\n",
      "Maximum time distance: 25\n",
      "Downloading S2\n",
      "Original size: (60, 636, 631, 6)\n",
      "Original 10 size: (60, 636, 631, 4)\n",
      "Downloading DEM\n",
      "[4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0] [0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4]\n",
      "The files have been loaded\n",
      "[[ 0]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 7]\n",
      " [ 8]\n",
      " [22]\n",
      " [28]\n",
      " [34]\n",
      " [36]\n",
      " [38]\n",
      " [39]\n",
      " [40]\n",
      " [53]\n",
      " [59]]\n",
      "[[ 0]\n",
      " [ 1]\n",
      " [36]\n",
      " [40]]\n",
      "Cloudy and missing images removed, radar processed\n",
      "Missed cloudy images removed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d98c68c8404492c866d6af35e4655b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=314), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A total of 34649 pixels were interpolated\n",
      "Clouds and shadows interpolated\n",
      "0\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "46/46 [==============================] - 28s 609ms/step\n",
      "There are: 0 out of bounds EVI\n",
      "Maximum time distance: 35\n",
      "1\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "46/46 [==============================] - 28s 611ms/step\n",
      "There are: 0 out of bounds EVI\n",
      "Maximum time distance: 35\n",
      "2\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "46/46 [==============================] - 29s 628ms/step\n",
      "There are: 0 out of bounds EVI\n",
      "Maximum time distance: 35\n",
      "3\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "46/46 [==============================] - 28s 616ms/step\n",
      "There are: 0 out of bounds EVI\n",
      "Maximum time distance: 35\n",
      "4\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "46/46 [==============================] - 27s 586ms/step\n",
      "There are: 0 out of bounds EVI\n",
      "Maximum time distance: 35\n",
      "5\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "46/46 [==============================] - 27s 588ms/step\n",
      "There are: 0 out of bounds EVI\n",
      "Maximum time distance: 35\n",
      "6\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "46/46 [==============================] - 27s 591ms/step\n",
      "There are: 0 out of bounds EVI\n",
      "Maximum time distance: 35\n",
      "7\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "46/46 [==============================] - 29s 629ms/step\n",
      "There are: 0 out of bounds EVI\n",
      "Maximum time distance: 35\n",
      "8\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "46/46 [==============================] - 28s 618ms/step\n",
      "There are: 0 out of bounds EVI\n",
      "Maximum time distance: 35\n",
      "9\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "46/46 [==============================] - 29s 624ms/step\n",
      "There are: 0 out of bounds EVI\n",
      "Maximum time distance: 35\n",
      "10\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "46/46 [==============================] - 28s 618ms/step\n",
      "There are: 0 out of bounds EVI\n",
      "Maximum time distance: 35\n",
      "11\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "46/46 [==============================] - 29s 640ms/step\n",
      "There are: 0 out of bounds EVI\n",
      "Maximum time distance: 35\n",
      "12\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "46/46 [==============================] - 29s 639ms/step\n",
      "There are: 0 out of bounds EVI\n",
      "Maximum time distance: 35\n",
      "13\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "46/46 [==============================] - 34s 742ms/step\n",
      "There are: 0 out of bounds EVI\n",
      "Maximum time distance: 35\n",
      "14\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "46/46 [==============================] - 28s 614ms/step\n",
      "There are: 0 out of bounds EVI\n",
      "Maximum time distance: 35\n",
      "15\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "46/46 [==============================] - 29s 630ms/step\n",
      "There are: 0 out of bounds EVI\n",
      "Maximum time distance: 35\n",
      "16\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "46/46 [==============================] - 28s 615ms/step\n",
      "There are: 0 out of bounds EVI\n",
      "Maximum time distance: 35\n",
      "17\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "46/46 [==============================] - 28s 617ms/step\n",
      "There are: 0 out of bounds EVI\n",
      "Maximum time distance: 35\n",
      "18\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "46/46 [==============================] - 28s 610ms/step\n",
      "There are: 0 out of bounds EVI\n",
      "Maximum time distance: 35\n",
      "19\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "46/46 [==============================] - 32s 700ms/step\n",
      "There are: 0 out of bounds EVI\n",
      "Maximum time distance: 35\n",
      "20\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "46/46 [==============================] - 29s 633ms/step\n",
      "There are: 0 out of bounds EVI\n",
      "Maximum time distance: 35\n",
      "21\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "46/46 [==============================] - 28s 615ms/step\n",
      "There are: 0 out of bounds EVI\n",
      "Maximum time distance: 35\n",
      "22\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "46/46 [==============================] - 28s 601ms/step\n",
      "There are: 0 out of bounds EVI\n",
      "Maximum time distance: 35\n",
      "23\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "46/46 [==============================] - 28s 609ms/step\n",
      "There are: 0 out of bounds EVI\n",
      "Maximum time distance: 35\n",
      "24\n",
      "Predicting using file: ../src/dsen2/models/s2_032_lr_1e-04.hdf5\n",
      "46/46 [==============================] - 28s 612ms/step\n",
      "There are: 0 out of bounds EVI\n",
      "Maximum time distance: 35\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tnrange, tqdm_notebook\n",
    "import math\n",
    "\n",
    "for x_tile in range(0, 1):\n",
    "    for y_tile in range(0, 1):\n",
    "        print(\"X: {} Y:{}\".format(x_tile, y_tile))\n",
    "        download_large_tile(coords, x_tile, y_tile)\n",
    "        process_large_tile(x_tile, y_tile)\n",
    "        #clean_up_folders(x_tile, y_tile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_sensing",
   "language": "python",
   "name": "remote_sensing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
