{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from osgeo import ogr, osr\n",
    "from sentinelhub import WmsRequest, WcsRequest, MimeType, CRS, BBox, constants\n",
    "import logging\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import os\n",
    "import yaml\n",
    "from sentinelhub import DataSource\n",
    "import scipy.sparse as sparse\n",
    "import scipy\n",
    "from scipy.sparse.linalg import splu\n",
    "from skimage.transform import resize\n",
    "from sentinelhub import CustomUrlParam\n",
    "from time import time as timer\n",
    "from time import sleep as sleep\n",
    "import multiprocessing\n",
    "import math\n",
    "import reverse_geocoder as rg\n",
    "import pycountry\n",
    "import pycountry_convert as pc\n",
    "import hickle as hkl\n",
    "from shapely.geometry import Point, Polygon\n",
    "import geopandas\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "import math\n",
    "import boto3\n",
    "from pyproj import Proj, transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../config.yaml\", 'r') as stream:\n",
    "    key = (yaml.safe_load(stream))\n",
    "    API_KEY = key['key']\n",
    "    AWSKEY = key['awskey']\n",
    "    AWSSECRET = key['awssecret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/john.brandt/anaconda3/envs/remote_sensing/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%run ../src/utils/slope.py\n",
    "%run ../src/utils/utils-bilinear.py\n",
    "%run ../src/dsen2/utils/DSen2Net.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ('2018-12-15', '2020-01-15')\n",
    "SIZE = 9*5\n",
    "IMSIZE = (SIZE * 14)+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../tile_data/elsalvador-imposible/ (-90.015579, 13.727334)\n"
     ]
    }
   ],
   "source": [
    "landscapes = {\n",
    "    'ethiopia-tigray': (13.540810, 38.177220),\n",
    "    'kenya-makueni-2': (-1.817109, 37.44563),\n",
    "    'ghana': (9.259359, -0.83375),\n",
    "    'niger-koure': (13.18158, 2.478),\n",
    "    'cameroon-farnorth': (10.596, 14.2722),\n",
    "    'mexico-campeche': (18.232495, -92.1234215),\n",
    "    'malawi-rumphi': (-11.044, 33.818),\n",
    "    'ghana-sisala-east': (10.385, -1.765),\n",
    "    'ghana-west-mamprusi': (10.390084, -0.846330),\n",
    "    'ghana-kwahu': (6.518909, -0.826008),\n",
    "    'senegal-16b': (15.82585, -15.34166),\n",
    "    'india-kochi': (9.909, 76.254),\n",
    "    'india-sidhi': (24.0705, 81.607),\n",
    "    'brazil-esperito-santo': (-20.147, -40.837),\n",
    "    'brazil-paraiba': (-22.559943, -44.186629),\n",
    "    'brazil-goias': (-14.905595, -48.907399),\n",
    "    'colombia-talima': (4.179529, -74.889171),\n",
    "    'drc-kafubu': (-11.749636, 27.586622),\n",
    "    'thailand-khon-kaen': (15.709725, 102.546518),\n",
    "    'indonesia-west-java': (-6.721101, 108.280949),\n",
    "    'madagascar': (-18.960152, 47.469587),\n",
    "    'tanzania': (-6.272258, 36.679824),\n",
    "    'chile': (-36.431237, -71.872030),\n",
    "    'indonesia-jakarta': (-6.352580, 106.677072),\n",
    "    'caf-baboua': (5.765917, 14.791618),   \n",
    "    'honduras': (14.096664, -88.720304),\n",
    "    'nicaragua': (12.398014, -86.963042),\n",
    "    'china': (26.673679, 107.464231),\n",
    "    'australia-west': (-32.666762, 117.411197),\n",
    "    'mexico-sonora': (29.244288, -111.243230),\n",
    "    'south-africa': (-30.981698, 28.727301),\n",
    "    'maldonado-uraguay': (-34.629250, -55.004331),\n",
    "    'dominican-rep-la-salvia': (18.872589, -70.462961),\n",
    "    'guatemala-coban': (15.3, -90.8),\n",
    "    'senegal-tucker-a': (15.350595, -15.459789),\n",
    "    'elsalvador-imposible': (13.727334, -90.015579)\n",
    "}\n",
    "\n",
    "landscape = 'elsalvador-imposible'\n",
    "\n",
    "#coords = (7.702058, -0.709011) # brong ahafo, bono east\n",
    "#coords = (7.398111, -1.269223) # cocoa\n",
    "#coords = (16.032170, -90.144511) # Guatemala\n",
    "#coords = (13.757749, -90.004949) # elsalvador imposible\n",
    "#coords = (13.933745, -84.690842) # Bonanza, Nicaragua\n",
    "\n",
    "OUTPUT_FOLDER = '../tile_data/{}/'.format(landscape)\n",
    "coords = landscapes[landscape]\n",
    "coords = (coords[1], coords[0])\n",
    "print(OUTPUT_FOLDER, coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "landscape_df = pd.DataFrame({'landscape': [x for x in landscapes.keys()], \n",
    "                             'latitude': [x[0] for x in landscapes.values()],\n",
    "                             'longitude': [x[1] for x in landscapes.values()]\n",
    "})\n",
    "\n",
    "landscape_df.to_csv(\"../data/latlongs/landscapes.csv\", index=False)\n",
    "print(len(landscape_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions (to be moved to a utils file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRID_SIZE_X = 1\n",
    "GRID_SIZE_Y = 1\n",
    "\n",
    "IMAGE_X = 14*GRID_SIZE_X\n",
    "IMAGE_Y = 14*GRID_SIZE_Y\n",
    "\n",
    "TEST_X = 5\n",
    "TEST_Y = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These arrays are for smoothly overlapping the cloud and shadow interpolation\n",
    "c_arr = np.array([[1, 1, 1, 1, 1,],\n",
    "                  [1, 2, 2, 2, 1,],\n",
    "                  [1, 2, 3, 2, 1,],\n",
    "                  [1, 2, 2, 2, 1,],\n",
    "                  [1, 1, 1, 1, 1,],])\n",
    "                  \n",
    "c_arr = c_arr / 3\n",
    "o_arr = 1 - c_arr\n",
    "c_arr = np.tile(c_arr[:, :, np.newaxis], (1, 1, 10))\n",
    "o_arr = np.tile(o_arr[:, :, np.newaxis], (1, 1, 10))\n",
    "\n",
    "def convertCoords(xy, src='', targ=''):\n",
    "    \"\"\" Converts coords from one EPSG to another\n",
    "\n",
    "        Parameters:\n",
    "         xy (tuple): input longitiude, latitude tuple\n",
    "         src (str): EPSG code associated with xy\n",
    "         targ (str): EPSG code of target output\n",
    "    \n",
    "        Returns:\n",
    "         pt (tuple): (x, y) tuple of xy in targ EPSG\n",
    "    \"\"\"\n",
    "\n",
    "    srcproj = osr.SpatialReference()\n",
    "    srcproj.ImportFromEPSG(src)\n",
    "    targproj = osr.SpatialReference()\n",
    "    if isinstance(targ, str):\n",
    "        targproj.ImportFromProj4(targ)\n",
    "    else:\n",
    "        targproj.ImportFromEPSG(targ)\n",
    "    transform = osr.CoordinateTransformation(srcproj, targproj)\n",
    "\n",
    "    pt = ogr.Geometry(ogr.wkbPoint)\n",
    "    pt.AddPoint(xy[0], xy[1])\n",
    "    pt.Transform(transform)\n",
    "    return([pt.GetX(), pt.GetY()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_epsg(points):\n",
    "    \"\"\" Calculates the UTM EPSG of an input WGS 84 lon, lat\n",
    "\n",
    "        Parameters:\n",
    "         points (tuple): input longitiude, latitude tuple\n",
    "    \n",
    "        Returns:\n",
    "         epsg_code (int): integer form of associated UTM EPSG\n",
    "    \"\"\"\n",
    "    lon, lat = points[0], points[1]\n",
    "    utm_band = str((math.floor((lon + 180) / 6 ) % 60) + 1)\n",
    "    if len(utm_band) == 1:\n",
    "        utm_band = '0'+utm_band\n",
    "    if lat >= 0:\n",
    "        epsg_code = '326' + utm_band\n",
    "    else:\n",
    "        epsg_code = '327' + utm_band\n",
    "    return int(epsg_code)\n",
    "\n",
    "def PolygonArea(corners):\n",
    "    n = len(corners) # of corners\n",
    "    area = 0.0\n",
    "    for i in range(n):\n",
    "        j = (i + 1) % n\n",
    "        area += corners[i][0] * corners[j][1]\n",
    "        area -= corners[j][0] * corners[i][1]\n",
    "    area = abs(area)\n",
    "    return area\n",
    "    \n",
    "def offset_x(coord, offset):\n",
    "    ''' Converts a WGS 84 to UTM, adds meters, and converts back'''\n",
    "    epsg = calculate_epsg(coord)\n",
    "    coord = convertCoords(coord, 4326, epsg)\n",
    "    coord[0] += offset\n",
    "    coord = convertCoords(coord, epsg, 4326)\n",
    "    return coord\n",
    "    \n",
    "def offset_y(coord, offset):\n",
    "    ''' Converts a WGS 84 to UTM, adds meters, and converts back'''\n",
    "    epsg = calculate_epsg(coord)\n",
    "    coord = convertCoords(coord, 4326, epsg)\n",
    "    coord[1] += offset\n",
    "    coord = convertCoords(coord, epsg, 4326)\n",
    "    return coord\n",
    "\n",
    "def calculate_area(bbx):\n",
    "    '''\n",
    "    Calculates the area in ha of a [(min_x, min_y), (max_x, max_y)] bbx\n",
    "    '''\n",
    "\n",
    "    epsg = calculate_epsg(bbx[0])\n",
    "    \n",
    "    mins = convertCoords(bbx[0], 4326, epsg)\n",
    "    maxs = convertCoords(bbx[1], 4326, epsg)\n",
    "    area = PolygonArea([(mins[0], mins[1]), # BL\n",
    "                        (mins[0], maxs[1]), # BR\n",
    "                        (maxs[0], mins[1]), # TL\n",
    "                        (maxs[0], mins[1]) # TR\n",
    "                        ])\n",
    "    hectares = math.floor(area / 1e4)\n",
    "    print(hectares)\n",
    "    \n",
    "\n",
    "def calculate_bbx(coord, step_x, step_y, expansion, multiplier = 1.):\n",
    "    ''' Calculates the four corners of a bounding box of step_x * step_y offset from coord'''\n",
    "    coord_bl = np.copy(coord)\n",
    "    coord1 = offset_x(coord_bl, 6300*step_x - expansion)\n",
    "    coord1 = offset_y(coord1 , 6300*step_y - expansion)\n",
    "    \n",
    "    coord_tr = np.copy(coord)\n",
    "    coord2 = offset_x(coord_tr, 6300*(step_x + multiplier) + expansion)\n",
    "    coord2 = offset_y(coord2, 6300*(step_y + multiplier) + expansion)\n",
    "    bbx = (coord2, coord1)\n",
    "    return bbx\n",
    "\n",
    "\n",
    "def calculate_bbx_pyproj(coord, step_x, step_y, expansion, multiplier = 1.):\n",
    "    ''' Calculates the four corners of a bounding box as above\n",
    "        but uses pyproj instead of OGR. It seems sentinelhub uses\n",
    "        pyproj, so this may be more pixel accurate (?)\n",
    "        x, y format\n",
    "    '''\n",
    "    \n",
    "    inproj = Proj('epsg:4326')\n",
    "    outproj_code = calculate_epsg(coord)\n",
    "    print(outproj_code)\n",
    "    outproj = Proj('epsg:' + str(outproj_code))\n",
    "    \n",
    "    \n",
    "    \n",
    "    coord_utm =  transform(inproj, outproj, coord[1], coord[0])\n",
    "    coord_utm_bottom_left = (coord_utm[0] + step_x*6300 - expansion,\n",
    "                             coord_utm[1] + step_y*6300 - expansion)\n",
    "    coord_utm_top_right = (coord_utm[0] + (step_x+multiplier) * 6300 + expansion,\n",
    "                           coord_utm[1] + (step_y+multiplier) * 6300 + expansion)\n",
    "    \n",
    "    coord_bottom_left = transform(outproj, inproj,\n",
    "                                  coord_utm_bottom_left[0],\n",
    "                                  coord_utm_bottom_left[1])\n",
    "    \n",
    "    coord_top_right = transform(outproj, inproj,\n",
    "                                  coord_utm_top_right[0],\n",
    "                                  coord_utm_top_right[1])\n",
    "    \n",
    "    zone = str(outproj_code)[3:]\n",
    "    direction = 'N' if coord[1] >= 0 else 'S'\n",
    "    utm_epsg = \"UTM_\" + zone + direction\n",
    "    print(utm_epsg)\n",
    "    return (coord_utm_bottom_left, coord_utm_top_right), CRS[utm_epsg]\n",
    "\n",
    "def pts_in_geojson(lats, longs, geojson):  \n",
    "    polys = geopandas.read_file(geojson)['geometry']\n",
    "    polys = geopandas.GeoSeries(polys)\n",
    "    pnts = [Point(x, y) for x, y in zip(list(lats), list(longs))]\n",
    "    \n",
    "    def _contains(pt):\n",
    "        return polys.contains(pt)[0]\n",
    "\n",
    "    if any([_contains(pt) for pt in pnts]):\n",
    "        return True\n",
    "    else: return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_clouds_new(bbox, epsg, dates = dates):\n",
    "\n",
    "    #for try_ in range(0, 5):\n",
    "        #try:\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    cloud_request = WmsRequest(\n",
    "        layer='CLOUD_NEW',\n",
    "        bbox=box,\n",
    "        time=dates,\n",
    "        width=(5*9*14)+2,\n",
    "        height=(5*9*14)+2,\n",
    "        image_format = MimeType.TIFF_d32f,\n",
    "        maxcc=0.7,\n",
    "        instance_id=API_KEY,\n",
    "        custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "        time_difference=datetime.timedelta(hours=72),\n",
    "    )\n",
    "\n",
    "\n",
    "    shadow_request = WmsRequest(\n",
    "        layer='SHADOW',\n",
    "        bbox=box,\n",
    "        time=dates,\n",
    "        width=(5*9*14)+2,\n",
    "        height=(5*9*14)+2,\n",
    "        image_format =  MimeType.TIFF_d16,\n",
    "        maxcc=0.7,\n",
    "        instance_id=API_KEY,\n",
    "        custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "        time_difference=datetime.timedelta(hours=72))\n",
    "\n",
    "    cloud_img = cloud_request.get_data()\n",
    "    cloud_img = np.array(cloud_img)\n",
    "    cloud_probs = cloud_img / 255\n",
    "    print(\"Cloud_probs shape: {}\".format(cloud_probs.shape))\n",
    "    \n",
    "    n_cloud_px = np.array([len(np.argwhere(cloud_probs[x, :, :].reshape((632)*(632)) > 0.33))\n",
    "                           for x in range(cloud_probs.shape[0])])\n",
    "    cloud_steps = np.argwhere(n_cloud_px > 632**2 / 5)\n",
    "    clean_steps = [x for x in range(cloud_probs.shape[0]) if x not in cloud_steps]\n",
    "    print(f\"Removing {len(cloud_steps)} from S2 download, saving {7.32 * len(cloud_steps)} PU\")\n",
    "\n",
    "    shadow_img = shadow_request.get_data(data_filter = clean_steps)\n",
    "    shadow_img = np.array(shadow_img)\n",
    "    print(\"Shadows_shape: {}\".format(shadow_img.shape))\n",
    "    print(np.max(shadow_img))\n",
    "    shadow_img = shadow_img / 65535\n",
    "    print(np.max(shadow_img))\n",
    " \n",
    "    cloud_probs = np.delete(cloud_probs, cloud_steps, 0)\n",
    "    shadows = mcm_shadow_mask(np.array(shadow_img), cloud_probs)\n",
    "\n",
    "    print(f\"Cloud probs: {cloud_probs.shape}\")\n",
    "    print(f\"Shadow shape {shadows.shape}\")\n",
    "    return cloud_img, cloud_probs, shadows, clean_steps\n",
    "\n",
    "    \n",
    "    \n",
    "def download_dem(bbox, epsg):\n",
    "\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    dem_s = (630)+4\n",
    "    dem_request = WmsRequest(data_source=DataSource.DEM,\n",
    "                         layer='DEM',\n",
    "                         bbox=box,\n",
    "                         width=dem_s,\n",
    "                         height=dem_s,\n",
    "                         instance_id=API_KEY,\n",
    "                         image_format=MimeType.TIFF_d32f,\n",
    "                         custom_url_params={CustomUrlParam.SHOWLOGO: False})\n",
    "    dem_image = dem_request.get_data()[0]\n",
    "    dem_image = calcSlope(dem_image.reshape((1, dem_s, dem_s)),\n",
    "                  np.full((dem_s, dem_s), 10), np.full((dem_s, dem_s), 10), zScale = 1, minSlope = 0.02)\n",
    "    dem_image = dem_image.reshape((dem_s,dem_s, 1))\n",
    "    dem_image = dem_image[1:dem_s-1, 1:dem_s-1, :]\n",
    "    return dem_image #/ np.max(dem_image)\n",
    " \n",
    "\n",
    "def download_layer(bbox,  clean_steps, epsg, dates = dates, year = 2019):\n",
    "    \"\"\" Downloads the L2A sentinel layer with 10 and 20 meter bands\n",
    "        \n",
    "        Parameters:\n",
    "         bbox (list): output of calc_bbox\n",
    "         epsg (float): EPSG associated with bbox \n",
    "         time (tuple): YY-MM-DD - YY-MM-DD bounds for downloading \n",
    "    \n",
    "        Returns:\n",
    "         img (arr):\n",
    "         img_request (obj): \n",
    "    \"\"\"\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    image_request = WcsRequest(\n",
    "            layer='L2A20',\n",
    "            bbox=box,\n",
    "            time=dates,\n",
    "            image_format = MimeType.TIFF_d16,\n",
    "            maxcc=0.7,\n",
    "            resx='10m', resy='10m',\n",
    "            instance_id=API_KEY,\n",
    "            custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'NEAREST',\n",
    "                                constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "            time_difference=datetime.timedelta(hours=72),\n",
    "        )\n",
    "    print(\"Downloading L2A 20m layer\")\n",
    "    img_bands = image_request.get_data(data_filter = clean_steps)\n",
    "    img_20 = np.stack(img_bands)\n",
    "    print(\"Original 20 meter bands size: {}\".format(img_20.shape))\n",
    "    img_20 = resize(img_20, (img_20.shape[0], 632, 632, img_20.shape[-1]), order = 0)\n",
    "\n",
    "    image_request = WcsRequest(\n",
    "            layer='L2A10',\n",
    "            bbox=box,\n",
    "            time=dates,\n",
    "            image_format = MimeType.TIFF_d32f,\n",
    "            maxcc=0.7,\n",
    "            resx='10m', resy='10m',\n",
    "            instance_id=API_KEY,\n",
    "            custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'BICUBIC',\n",
    "                                constants.CustomUrlParam.UPSAMPLING: 'BICUBIC'},\n",
    "            time_difference=datetime.timedelta(hours=72),\n",
    "    )\n",
    "    print(\"Downloading L2A 10m layer\")\n",
    "    img_bands = image_request.get_data(data_filter = clean_steps)\n",
    "    img_10 = np.stack(img_bands)\n",
    "    print(\"Original 10 meter bands size: {}\".format(img_10.shape))\n",
    "    img_10 = resize(img_10, (img_10.shape[0], 632, 632, img_10.shape[-1]), order = 0)\n",
    "\n",
    "    img = np.concatenate([img_10, img_20], axis = -1)\n",
    "    print(f\"Sentinel 2 used {1.3*1.3*img.shape[0]*10*(2/3)} PU\")\n",
    "\n",
    "    image_dates = []\n",
    "    for date in image_request.get_dates():\n",
    "        if date.year == year - 1:\n",
    "            image_dates.append(-365 + starting_days[(date.month-1)] + date.day)\n",
    "        if date.year == year:\n",
    "            image_dates.append(starting_days[(date.month-1)] + date.day)\n",
    "        if date.year == year + 1:\n",
    "            image_dates.append(365 + starting_days[(date.month-1)]+date.day)\n",
    "    image_dates = [val for idx, val in enumerate(image_dates) if idx in clean_steps]\n",
    "    image_dates = np.array(image_dates)\n",
    "\n",
    "    return img, image_dates\n",
    "        \n",
    "        \n",
    "def download_sentinel_1(bbox, epsg, imsize = 632, \n",
    "                        dates = dates, layer = \"SENT\", year = 2019):\n",
    "    #for try_ in range(5):\n",
    "        #try:\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    image_request = WcsRequest(\n",
    "            layer=layer,\n",
    "            bbox=box,\n",
    "            time=dates,\n",
    "            image_format = MimeType.TIFF_d16,\n",
    "            data_source=DataSource.SENTINEL1_IW,\n",
    "            maxcc=1.0,\n",
    "            resx='5m', resy='5m',\n",
    "            instance_id=API_KEY,\n",
    "            custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'NEAREST',\n",
    "                                constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "            time_difference=datetime.timedelta(hours=96),\n",
    "        )\n",
    "    data_filter = None\n",
    "    if len(image_request.download_list) > 50:\n",
    "        data_filter = [x for x in range(len(image_request.download_list)) if x % 2 == 0]\n",
    "    img_bands = image_request.get_data(data_filter = data_filter)\n",
    "    s1 = np.stack(img_bands)\n",
    "    s1 = resize(s1, (s1.shape[0], imsize*2, imsize*2, s1.shape[-1]), order = 0)\n",
    "    s1 = np.reshape(s1, (s1.shape[0], s1.shape[1]//2, 2, s1.shape[2] // 2, 2, s1.shape[-1]))\n",
    "    s1 = np.mean(s1, (2, 4))\n",
    "    print(f\"Sentinel 1 used {1.3*1.3*s1.shape[0]*2*(2/3)} PU for \\\n",
    "          {s1.shape[0]} out of {len(image_request.download_list)} images\")\n",
    "\n",
    "    image_dates = []\n",
    "    for date in image_request.get_dates():\n",
    "        if date.year == year - 1:\n",
    "            image_dates.append(-365 + starting_days[(date.month-1)] + date.day)\n",
    "        if date.year == year:\n",
    "            image_dates.append(starting_days[(date.month-1)] + date.day)\n",
    "        if date.year == year + 1:\n",
    "            image_dates.append(365 + starting_days[(date.month-1)]+date.day)\n",
    "    image_dates = np.array(image_dates)\n",
    "    s1c = np.copy(s1)\n",
    "    s1c[np.where(s1c < 1.)] = 0\n",
    "    n_pix_oob = np.sum(s1c, axis = (1, 2, 3))\n",
    "    to_remove = np.argwhere(n_pix_oob > (imsize*2*imsize*2)/50)\n",
    "    s1 = np.delete(s1, to_remove, 0)\n",
    "    image_dates = np.delete(image_dates, to_remove)\n",
    "    return s1, image_dates\n",
    "\n",
    "        #except Exception as e:\n",
    "            #logging.fatal(e, exc_info=True)\n",
    "            #sleep((try_+1)*30)\n",
    "\n",
    "def identify_s1_layer(coords):\n",
    "    results = rg.search(coords)\n",
    "    country = results[-1]['cc']\n",
    "    continent_name = pc.country_alpha2_to_continent_code(country)\n",
    "    if continent_name in ['AF', 'OC']:\n",
    "        layer = \"SENT\"\n",
    "    if continent_name in ['SA']:\n",
    "        if coords[0] > -7.11:\n",
    "            layer = \"SENT\"\n",
    "        else:\n",
    "            layer = \"SENT_DESC\"\n",
    "    if continent_name in ['AS']:\n",
    "        if coords[0] > 23.3:\n",
    "            layer = \"SENT\"\n",
    "        else:\n",
    "            layer = \"SENT_DESC\"\n",
    "    if continent_name in ['NA']:\n",
    "        layer = \"SENT_DESC\"\n",
    "    print(continent_name)\n",
    "    print(layer)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud and shadow removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_cloud_and_shadows(tiles, c_probs, shadows, image_dates):\n",
    "    \"\"\" Interpolates clouds and shadows for each time step with \n",
    "        linear combination of proximal clean time steps for each\n",
    "        region of specified window size\n",
    "        \n",
    "        Parameters:\n",
    "         tiles (arr):\n",
    "         probs (arr): \n",
    "         shadows (arr):\n",
    "         image_dates (list):\n",
    "         wsize (int): \n",
    "    \n",
    "        Returns:\n",
    "         tiles (arr): \n",
    "    \"\"\"\n",
    "    wsize = 5\n",
    "    c_probs = c_probs - np.min(c_probs, axis = 0)\n",
    "    c_probs[np.where(c_probs > 0.33)] = 1.\n",
    "    c_probs[np.where(c_probs < 0.33)] = 0.\n",
    "    c_probs = np.reshape(c_probs, (c_probs.shape[0], 632//8, 8, 632//8, 8))\n",
    "    c_probs = np.sum(c_probs, (2, 4))\n",
    "    c_probs = resize(c_probs, (c_probs.shape[0], 632, 632), 0)\n",
    "    c_probs[np.where(c_probs < 16)] = 0\n",
    "    c_probs[np.where(c_probs >= 16)] = 1\n",
    "    secondary_c_probs = np.copy(c_probs)\n",
    "    c_probs += shadows\n",
    "    c_probs[np.where(c_probs >= 1.)] = 1.\n",
    "    number_interpolated = 0\n",
    "    for cval in tnrange(0, IMSIZE - 4, 2):\n",
    "        for rval in range(0, IMSIZE - 4, 2):\n",
    "            subs = c_probs[:, cval:cval + wsize, rval:rval+wsize]\n",
    "            sums = np.sum(subs, axis = (1, 2))\n",
    "            satisfactory = [x for x in range(c_probs.shape[0]) if sums[x] < 8]\n",
    "            if len(satisfactory) == 0:\n",
    "                satisfactory = [x for x in range(c_probs.shape[0])]\n",
    "            satisfactory = np.array(satisfactory)\n",
    "            for date in range(0, tiles.shape[0]):\n",
    "                if np.sum(subs[date, :, :]) > 8:\n",
    "                    number_interpolated += 1\n",
    "                    before, after = calculate_proximal_steps(date, satisfactory)\n",
    "                    before = date + before\n",
    "                    after = date + after\n",
    "                    bef = tiles[before, cval:cval+wsize, rval:rval+wsize, : ]\n",
    "                    aft = tiles[after, cval:cval+wsize, rval:rval+wsize, : ]\n",
    "                    before = image_dates[before]\n",
    "                    after = image_dates[after]\n",
    "                    before_diff = abs(image_dates[date] - before)\n",
    "                    after_diff = abs(image_dates[date] - after)\n",
    "                    bef_wt = 1 - before_diff / (before_diff + after_diff)\n",
    "                    aft_wt = 1 - bef_wt\n",
    "                    candidate = bef_wt*bef + aft_wt*aft\n",
    "                    candidate = candidate*c_arr + tiles[date, cval:cval+wsize, rval:rval+wsize, : ]*o_arr\n",
    "                    tiles[date, cval:cval+wsize, rval:rval+wsize, : ] = candidate \n",
    "    print(\"A total of {} pixels were interpolated\".format(number_interpolated))\n",
    "    return tiles, c_probs, secondary_c_probs\n",
    "\n",
    "def remove_missed_clouds(img):\n",
    "    iqr = np.percentile(img[:, :, :, 3].flatten(), 75) - np.percentile(img[:, :, :, 3].flatten(), 25)\n",
    "    thresh_t = np.percentile(img[:, :, :, 3].flatten(), 75) + iqr*2\n",
    "    thresh_b = np.percentile(img[:, :, :, 3].flatten(), 25) - iqr*2\n",
    "    diffs_fw = np.diff(img, 1, axis = 0)\n",
    "    diffs_fw = np.mean(diffs_fw, axis = (1, 2, 3))\n",
    "    diffs_fw = np.array([0] + list(diffs_fw))\n",
    "    diffs_bw = np.diff(np.flip(img, 0), 1, axis = 0)\n",
    "    diffs_bw = np.flip(np.mean(diffs_bw, axis = (1, 2, 3)))\n",
    "    diffs_bw = np.array(list(diffs_bw) + [0])\n",
    "    diffs = abs(diffs_fw - diffs_bw) * 100\n",
    "    #diffs = [int(x) for x in diffs]\n",
    "    outlier_percs = []\n",
    "    for step in range(img.shape[0]):\n",
    "        bottom = len(np.argwhere(img[step, :, :, 3].flatten() > thresh_t))\n",
    "        top = len(np.argwhere(img[step, :, :, 3].flatten() < thresh_b))\n",
    "        p = 100* ((bottom + top) / (IMSIZE*IMSIZE))\n",
    "        outlier_percs.append(p)\n",
    "    to_remove = np.argwhere(np.array(outlier_percs) > 20)\n",
    "    return to_remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data interpolation\n",
    "\n",
    "Because the `smooth` function is called 5.6 million times on each 4000 hectare array, most of the computations are done outside of the function with the predefined `lmbd` and `d`, and then passed in as the `coefmat`, to avoid needless recomputation. This saves 141 CPU minutes per 4000 hectare array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmbd = 800\n",
    "diagonals = np.zeros(2*2+1)\n",
    "diagonals[2] = 1.\n",
    "for i in range(2):\n",
    "    diff = diagonals[:-1] - diagonals[1:]\n",
    "    diagonals = diff\n",
    "offsets = np.arange(2+1)\n",
    "shape = (70, 72)\n",
    "E = sparse.eye(72, format = 'csc')\n",
    "D = scipy.sparse.diags(diagonals, offsets, shape)\n",
    "D = D.conj().T.dot(D) * lmbd\n",
    "coefmat = E + D\n",
    "\n",
    "def smooth(y, coefmat = coefmat):\n",
    "    ''' \n",
    "    Apply whittaker smoothing to a 1-dimensional array, returning a 1-dimensional array\n",
    "    '''\n",
    "    return splu(coefmat).solve(np.array(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5670243730128277\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "def unpacking_apply_along_axis(all_args):\n",
    "    (func1d, axis, arr, args, kwargs) = all_args\n",
    "    return np.apply_along_axis(func1d, axis, arr)\n",
    "\n",
    "def parallel_apply_along_axis(func1d, axis, arr, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Like numpy.apply_along_axis(), but takes advantage of multiple\n",
    "    cores.\n",
    "    \"\"\"        \n",
    "    # Effective axis where apply_along_axis() will be applied by each\n",
    "    # worker (any non-zero axis number would work, so as to allow the use\n",
    "    # of `np.array_split()`, which is only done on axis 0):\n",
    "    effective_axis = 1 if axis == 0 else axis\n",
    "    if effective_axis != axis:\n",
    "        arr = arr.swapaxes(axis, effective_axis)\n",
    "\n",
    "    # Chunks for the mapping (only a few chunks):\n",
    "    chunks = [(func1d, effective_axis, sub_arr, args, kwargs)\n",
    "              for sub_arr in np.array_split(arr, 4)]\n",
    "\n",
    "    pool = multiprocessing.Pool(4)\n",
    "    individual_results = pool.map(unpacking_apply_along_axis, chunks)\n",
    "    # Freeing the workers:\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    individual_results = np.concatenate(individual_results)\n",
    "    if effective_axis != axis:\n",
    "        individual_results = individual_results.swapaxes(axis, effective_axis)\n",
    "    return np.concatenate(individual_results)\n",
    "\n",
    "test = np.ones((72, 10000))\n",
    "s = timer()\n",
    "x = parallel_apply_along_axis(smooth, 0, test)\n",
    "e = timer()\n",
    "print(e - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.048263742995914\n"
     ]
    }
   ],
   "source": [
    "test = np.ones((72, 10000))\n",
    "s = timer()\n",
    "x = np.apply_along_axis(smooth, 0, test)\n",
    "e = timer()\n",
    "print(e - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_and_save_best_images(img_bands, image_dates):\n",
    "    \"\"\" Interpolate input data of (Time, X, Y, Band) to a constant\n",
    "        (72, X, Y, Band) shape with one time step every five days\n",
    "        \n",
    "        Parameters:\n",
    "         img_bands (arr):\n",
    "         image_dates (list):\n",
    "         \n",
    "        Returns:\n",
    "         keep_steps (arr):\n",
    "         max_distance (int)\n",
    "    \"\"\"\n",
    "\n",
    "    biweekly_dates = [day for day in range(0, 360, 5)] # ideal imagery dates are every 15 days\n",
    "    \n",
    "    # Identify the dates where there is < 20% cloud cover\n",
    "    #satisfactory_ids = list(np.argwhere(np.array(means) < 4.).reshape(-1, )) \n",
    "    satisfactory_ids = [x for x in range(0, img_bands.shape[0])]\n",
    "    satisfactory_dates = [value for idx, value in enumerate(image_dates) if idx in satisfactory_ids]\n",
    "    \n",
    "    \n",
    "    selected_images = {}\n",
    "    for i in biweekly_dates:\n",
    "        distances = [abs(date - i) for date in satisfactory_dates]\n",
    "        closest = np.min(distances)\n",
    "        closest_id = np.argmin(distances)\n",
    "        # If there is imagery within 8 days, select it\n",
    "        if closest < 8:\n",
    "            date = satisfactory_dates[closest_id]\n",
    "            image_idx = int(np.argwhere(np.array(image_dates) == date)[0])\n",
    "            selected_images[i] = {'image_date': [date], 'image_ratio': [1], 'image_idx': [image_idx]}\n",
    "        # If there is not imagery within 8 days, look for the closest above and below imagery\n",
    "        else:\n",
    "            distances = np.array([(date - i) for date in satisfactory_dates])\n",
    "            # Number of days above and below the selected date of the nearest clean imagery\n",
    "            above = distances[np.where(distances < 0, distances, -np.inf).argmax()]\n",
    "            below = distances[np.where(distances > 0, distances, np.inf).argmin()]\n",
    "            if abs(above) > 240: # If date is the last date, occassionally argmax would set above to - number\n",
    "                above = below\n",
    "            if abs(below) > 240:\n",
    "                below = above\n",
    "            if above != below:\n",
    "                below_ratio = above / (above - below)\n",
    "                above_ratio = 1 - below_ratio\n",
    "            else:\n",
    "                above_ratio = below_ratio = 0.5\n",
    "                \n",
    "            # Extract the image date and imagery index for the above and below values\n",
    "            above_date = i + above\n",
    "            above_image_idx = int(np.argwhere(np.array(image_dates) == above_date)[0])\n",
    "            \n",
    "            below_date = i + below\n",
    "            below_image_idx = int(np.argwhere(np.array(image_dates) == below_date)[0])\n",
    "            \n",
    "            selected_images[i] = {'image_date': [above_date, below_date], 'image_ratio': [above_ratio, below_ratio],\n",
    "                                 'image_idx': [above_image_idx, below_image_idx]}\n",
    "                            \n",
    "    max_distance = 0\n",
    "    \n",
    "    for i in selected_images.keys():\n",
    "        if len(selected_images[i]['image_date']) == 2:\n",
    "            dist = selected_images[i]['image_date'][1] - selected_images[i]['image_date'][0]\n",
    "            if dist > max_distance:\n",
    "                max_distance = dist\n",
    "    \n",
    "    print(\"Maximum time distance: {}\".format(max_distance))\n",
    "        \n",
    "    keep_steps = []\n",
    "    for i in selected_images.keys():\n",
    "        info = selected_images[i]\n",
    "        if len(info['image_idx']) == 1:\n",
    "            step = img_bands[info['image_idx'][0]]\n",
    "        if len(info['image_idx']) == 2:\n",
    "            step1 = img_bands[info['image_idx'][0]] * 0.5#info['image_ratio'][0]\n",
    "            step2 = img_bands[info['image_idx'][1]] * 0.5 #info['image_ratio'][1]\n",
    "            step = step1 + step2\n",
    "        keep_steps.append(step)\n",
    "        \n",
    "    keep_steps = np.stack(keep_steps)\n",
    "    return keep_steps\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiling and coordinate selection functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbolic Model Created.\n"
     ]
    }
   ],
   "source": [
    "MDL_PATH = \"../src/dsen2/models/\"\n",
    "\n",
    "input_shape = ((4, None, None), (6, None, None))\n",
    "model = s2model(input_shape, num_layers=6, feature_size=128)\n",
    "predict_file = MDL_PATH+'s2_032_lr_1e-04.hdf5'\n",
    "print('Symbolic Model Created.')\n",
    "\n",
    "model.load_weights(predict_file)\n",
    "\n",
    "def DSen2(d10, d20):\n",
    "    \"\"\"Super resolves 20 meter bans using the DSen2 convolutional\n",
    "       neural network, as specified in Lanaras et al. 2018\n",
    "       https://github.com/lanha/DSen2\n",
    "\n",
    "        Parameters:\n",
    "         d10 (arr): (4, X, Y) shape array with 10 meter resolution\n",
    "         d20 (arr): (6, X, Y) shape array with 20 meter resolution\n",
    "\n",
    "        Returns:\n",
    "         prediction (arr): (6, X, Y) shape array with 10 meter superresolved\n",
    "                          output of DSen2 on d20 array\n",
    "    \"\"\"\n",
    "    test = [d10, d20]\n",
    "    input_shape = ((4, None, None), (6, None, None))\n",
    "    prediction = _predict(test, input_shape, deep=False)\n",
    "    #prediction *= 5\n",
    "    return prediction\n",
    "\n",
    "def _predict(test, input_shape, model = model, deep=False, run_60=False):\n",
    "    \n",
    "    prediction = model.predict(test, verbose=1)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_per_month = [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30]\n",
    "starting_days = np.cumsum(days_per_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_proximal_steps(date, satisfactory):\n",
    "    arg_before = None\n",
    "    arg_after = None\n",
    "    if date > 0:\n",
    "        idx_before = satisfactory - date\n",
    "        arg_before = idx_before[np.where(idx_before < 0, idx_before, -np.inf).argmax()]\n",
    "    if date < np.max(satisfactory):\n",
    "        idx_after = satisfactory - date\n",
    "        arg_after = idx_after[np.where(idx_after > 0, idx_after, np.inf).argmin()]\n",
    "    if not arg_after and not arg_before:\n",
    "        arg_after = date\n",
    "        arg_before = date\n",
    "    if not arg_after:\n",
    "        arg_after = arg_before\n",
    "    if not arg_before:\n",
    "        arg_before = arg_after\n",
    "    #print(arg_before, date, arg_after)\n",
    "    return arg_before, arg_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def mcm_shadow_mask(arr, c_probs):\n",
    "    \"\"\" Calculates the multitemporal shadow mask for Sentinel-2 using\n",
    "        the methods from Candra et al. 2020 on L1C images and matching\n",
    "        outputs to the s2cloudless cloud probabilities\n",
    "\n",
    "        Parameters:\n",
    "         arr (arr): (Time, X, Y, Band) array of L1C data scaled from [0, 1]\n",
    "         c_probs (arr): (Time, X, Y) array of S2cloudless cloud probabilities\n",
    "    \n",
    "        Returns:\n",
    "         shadows_new (arr): cloud mask after Candra et al. 2020 and cloud matching \n",
    "         shadows_original (arr): cloud mask after Candra et al. 2020\n",
    "    \"\"\"\n",
    "    def _rank_array(arr):\n",
    "        order = arr.argsort()\n",
    "        ranks = order.argsort()\n",
    "        return ranks\n",
    "    mean_c_probs = np.mean(c_probs, axis = (1, 2))\n",
    "    cloudy_steps = np.argwhere(mean_c_probs > 0.25)\n",
    "    images_clean = np.delete(arr, cloudy_steps, 0)\n",
    "    cloud_ranks = _rank_array(mean_c_probs)\n",
    "    diffs = abs(np.sum(arr - np.mean(images_clean, axis = 0), axis = (1, 2, 3)))\n",
    "    diff_ranks = _rank_array(diffs)\n",
    "    overall_rank = diff_ranks + cloud_ranks\n",
    "    reference_idx = np.argmin(overall_rank)\n",
    "    ri = arr[reference_idx]\n",
    "        \n",
    "    shadows = np.zeros((arr.shape[0], 632, 632))    \n",
    "    # Candra et al. 2020\n",
    "    \n",
    "    for time in tnrange(arr.shape[0]):\n",
    "        for x in range(arr.shape[1]):\n",
    "            for y in range(arr.shape[2]):\n",
    "                ti_slice = arr[time, x, y]\n",
    "                ri_slice = ri[x, y]\n",
    "                deltab2 = ti_slice[0] - ri_slice[0]\n",
    "                deltab8a = ti_slice[1] - ri_slice[1]\n",
    "                deltab11 = ti_slice[2] - ri_slice[2]\n",
    "\n",
    "                if deltab2 < 0.10: #(1000/65535):\n",
    "                    if deltab8a < -0.04: #(-400/65535):\n",
    "                        if deltab11 < -0.04: #(-400/65535):\n",
    "                            if ti_slice[0] < 0.095: #(950/65535):\n",
    "                                shadows[time, x, y] = 1.\n",
    "                                                       \n",
    "                            \n",
    "    # Remove shadows if cannot coreference a cloud\n",
    "    shadow_large = np.reshape(shadows, (shadows.shape[0], 79, 8, 79, 8))\n",
    "    shadow_large = np.sum(shadow_large, axis = (2, 4))\n",
    "    \n",
    "    cloud_large = np.copy(c_probs)\n",
    "    cloud_large[np.where(c_probs > 0.33)] = 1.\n",
    "    cloud_large[np.where(c_probs < 0.33)] = 0.\n",
    "    cloud_large = np.reshape(cloud_large, (shadows.shape[0], 79, 8, 79, 8))\n",
    "    cloud_large = np.sum(cloud_large, axis = (2, 4))\n",
    "    for time in tnrange(shadow_large.shape[0]):\n",
    "        for x in range(shadow_large.shape[1]):\n",
    "            x_low = np.max([x - 8, 0])\n",
    "            x_high = np.min([x + 8, shadow_large.shape[1] - 1])\n",
    "            for y in range(shadow_large.shape[2]):\n",
    "                y_low = np.max([y - 8, 0])\n",
    "                y_high = np.min([y + 8, shadow_large.shape[1] - 1])\n",
    "                if shadow_large[time, x, y] < 8:\n",
    "                    shadow_large[time, x, y] = 0.\n",
    "                if shadow_large[time, x, y] >= 8:\n",
    "                    shadow_large[time, x, y] = 1.\n",
    "                c_prob_window = cloud_large[time, x_low:x_high, y_low:y_high]\n",
    "                if np.max(c_prob_window) < 16:\n",
    "                    shadow_large[time, x, y] = 0.\n",
    "                    \n",
    "    shadow_large = resize(shadow_large, (shadow_large.shape[0], 632, 632), order = 0)\n",
    "    shadows *= shadow_large\n",
    "    \n",
    "    # Go through and aggregate the shadow map to an 80m grid, and extend it one grid size around\n",
    "    # any positive ID\n",
    "    \n",
    "    shadows = np.reshape(shadows, (shadows.shape[0], 79, 8, 79, 8))\n",
    "    shadows = np.sum(shadows, axis = (2, 4))\n",
    "    shadows[np.where(shadows < 12)] = 0.\n",
    "    shadows[np.where(shadows >= 12)] = 1.\n",
    "    \n",
    "    shadows = resize(shadows, (shadows.shape[0], 632, 632), order = 0)\n",
    "    shadows = np.reshape(shadows, (shadows.shape[0], 632//4, 4, 632//4, 4))\n",
    "    shadows = np.max(shadows, (2, 4))\n",
    "    \n",
    "    shadows_new = np.zeros_like(shadows)\n",
    "    for time in range(shadows.shape[0]):\n",
    "        for x in range(shadows.shape[1]):\n",
    "            for y in range(shadows.shape[2]):\n",
    "                if shadows[time, x, y] == 1:\n",
    "                    min_x = np.max([x - 1, 0])\n",
    "                    max_x = np.min([x + 2, 157])\n",
    "                    min_y = np.max([y - 1, 0])\n",
    "                    max_y = np.min([y + 2, 157])\n",
    "                    for x_idx in range(min_x, max_x):\n",
    "                        for y_idx in range(min_y, max_y):\n",
    "                            shadows_new[time, x_idx, y_idx] = 1.\n",
    "    shadows_new = resize(shadows_new, (shadows.shape[0], 632, 632), order = 0)\n",
    "    return shadows_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_output_and_temp_folders(idx, output_folder = OUTPUT_FOLDER):\n",
    "    \n",
    "    def _find_and_make_dirs(dirs):\n",
    "        if not os.path.exists(os.path.realpath(dirs)):\n",
    "            os.makedirs(os.path.realpath(dirs))\n",
    "            \n",
    "    _find_and_make_dirs(output_folder + \"raw/\")\n",
    "    _find_and_make_dirs(output_folder + \"raw/clouds/\")\n",
    "    _find_and_make_dirs(output_folder + \"raw/s1/\")\n",
    "    _find_and_make_dirs(output_folder + \"raw/s2/\")\n",
    "    _find_and_make_dirs(output_folder + \"raw/misc/\")\n",
    "    _find_and_make_dirs(output_folder + \"processed/\")\n",
    "    \n",
    "\n",
    "def check_contains(coord, step_x, step_y, folder = OUTPUT_FOLDER, year = 2019, s1_layer = \"SENT\"):\n",
    "    contains = False\n",
    "    bottomleft = offset_x(coord, step_x*6300)\n",
    "    bottomleft = offset_y(bottomleft , step_y*6300)\n",
    "    for subtile in range(1, 5, 1):\n",
    "        bottomright = offset_x(bottomleft, (subtile*6300) / 5)\n",
    "        bottomright = offset_y(bottomright, (subtile*6300) / 5)\n",
    "        #bbx = calculate_bbx(coord, step_x * (subtile/10), step_y * (subtile / 10), expansion = 10)\n",
    "        if os.path.exists(folder):\n",
    "            if any([x.endswith(\".geojson\") for x in os.listdir(folder)]):\n",
    "                geojson_path = folder + [x for x in os.listdir(folder) if x.endswith(\".geojson\")][0]\n",
    "                bool_contains = pts_in_geojson(lats = [bottomleft[0], bottomright[0]], \n",
    "                                               longs = [bottomleft[1], bottomright[1]],\n",
    "                                               geojson = geojson_path)\n",
    "               # bool_contains = pts_in_geojson([bbx[0][0], bbx[1][0]],\n",
    "                #                          [bbx[0][1], bbx[1][1]],\n",
    "                #                          geojson_path)\n",
    "                contains = True if bool_contains else contains\n",
    "            else:\n",
    "                contains = True\n",
    "    return contains\n",
    "\n",
    "def download_large_tile(coord, step_x, step_y, folder = OUTPUT_FOLDER, year = 2019, s1_layer = \"SENT\"):\n",
    "    \n",
    "    bbx, epsg = calculate_bbx_pyproj(coord, step_x, step_y, expansion = 10)\n",
    "    dem_bbx, _ = calculate_bbx_pyproj(coord, step_x, step_y, expansion = 20)\n",
    "\n",
    "    #bbx = calculate_bbx(coord, step_x, step_y, expansion = 10)\n",
    "    #dem_bbx = calculate_bbx(coord, step_x, step_y, expansion = 20)\n",
    "    idx = str(step_y) + \"_\" + str(step_x)\n",
    "    print(idx)\n",
    "    idx = str(idx)\n",
    "    make_output_and_temp_folders(idx)\n",
    "\n",
    "    print(\"Calculating cloud cover\")\n",
    "    if not os.path.exists(folder + \"processed/\" + str(((step_y+1)*5)-1) + \"/\" + str(((step_x+1)*5)-1) + \".hkl\"):\n",
    "        if not os.path.exists(folder + \"raw/clouds/clouds_{}.hkl\".format(idx)):\n",
    "            l1c, cloud_probs, shadows, clean_steps = identify_clouds_new(bbx, epsg = epsg)\n",
    "            hkl.dump(cloud_probs, folder + \"raw/clouds/clouds_{}.hkl\".format(idx), mode='w', compression='gzip')\n",
    "            hkl.dump(shadows, folder + \"raw/clouds/shadows_{}.hkl\".format(idx), mode='w', compression='gzip')\n",
    "\n",
    "        if not os.path.exists(folder + \"raw/s1/{}.hkl\".format(idx)):\n",
    "            print(\"Downloading S1\")\n",
    "            s1_layer = identify_s1_layer((coord[1], coord[0]))\n",
    "            s1, s1_dates = download_sentinel_1(bbx, layer = s1_layer, epsg = epsg)\n",
    "            s1 = process_sentinel_1_tile(s1, s1_dates)\n",
    "            hkl.dump(s1, folder + \"raw/s1/{}.hkl\".format(idx), mode='w', compression='gzip')\n",
    "            hkl.dump(s1_dates, folder + \"raw/misc/s1_dates_{}.hkl\".format(idx), mode='w', compression='gzip')\n",
    "\n",
    "        if not os.path.exists(folder + \"raw/s2/{}.hkl\".format(idx)):\n",
    "            print(\"Downloading S2\")\n",
    "            s2, s2_dates = download_layer(bbx, clean_steps = clean_steps, epsg = epsg)\n",
    "            hkl.dump(s2, folder + \"raw/s2/{}.hkl\".format(idx), mode='w', compression='gzip')\n",
    "            hkl.dump(s2_dates, folder + \"raw/misc/s2_dates_{}.hkl\".format(idx), mode='w', compression='gzip')\n",
    "\n",
    "        if not os.path.exists(folder + \"raw/misc/dem_{}.hkl\".format(idx)):\n",
    "            print(\"Downloading DEM\")\n",
    "            dem = download_dem(dem_bbx, epsg = epsg) # get the DEM BBOX\n",
    "            hkl.dump(dem, folder + \"raw/misc/dem_{}.hkl\".format(idx), mode='w', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bad_steps(sentinel2, clouds):\n",
    "    n_cloud_px = np.array([len(np.argwhere(clouds[x, :, :].reshape((632)*(632)) > 0.33)) for x in range(clouds.shape[0])])\n",
    "    cloud_steps = np.argwhere(n_cloud_px > 632**2 / 5)\n",
    "    missing_images = [np.argwhere(sentinel2[x, :, : :10].flatten() == 0.0) for x in range(sentinel2.shape[0])]\n",
    "    missing_images = np.array([len(x) for x in missing_images])\n",
    "    missing_images_p = [np.argwhere(sentinel2[x, :, : :10].flatten() >= 1) for x in range(sentinel2.shape[0])]\n",
    "    missing_images_p = np.array([len(x) for x in missing_images_p])\n",
    "    missing_images += missing_images_p\n",
    "    missing_images = np.argwhere(missing_images >= (632**2) / 50)\n",
    "    to_remove = np.unique(np.concatenate([cloud_steps.flatten(), missing_images.flatten()]))\n",
    "    return to_remove\n",
    "\n",
    "def superresolve(sentinel2):\n",
    "    d10 = sentinel2[:, :, :, 0:4]\n",
    "    d20 = sentinel2[:, :, :, 4:10]\n",
    "\n",
    "    d10 = np.swapaxes(d10, 1, -1)\n",
    "    d10 = np.swapaxes(d10, 2, 3)\n",
    "    d20 = np.swapaxes(d20, 1, -1)\n",
    "    d20 = np.swapaxes(d20, 2, 3)\n",
    "    superresolved = DSen2(d10, d20)\n",
    "    superresolved = np.swapaxes(superresolved, 1, -1)\n",
    "    superresolved = np.swapaxes(superresolved, 1, 2)\n",
    "    return superresolved # returns band IDXs 3, 4, 5, 7, 8, 9\n",
    "\n",
    "def process_sentinel_1_tile(sentinel1, dates):\n",
    "    s1 = calculate_and_save_best_images(sentinel1, dates)\n",
    "    # Retain only iamgery every 15 days\n",
    "    biweekly_dates = np.array([day for day in range(0, 360, 5)])\n",
    "    to_remove = np.argwhere(biweekly_dates % 15 != 0)\n",
    "    s1 = np.delete(s1, to_remove, 0)\n",
    "    return s1\n",
    "\n",
    "\n",
    "def interpolate_array_master(x):\n",
    "    no_dem = np.delete(x, 10, -1)\n",
    "    no_dem = np.reshape(no_dem, (72, 128*128*14))\n",
    "    no_dem = parallel_apply_along_axis(smooth, 0, no_dem)\n",
    "    #no_dem = np.apply_along_axis(smooth, 0, no_dem)\n",
    "    no_dem = np.reshape(no_dem, (72, 128, 128, 14))\n",
    "    x[:, :, :, :10] = no_dem[:, :, :, :10]\n",
    "    x[:, :, :, 11:] = no_dem[:, :, :, 10:]\n",
    "\n",
    "    biweekly_dates = np.array([day for day in range(0, 360, 5)])\n",
    "    to_remove = np.argwhere(biweekly_dates % 15 != 0)\n",
    "    x = np.delete(x, to_remove, 0)\n",
    "    return x\n",
    "    \n",
    "        \n",
    "\n",
    "def interpolate_array(x):\n",
    "    no_dem = np.delete(x, 10, -1)\n",
    "    no_dem = np.reshape(no_dem, (72, 128*128*14))\n",
    "    no_dem = np.swapaxes(no_dem, 0, 1)\n",
    "\n",
    "    pool = multiprocessing.Pool(6)\n",
    "    no_dem = pool.map(smooth, no_dem)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    no_dem = np.swapaxes(no_dem, 0, 1)\n",
    "    no_dem = np.reshape(no_dem, (72, 128, 128, 14))\n",
    "    x[:, :, :, :10] = no_dem[:, :, :, :10]\n",
    "    x[:, :, :, 11:] = no_dem[:, :, :, 10:]\n",
    "\n",
    "    biweekly_dates = np.array([day for day in range(0, 360, 5)])\n",
    "    to_remove = np.argwhere(biweekly_dates % 15 != 0)\n",
    "    x = np.delete(x, to_remove, 0)\n",
    "    return x\n",
    "\n",
    "def process_large_tile(coord, step_x, step_y, folder = OUTPUT_FOLDER):\n",
    "    bbx = calculate_bbx(coord, step_x, step_y, expansion = 10)\n",
    "    idx = str(step_y) + \"_\" + str(step_x)\n",
    "    x_vals = []\n",
    "    y_vals = []\n",
    "    # save to disk\n",
    "    for i in range(25):\n",
    "        y_val = (24 - i) // 5\n",
    "        x_val = 5 - ((25 - i) % 5)\n",
    "        x_val = 0 if x_val == 5 else x_val\n",
    "        x_vals.append(x_val)\n",
    "        y_vals.append(y_val)\n",
    "\n",
    "    y_vals = [i + (5*step_y) for i in y_vals]\n",
    "    x_vals = [i + (5*step_x) for i in x_vals]\n",
    "\n",
    "    processed = False\n",
    "    for x, y in zip(x_vals, y_vals):\n",
    "        if os.path.exists(folder + \"processed/{}/{}.hkl\".format(str(y), str(x))):\n",
    "            processed = True\n",
    "    if not processed:\n",
    "\n",
    "        clouds = hkl.load(folder + \"raw/clouds/clouds_{}.hkl\".format(idx))\n",
    "        sentinel1 = hkl.load(folder + \"raw/s1/{}.hkl\".format(idx))\n",
    "        radar_dates = hkl.load(folder + \"raw/misc/s1_dates_{}.hkl\".format(idx))\n",
    "        sentinel2 = hkl.load(folder + \"raw/s2/{}.hkl\".format(idx))\n",
    "        dem = hkl.load(folder + \"raw/misc/dem_{}.hkl\".format(idx))\n",
    "        image_dates = hkl.load(folder + \"raw/misc/s2_dates_{}.hkl\".format(idx))\n",
    "        if os.path.exists(folder + \"raw/clouds/shadows_{}.hkl\".format(idx)):\n",
    "            shadows = hkl.load(folder + \"raw/clouds/shadows_{}.hkl\".format(idx))\n",
    "        else:\n",
    "            print(\"No shadows file, so calculating shadows with L2A\")\n",
    "            shadows = mcm_shadow_mask(sentinel2, clouds)\n",
    "        print(\"The files have been loaded\")\n",
    "\n",
    "        #sentinel1 = process_sentinel_1_tile(sentinel1, radar_dates)\n",
    "        to_remove = calculate_bad_steps(sentinel2, clouds)\n",
    "        sentinel2 = np.delete(sentinel2, to_remove, axis = 0)\n",
    "        clouds = np.delete(clouds, to_remove, axis = 0)\n",
    "        shadows = np.delete(shadows, to_remove, axis = 0)\n",
    "        image_dates = np.delete(image_dates, to_remove)\n",
    "        print(\"Cloudy and missing images removed, radar processed\")\n",
    "\n",
    "        to_remove = remove_missed_clouds(sentinel2)\n",
    "        sentinel2 = np.delete(sentinel2, to_remove, axis = 0)\n",
    "        clouds = np.delete(clouds, to_remove, axis = 0)\n",
    "        image_dates = np.delete(image_dates, to_remove)\n",
    "        shadows = np.delete(shadows, to_remove, axis = 0)\n",
    "        print(\"Missed cloudy images removed\")\n",
    "\n",
    "        x, _, _ = remove_cloud_and_shadows(sentinel2, clouds, shadows, image_dates)\n",
    "        print(\"Clouds and shadows interpolated\")\n",
    "\n",
    "        index = 0\n",
    "        for start_x, end_x in zip(range(0, 633, 126), range(128, 633, 126)):\n",
    "            for start_y, end_y in zip(range(0, 633, 126), range(128, 633, 126)):\n",
    "                print(index)\n",
    "                if not os.path.exists(folder + \"processed/{}/{}.hkl\".format(str(y_vals[index]), str(x_vals[index]))):\n",
    "                    subtile = x[:, start_x:end_x, start_y:end_y, :]\n",
    "                    resolved = superresolve(subtile)\n",
    "                    subtile[:, :, :, 4:10] = resolved\n",
    "                    \n",
    "                    dem_i = np.tile(dem[np.newaxis, start_x:end_x, start_y:end_y, :], (x.shape[0], 1, 1, 1))\n",
    "                    subtile = np.concatenate([subtile, dem_i / 90], axis = -1)\n",
    "                    t2 = timer()\n",
    "                    subtile, amin = evi(subtile, verbose = True)\n",
    "                    subtile = bi(subtile, verbose = True)\n",
    "                    subtile = msavi2(subtile, verbose = True)\n",
    "                    subtile = si(subtile, verbose = True)\n",
    "                    t3 = timer()\n",
    "                    print(\"Indices: {}\".format(t3 - t2))\n",
    "\n",
    "                    subtile = calculate_and_save_best_images(subtile, image_dates)\n",
    "                    subtile = interpolate_array_master(subtile)\n",
    "                    t5 = timer()\n",
    "                    print(\"Interpolate: {}\".format(t5 - t3))\n",
    "                    subtile = np.concatenate([subtile, sentinel1[:, start_x:end_x,\n",
    "                                                                start_y:end_y, :]], axis = -1)\n",
    "                    \n",
    "\n",
    "                    out_y_folder = folder + \"processed/{}/\".format(str(y_vals[index]))\n",
    "                    if not os.path.exists(os.path.realpath(out_y_folder)):\n",
    "                        os.makedirs(os.path.realpath(out_y_folder))\n",
    "                    sleep(2)\n",
    "                    hkl.dump(subtile,\n",
    "                             folder + \"processed/{}/{}.hkl\".format(str(y_vals[index]), str(x_vals[index])),\n",
    "                             mode='w', compression='gzip')\n",
    "                    #np.save(folder + \"processed/{}/{}.npy\".format(str(y_vals[index]), str(x_vals[index])), subtile)\n",
    "                index += 1\n",
    "            \n",
    "def clean_up_folders():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15452\n"
     ]
    }
   ],
   "source": [
    "expansion = -10\n",
    "multiplier = 1\n",
    "step_x = 1\n",
    "coord1 = offset_x(coords, 6300*(step_x + multiplier) + expansion)\n",
    "coord1 = offset_y(coord1,6300* (step_x + multiplier) + expansion)\n",
    "calculate_area([coords, coord1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "X: 0 Y:0\n",
      "Downloaded 1\n",
      "32615\n",
      "UTM_15N\n",
      "32615\n",
      "UTM_15N\n",
      "0_0\n",
      "Calculating cloud cover\n",
      "The files have been loaded\n",
      "Cloudy and missing images removed, radar processed\n",
      "Missed cloudy images removed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "478b2e9fc6064727bc6af5696bba383e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=314), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A total of 272085 pixels were interpolated\n",
      "Clouds and shadows interpolated\n",
      "0\n",
      "47/47 [==============================] - 33s 702ms/step\n",
      "There are: 2 out of bounds EVI\n",
      "Indices: 0.27699084900086746\n",
      "Maximum time distance: 35\n",
      "Interpolate: 9.068936088006012\n",
      "1\n",
      "47/47 [==============================] - 32s 672ms/step\n",
      "There are: 9 out of bounds EVI\n",
      "Indices: 0.28106164300697856\n",
      "Maximum time distance: 35\n",
      "Interpolate: 9.91795407400059\n",
      "2\n",
      "47/47 [==============================] - 31s 651ms/step\n",
      "There are: 1 out of bounds EVI\n",
      "Indices: 0.23288311299984343\n",
      "Maximum time distance: 35\n",
      "Interpolate: 8.97697501599032\n",
      "3\n",
      "47/47 [==============================] - 30s 646ms/step\n",
      "There are: 1 out of bounds EVI\n",
      "Indices: 0.2397079880029196\n",
      "Maximum time distance: 35\n",
      "Interpolate: 8.289792280003894\n",
      "4\n",
      "47/47 [==============================] - 30s 640ms/step\n",
      "There are: 61 out of bounds EVI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/john.brandt/Documents/GitHub/restoration-mapper/src/utils/utils-bilinear.py:90: RuntimeWarning: invalid value encountered in power\n",
      "  sis = np.power( (1-BLUE) * (1 - GREEN) * (1 - RED), 1/3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices: 0.27187182000488974\n",
      "Maximum time distance: 35\n",
      "Interpolate: 8.961085863003973\n",
      "5\n",
      "47/47 [==============================] - 30s 643ms/step\n",
      "There are: 6 out of bounds EVI\n",
      "Indices: 0.25602785000228323\n",
      "Maximum time distance: 35\n",
      "Interpolate: 8.615875770003186\n",
      "6\n",
      "47/47 [==============================] - 31s 652ms/step\n",
      "There are: 9 out of bounds EVI\n",
      "Indices: 0.27435737899213564\n",
      "Maximum time distance: 35\n",
      "Interpolate: 8.918672180006979\n",
      "7\n",
      "47/47 [==============================] - 32s 672ms/step\n",
      "There are: 2 out of bounds EVI\n",
      "Indices: 0.23627671400026884\n",
      "Maximum time distance: 35\n",
      "Interpolate: 9.12967439800559\n",
      "8\n",
      "47/47 [==============================] - 31s 668ms/step\n",
      "There are: 0 out of bounds EVI\n",
      "Indices: 0.24340162900625728\n",
      "Maximum time distance: 35\n",
      "Interpolate: 8.268349834994297\n",
      "9\n",
      "47/47 [==============================] - 31s 657ms/step\n",
      "There are: 1 out of bounds EVI\n",
      "Indices: 0.24222063799970783\n",
      "Maximum time distance: 35\n",
      "Interpolate: 8.75698769200244\n",
      "10\n",
      "47/47 [==============================] - 31s 667ms/step\n",
      "There are: 2 out of bounds EVI\n",
      "Indices: 0.24167908199888188\n",
      "Maximum time distance: 35\n",
      "Interpolate: 8.582706113011227\n",
      "11\n",
      "47/47 [==============================] - 30s 643ms/step\n",
      "There are: 23 out of bounds EVI\n",
      "Indices: 0.23846784701163415\n",
      "Maximum time distance: 35\n",
      "Interpolate: 8.319846443991992\n",
      "12\n",
      "47/47 [==============================] - 30s 646ms/step\n",
      "There are: 1 out of bounds EVI\n",
      "Indices: 0.22996323500410654\n",
      "Maximum time distance: 35\n",
      "Interpolate: 8.371801623987267\n",
      "13\n",
      "47/47 [==============================] - 30s 643ms/step\n",
      "There are: 6 out of bounds EVI\n",
      "Indices: 0.24569425500521902\n",
      "Maximum time distance: 35\n",
      "Interpolate: 8.17669148099958\n",
      "14\n",
      "47/47 [==============================] - 30s 640ms/step\n",
      "There are: 1 out of bounds EVI\n",
      "Indices: 0.23906151599658187\n",
      "Maximum time distance: 35\n",
      "Interpolate: 8.387140325008659\n",
      "15\n",
      "47/47 [==============================] - 30s 640ms/step\n",
      "There are: 0 out of bounds EVI\n",
      "Indices: 0.2413284539943561\n",
      "Maximum time distance: 35\n",
      "Interpolate: 8.107546512997942\n",
      "16\n",
      "47/47 [==============================] - 30s 649ms/step\n",
      "There are: 7 out of bounds EVI\n",
      "Indices: 0.2789315879927017\n",
      "Maximum time distance: 35\n",
      "Interpolate: 8.440412454001489\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "downloaded = 0\n",
    "\n",
    "if not os.path.exists(os.path.realpath(OUTPUT_FOLDER)):\n",
    "            os.makedirs(os.path.realpath(OUTPUT_FOLDER))\n",
    "        \n",
    "for x_tile in range(0, 2):\n",
    "    for y_tile in range(0, 2):\n",
    "        contains = check_contains(coords, x_tile, y_tile)\n",
    "        print(contains)\n",
    "        if contains:\n",
    "            print(\"X: {} Y:{}\".format(x_tile, y_tile))\n",
    "            downloaded += 1\n",
    "            print(f\"Downloaded {downloaded}\")\n",
    "            download_large_tile(coord = coords, step_x = x_tile, step_y = y_tile)\n",
    "            process_large_tile(coords, x_tile, y_tile)\n",
    "            print(\"\\n\")\n",
    "            #clean_up_folders(x_tile, y_tile)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_sensing",
   "language": "python",
   "name": "remote_sensing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
