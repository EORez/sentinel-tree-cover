{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from osgeo import ogr, osr\n",
    "from sentinelhub import WmsRequest, WcsRequest, MimeType, CRS, BBox, constants\n",
    "import logging\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import os\n",
    "import yaml\n",
    "from sentinelhub import DataSource\n",
    "import scipy.sparse as sparse\n",
    "import scipy\n",
    "from scipy.sparse.linalg import splu\n",
    "from skimage.transform import resize\n",
    "from sentinelhub import CustomUrlParam\n",
    "from time import time as timer\n",
    "from time import sleep as sleep\n",
    "import multiprocessing\n",
    "import math\n",
    "import reverse_geocoder as rg\n",
    "import pycountry\n",
    "import pycountry_convert as pc\n",
    "import hickle as hkl\n",
    "from shapely.geometry import Point, Polygon\n",
    "import geopandas\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "import math\n",
    "import boto3\n",
    "from pyproj import Proj, transform\n",
    "from timeit import default_timer as timer\n",
    "from typing import Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../config.yaml\", 'r') as stream:\n",
    "    key = (yaml.safe_load(stream))\n",
    "    API_KEY = key['key']\n",
    "    AWSKEY = key['awskey']\n",
    "    AWSSECRET = key['awssecret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../src/preprocessing/slope.py\n",
    "%run ../src/preprocessing/indices.py\n",
    "%run ../src/downloading/utils.py\n",
    "%run ../src/preprocessing/cloud_removal.py\n",
    "%run ../src/preprocessing/whittaker_smoother.py\n",
    "%run ../src/dsen2/utils/DSen2Net.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2018\n",
    "dates = ('{}-12-01'.format(str(year - 1)) , '{}-02-01'.format(str(year + 1)))\n",
    "dates_sentinel_1 = ('{}-01-01'.format(str(year)) , '{}-12-31'.format(str(year)))\n",
    "SIZE = 9*5\n",
    "IMSIZE = (7*2) + (SIZE * 14)+2\n",
    "\n",
    "days_per_month = [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30]\n",
    "starting_days = np.cumsum(days_per_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../tile_data/guatemala-gain-2/ (-89.11829, 15.19648)\n"
     ]
    }
   ],
   "source": [
    "landscapes = {\n",
    "    'ethiopia-tigray': (13.540810, 38.177220),\n",
    "    'kenya-makueni-2': (-1.817109, 37.44563),\n",
    "    'ghana': (9.259359, -0.83375),\n",
    "    'niger-koure': (13.18158, 2.478),\n",
    "    'cameroon-farnorth': (10.596, 14.2722),\n",
    "    'mexico-campeche': (18.232495, -92.1234215),\n",
    "    'malawi-rumphi-old': (-11.044, 33.818),\n",
    "    'malawi-rumphi': (-11.15, 33.246),\n",
    "    'ghana-sisala-east': (10.385, -1.765),\n",
    "    'ghana-west-mamprusi': (10.390084, -0.846330),\n",
    "    'ghana-kwahu': (6.518909, -0.826008),\n",
    "    'senegal-16b': (15.82585, -15.34166),\n",
    "    'india-kochi': (9.909, 76.254),\n",
    "    'india-sidhi': (24.0705, 81.607),\n",
    "    'brazil-esperito-santo': (-20.147, -40.837),\n",
    "    'brazil-paraiba': (-22.559943, -44.186629),\n",
    "    'brazil-goias': (-14.905595, -48.907399),\n",
    "    'colombia-talima': (4.179529, -74.889171),\n",
    "    'drc-kafubu': (-11.749636, 27.586622),\n",
    "    'thailand-khon-kaen': (15.709725, 102.546518),\n",
    "    'indonesia-west-java': (-6.721101, 108.280949),\n",
    "    'madagascar': (-18.960152, 47.469587),\n",
    "    'tanzania': (-6.272258, 36.679824),\n",
    "    'chile': (-36.431237, -71.872030),\n",
    "    'indonesia-jakarta': (-6.352580, 106.677072),\n",
    "    'caf-baboua': (5.765917, 14.791618),   \n",
    "    'honduras': (14.096664, -88.720304),\n",
    "    'nicaragua': (12.398014, -86.963042),\n",
    "    'china': (26.673679, 107.464231),\n",
    "    'australia-west': (-32.666762, 117.411197),\n",
    "    'mexico-sonora': (29.244288, -111.243230),\n",
    "    'south-africa': (-30.981698, 28.727301),\n",
    "    'maldonado-uraguay': (-34.629250, -55.004331),\n",
    "    'dominican-rep-la-salvia': (18.872589, -70.462961),\n",
    "    'guatemala-coban': (15.3, -90.8),\n",
    "    'senegal-tucker-a': (15.350595, -15.459789),\n",
    "    'elsalvador-imposible': (13.727334, -90.015579),\n",
    "    'peru-shatoja-district': (-6.566366, -76.759752),\n",
    "    'angola-galanga': (-12.104782, 15.151222),\n",
    "    'morocco-chefchaouen': (34.942560, -4.772589),\n",
    "    'georgia-imereti': (42.223069, 42.603353),\n",
    "    'drc-mai-ndombe' : (-3.696119, 20.362077),\n",
    "    'malawi-salima': (-13.6, 34.32),\n",
    "    'brazil-para': (-2.064534, -56.578095),\n",
    "    'brazil-para-2': (-7.351687, -48.457507),\n",
    "    'pakistan-mardan': (34.355452, 71.945095),\n",
    "    'botswana-kweneng': (-24.360968, 25.176526),\n",
    "    'nicaragua-bonanza': (13.933745, -84.690842),\n",
    "    'ghana-cocoa': (7.398111, -1.269223),\n",
    "    'ghana-brong-ahafo': (7.70258, -0.70911),\n",
    "    'mexico-change-det': (21.212083, -88.993677),\n",
    "    'costa-rica-change-det': (8.47520, -82.94909),\n",
    "    'honduras-colon': (15.617889, -85.447611),\n",
    "    'mexico-campeche-change': (18.151747, -92.152278),\n",
    "    'guatemala-gain': (16.464444, -89.479170),\n",
    "    'guatemala-gain-2': (15.196480, -89.118290)\n",
    "}\n",
    "\n",
    "landscape = 'guatemala-gain-2'\n",
    "\n",
    "OUTPUT_FOLDER = '../tile_data/{}/'.format(landscape)\n",
    "coords = landscapes[landscape]\n",
    "coords = (coords[1], coords[0])\n",
    "print(OUTPUT_FOLDER, coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "landscape_df = pd.DataFrame({'landscape': [x for x in landscapes.keys()], \n",
    "                             'latitude': [x[0] for x in landscapes.values()],\n",
    "                             'longitude': [x[1] for x in landscapes.values()]\n",
    "})\n",
    "\n",
    "landscape_df.to_csv(\"../data/latlongs/landscapes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projection functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bbx_pyproj(coord: Tuple[float, float],\n",
    "                         step_x: int, step_y: int,\n",
    "                         expansion: int, multiplier: int = 1.) -> (Tuple[float, float], 'CRS'):\n",
    "    ''' Calculates the four corners of a bounding box\n",
    "        [bottom left, top right] as well as the UTM EPSG using Pyproj\n",
    "        \n",
    "        Note: The input for this function is (x, y), not (lat, long)\n",
    "        \n",
    "        Parameters:\n",
    "         coord (tuple): Initial (long, lat) coord\n",
    "         step_x (int): X tile number of a 6300x6300 meter tile\n",
    "         step_y (int): Y tile number of a 6300x6300 meter tile\n",
    "         expansion (int): Typically 10 meters - the size of the border for the predictions\n",
    "         multiplier (int): Currently deprecated\n",
    "         \n",
    "        Returns:\n",
    "         coords (tuple):\n",
    "         CRS (int):\n",
    "    '''\n",
    "    \n",
    "    inproj = Proj('epsg:4326')\n",
    "    outproj_code = calculate_epsg(coord)\n",
    "    outproj = Proj('epsg:' + str(outproj_code))\n",
    "    \n",
    "    \n",
    "    \n",
    "    coord_utm =  transform(inproj, outproj, coord[1], coord[0])\n",
    "    coord_utm_bottom_left = (coord_utm[0] + step_x*6300 - expansion,\n",
    "                             coord_utm[1] + step_y*6300 - expansion)\n",
    "    \n",
    "    coord_utm_top_right = (coord_utm[0] + (step_x+multiplier) * 6300 + expansion,\n",
    "                           coord_utm[1] + (step_y+multiplier) * 6300 + expansion)\n",
    "\n",
    "    zone = str(outproj_code)[3:]\n",
    "    direction = 'N' if coord[1] >= 0 else 'S'\n",
    "    utm_epsg = \"UTM_\" + zone + direction\n",
    "    return (coord_utm_bottom_left, coord_utm_top_right), CRS[utm_epsg]\n",
    "\n",
    "\n",
    "def pts_in_geojson(lats: List[float], longs: List[float], geojson: 'geojson') -> bool:  \n",
    "    \"\"\" Identifies whether candidate download tile is within an input geojson\n",
    "        \n",
    "        Parameters:\n",
    "         lats (list): list of latitudes\n",
    "         longs (list): list of longitudes\n",
    "         geojson (float): path to input geojson\n",
    "    \n",
    "        Returns:\n",
    "         bool \n",
    "    \"\"\"\n",
    "    polys = geopandas.read_file(geojson)['geometry']\n",
    "    polys = geopandas.GeoSeries(polys)\n",
    "    pnts = [Point(x, y) for x, y in zip(list(lats), list(longs))]\n",
    "    \n",
    "    def _contains(pt):\n",
    "        return polys.contains(pt)[0]\n",
    "\n",
    "    if any([_contains(pt) for pt in pnts]):\n",
    "        return True\n",
    "    else: return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data download functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_clouds(bbox: List[Tuple[float, float]],\n",
    "                        epsg: 'CRS', dates: dict = dates) -> (np.ndarray, np.ndarray, np.ndarray):\n",
    "\n",
    "    \"\"\" Downloads and calculates cloud cover and shadow\n",
    "        \n",
    "        Parameters:\n",
    "         bbox (list): output of calc_bbox\n",
    "         epsg (float): EPSG associated with bbox \n",
    "         dates (tuple): YY-MM-DD - YY-MM-DD bounds for downloading \n",
    "    \n",
    "        Returns:\n",
    "         cloud_img (np.array):\n",
    "         shadows (np.array): \n",
    "         clean_steps (np.array):\n",
    "    \"\"\"\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    cloud_request = WcsRequest(\n",
    "        layer='CLOUD_NEW',\n",
    "        bbox=box,\n",
    "        time=dates,\n",
    "        resx='160m', \n",
    "        resy='160m',\n",
    "        image_format = MimeType.TIFF_d8,\n",
    "        maxcc=0.7,\n",
    "        instance_id=API_KEY,\n",
    "        custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "        time_difference=datetime.timedelta(hours=72),\n",
    "    )\n",
    "\n",
    "\n",
    "    shadow_request = WcsRequest(\n",
    "        layer='SHADOW',\n",
    "        bbox=box,\n",
    "        time=dates,\n",
    "        resx='10m',\n",
    "        resy='10m',\n",
    "        image_format =  MimeType.TIFF_d16,\n",
    "        maxcc=0.7,\n",
    "        instance_id=API_KEY,\n",
    "        custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "        time_difference=datetime.timedelta(hours=72))\n",
    "\n",
    "    cloud_img = cloud_request.get_data()\n",
    "    cloud_img = np.array(cloud_img)\n",
    "    print(f\"The max clouds is {np.max(cloud_img)}\")\n",
    "    print(cloud_img.shape)\n",
    "    if np.max(cloud_img > 10):\n",
    "        cloud_img = cloud_img / 255\n",
    "        \n",
    "    assert np.max(cloud_img) <= 1.\n",
    "    c_probs_pus = ((40*40)/(512*512)) *(1/3)*cloud_img.shape[0]\n",
    "    print(f\"Cloud_probs used {c_probs_pus} processing units\")\n",
    "    \n",
    "    cloud_img = resize(cloud_img, (cloud_img.shape[0], IMSIZE, IMSIZE), order = 0)\n",
    "    n_cloud_px = np.array([len(np.argwhere(cloud_img[x, :, :].reshape((IMSIZE)*(IMSIZE)) > 0.33))\n",
    "                           for x in range(cloud_img.shape[0])])\n",
    "    cloud_steps = np.argwhere(n_cloud_px > IMSIZE**2 / 5)\n",
    "    clean_steps = [x for x in range(cloud_img.shape[0]) if x not in cloud_steps]\n",
    "    print(f\"Removing {len(cloud_steps)} from S2 download, saving {7.32 * len(cloud_steps)} PU\")\n",
    "\n",
    "    shadow_img = shadow_request.get_data(data_filter = clean_steps)\n",
    "    shadow_img = np.array(shadow_img)\n",
    "    print(shadow_img.shape)\n",
    "    shadow_img = resize(shadow_img, (shadow_img.shape[0], IMSIZE, IMSIZE, shadow_img.shape[-1]), order = 0)\n",
    "    print(shadow_img.shape)\n",
    "    shadow_pus = ((IMSIZE*IMSIZE)/(512*512)) * shadow_img.shape[0]\n",
    "    print(f\"Shadows used: {shadow_pus} processing units\")\n",
    "    print(f\"The max shadows is {np.max(shadow_img)}\")\n",
    "    if np.max(shadow_img > 10):\n",
    "        shadow_img = shadow_img / 65535\n",
    "    print(np.max(shadow_img))\n",
    " \n",
    "    cloud_img = np.delete(cloud_img, cloud_steps, 0)\n",
    "    shadows = mcm_shadow_mask(np.array(shadow_img), cloud_img)\n",
    "\n",
    "    print(f\"Cloud probs: {cloud_img.shape}\")\n",
    "    print(f\"Shadow shape {shadows.shape}\")\n",
    "    return cloud_img, shadows, clean_steps\n",
    "\n",
    "    \n",
    "    \n",
    "def download_dem(bbox: List[Tuple[float, float]], epsg: 'CRS') -> np.ndarray:\n",
    "    \"\"\" Downloads the DEM layer from Sentinel hub\n",
    "        \n",
    "        Parameters:\n",
    "         bbox (list): output of calc_bbox\n",
    "         epsg (float): EPSG associated with bbox \n",
    "    \n",
    "        Returns:\n",
    "         dem_image (arr):\n",
    "    \"\"\"\n",
    "\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    dem_s = (630)+4+8+8\n",
    "    dem_request = WmsRequest(data_source=DataSource.DEM,\n",
    "                         layer='DEM',\n",
    "                         bbox=box,\n",
    "                         width=dem_s,\n",
    "                         height=dem_s,\n",
    "                         instance_id=API_KEY,\n",
    "                         image_format=MimeType.TIFF_d32f,\n",
    "                         custom_url_params={CustomUrlParam.SHOWLOGO: False})\n",
    "    dem_image = dem_request.get_data()[0]\n",
    "    dem_image = calcSlope(dem_image.reshape((1, dem_s, dem_s)),\n",
    "                  np.full((dem_s, dem_s), 10), np.full((dem_s, dem_s), 10), zScale = 1, minSlope = 0.02)\n",
    "    dem_image = dem_image.reshape((dem_s,dem_s, 1))\n",
    "    dem_image = dem_image[1:dem_s-1, 1:dem_s-1, :]\n",
    "    print(f\"DEM used {((IMSIZE*IMSIZE)/(512*512))*2} processing units\")\n",
    "    return dem_image\n",
    " \n",
    "\n",
    "def download_layer(bbox: List[Tuple[float, float]],\n",
    "                   clean_steps: np.ndarray, epsg: 'CRS',\n",
    "                   dates: dict = dates, year: int = year) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\" Downloads the L2A sentinel layer with 10 and 20 meter bands\n",
    "        \n",
    "        Parameters:\n",
    "         bbox (list): output of calc_bbox\n",
    "         clean_steps (list): list of steps to filter download request\n",
    "         epsg (float): EPSG associated with bbox \n",
    "         time (tuple): YY-MM-DD - YY-MM-DD bounds for downloading \n",
    "    \n",
    "        Returns:\n",
    "         img (arr):\n",
    "         img_request (obj): \n",
    "    \"\"\"\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    image_request = WcsRequest(\n",
    "            layer='L2A20',\n",
    "            bbox=box,\n",
    "            time=dates,\n",
    "            image_format = MimeType.TIFF_d16,\n",
    "            maxcc=0.7,\n",
    "            resx='20m', resy='20m',\n",
    "            instance_id=API_KEY,\n",
    "            custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'NEAREST',\n",
    "                                constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "            time_difference=datetime.timedelta(hours=72),\n",
    "        )\n",
    "    print(\"Downloading L2A 20m layer\")\n",
    "    img_bands = image_request.get_data(data_filter = clean_steps)\n",
    "    img_20 = np.stack(img_bands)\n",
    "    print(f\"The max 20m is {np.max(img_20)}\")\n",
    "    if np.max(img_20) >= 10:\n",
    "        img_20 = img_20 / 65535\n",
    "    assert np.max(img_20) <= 2.\n",
    "    \n",
    "    s2_20_usage = (img_20.shape[1]*img_20.shape[2])/(512*512) * (6/3) * img_20.shape[0]\n",
    "    print(\"Original 20 meter bands size: {}, using {} PU\".format(img_20.shape, s2_20_usage))\n",
    "    img_20 = resize(img_20, (img_20.shape[0], IMSIZE, IMSIZE, img_20.shape[-1]), order = 0)\n",
    "\n",
    "    image_request = WcsRequest(\n",
    "            layer='L2A10',\n",
    "            bbox=box,\n",
    "            time=dates,\n",
    "            image_format = MimeType.TIFF_d16,\n",
    "            maxcc=0.7,\n",
    "            resx='10m', resy='10m',\n",
    "            instance_id=API_KEY,\n",
    "            custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'BICUBIC',\n",
    "                                constants.CustomUrlParam.UPSAMPLING: 'BICUBIC'},\n",
    "            time_difference=datetime.timedelta(hours=72),\n",
    "    )\n",
    "    \n",
    "    print(\"Downloading L2A 10m layer\")\n",
    "    img_bands = image_request.get_data(data_filter = clean_steps)\n",
    "    img_10 = np.stack(img_bands)\n",
    "    print(f\"The max 10m is {np.max(img_10)}\")\n",
    "    if np.max(img_10) >= 10:\n",
    "        img_10 = img_10 / 65535\n",
    "    assert np.max(img_10) <= 2.\n",
    "    \n",
    "    s2_10_usage = (img_10.shape[1]*img_10.shape[2])/(512*512) * (4/3) * img_10.shape[0]\n",
    "    print(\"Original 20 meter bands size: {}, using {} PU\".format(img_10.shape, s2_10_usage))\n",
    "    img_10 = resize(img_10, (img_10.shape[0], IMSIZE, IMSIZE, img_10.shape[-1]), order = 0)\n",
    "    img = np.concatenate([img_10, img_20], axis = -1)\n",
    "    print(f\"Sentinel 2 used {s2_20_usage + s2_10_usage} PU\")\n",
    "\n",
    "    image_dates = []\n",
    "    for date in image_request.get_dates():\n",
    "        if date.year == year - 1:\n",
    "            image_dates.append(-365 + starting_days[(date.month-1)] + date.day)\n",
    "        if date.year == year:\n",
    "            image_dates.append(starting_days[(date.month-1)] + date.day)\n",
    "        if date.year == year + 1:\n",
    "            image_dates.append(365 + starting_days[(date.month-1)]+date.day)\n",
    "    image_dates = [val for idx, val in enumerate(image_dates) if idx in clean_steps]\n",
    "    image_dates = np.array(image_dates)\n",
    "    return img, image_dates\n",
    "\n",
    "        \n",
    "def download_sentinel_1(bbox: List[Tuple[float, float]],\n",
    "                        epsg: 'CRS', imsize: int = IMSIZE, \n",
    "                        dates: dict = dates_sentinel_1, layer: str = \"SENT\",\n",
    "                        year: int = year) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\" Downloads the GRD Sentinel 1 VV-VH layer from Sentinel Hub\n",
    "        \n",
    "        Parameters:\n",
    "         bbox (list): output of calc_bbox\n",
    "         epsg (float): EPSG associated with bbox \n",
    "         imsize (int):\n",
    "         dates (tuple): YY-MM-DD - YY-MM-DD bounds for downloading \n",
    "         layer (str):\n",
    "         year (int): \n",
    "    \n",
    "        Returns:\n",
    "         s1 (arr):\n",
    "         image_dates (arr): \n",
    "    \"\"\"\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    image_request = WcsRequest(\n",
    "            layer=layer,\n",
    "            bbox=box,\n",
    "            time=dates,\n",
    "            image_format = MimeType.TIFF_d16,\n",
    "            data_source=DataSource.SENTINEL1_IW,\n",
    "            maxcc=1.0,\n",
    "            resx='10m', resy='5m',\n",
    "            instance_id=API_KEY,\n",
    "            custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'NEAREST',\n",
    "                                constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "            time_difference=datetime.timedelta(hours=96),\n",
    "        )\n",
    "    data_filter = None\n",
    "    if len(image_request.download_list) > 50:\n",
    "        data_filter = [x for x in range(len(image_request.download_list)) if x % 2 == 0]\n",
    "    img_bands = image_request.get_data(data_filter = data_filter)\n",
    "    s1 = np.stack(img_bands)\n",
    "    if np.max(s1) >= 1000:\n",
    "            s1 = s1 / 65535.\n",
    "    \n",
    "    print(f\"The max s1 is {np.max(s1)}\")\n",
    "    print(f\"Sentinel 1 used {(2/3)*s1.shape[0] * (s1.shape[1]*s1.shape[2])/(512*512)} PU for \\\n",
    "          {s1.shape[0]} out of {len(image_request.download_list)} images\")\n",
    "    s1 = resize(s1, (s1.shape[0], imsize*2, imsize*2, s1.shape[-1]), order = 0)\n",
    "    s1 = np.reshape(s1, (s1.shape[0], s1.shape[1]//2, 2, s1.shape[2] // 2, 2, s1.shape[-1]))\n",
    "    s1 = np.mean(s1, (2, 4))\n",
    "\n",
    "    image_dates = []\n",
    "    for date in image_request.get_dates():\n",
    "        if date.year == year - 1:\n",
    "            image_dates.append(-365 + starting_days[(date.month-1)] + date.day)\n",
    "        if date.year == year:\n",
    "            image_dates.append(starting_days[(date.month-1)] + date.day)\n",
    "        if date.year == year + 1:\n",
    "            image_dates.append(365 + starting_days[(date.month-1)]+date.day)\n",
    "    image_dates = np.array(image_dates)\n",
    "    s1c = np.copy(s1)\n",
    "    s1c[np.where(s1c < 1.)] = 0\n",
    "    n_pix_oob = np.sum(s1c, axis = (1, 2, 3))\n",
    "    to_remove = np.argwhere(n_pix_oob > (imsize*2*imsize*2)/50)\n",
    "    s1 = np.delete(s1, to_remove, 0)\n",
    "    image_dates = np.delete(image_dates, to_remove)\n",
    "    return s1, image_dates\n",
    "\n",
    "\n",
    "def identify_s1_layer(coords: Tuple[float, float]) -> str:\n",
    "    \"\"\" Identifies whether to download ascending or descending \n",
    "        sentinel 1 orbit based upon predetermined geographic coverage\n",
    "        \n",
    "        Reference: https://sentinel.esa.int/web/sentinel/missions/\n",
    "                   sentinel-1/satellite-description/geographical-coverage\n",
    "        \n",
    "        Parameters:\n",
    "         coords (tuple): \n",
    "    \n",
    "        Returns:\n",
    "         layer (str): either of SENT, SENT_DESC \n",
    "    \"\"\"\n",
    "    results = rg.search(coords)\n",
    "    country = results[-1]['cc']\n",
    "    continent_name = pc.country_alpha2_to_continent_code(country)\n",
    "    if continent_name in ['AF', 'OC']:\n",
    "        layer = \"SENT\"\n",
    "    if continent_name in ['SA']:\n",
    "        if coords[0] > -7.11:\n",
    "            layer = \"SENT\"\n",
    "        else:\n",
    "            layer = \"SENT_DESC\"\n",
    "    if continent_name in ['AS']:\n",
    "        if coords[0] > 23.3:\n",
    "            layer = \"SENT\"\n",
    "        else:\n",
    "            layer = \"SENT_DESC\"\n",
    "    if continent_name in ['NA']:\n",
    "        layer = \"SENT_DESC\"\n",
    "    print(continent_name)\n",
    "    print(layer)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud and shadow removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_missed_clouds(img: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" Removes clouds that may have been missed by s2cloudless\n",
    "        by looking at a temporal change outside of IQR\n",
    "        \n",
    "        Parameters:\n",
    "         img (arr): \n",
    "    \n",
    "        Returns:\n",
    "         to_remove (arr): \n",
    "    \"\"\"\n",
    "    \n",
    "    iqr = np.percentile(img[:, :, :, 3].flatten(), 75) - np.percentile(img[:, :, :, 3].flatten(), 25)\n",
    "    thresh_t = np.percentile(img[:, :, :, 3].flatten(), 75) + iqr*2\n",
    "    thresh_b = np.percentile(img[:, :, :, 3].flatten(), 25) - iqr*2\n",
    "    diffs_fw = np.diff(img, 1, axis = 0)\n",
    "    diffs_fw = np.mean(diffs_fw, axis = (1, 2, 3))\n",
    "    diffs_fw = np.array([0] + list(diffs_fw))\n",
    "    diffs_bw = np.diff(np.flip(img, 0), 1, axis = 0)\n",
    "    diffs_bw = np.flip(np.mean(diffs_bw, axis = (1, 2, 3)))\n",
    "    diffs_bw = np.array(list(diffs_bw) + [0])\n",
    "    diffs = abs(diffs_fw - diffs_bw) * 100\n",
    "    outlier_percs = []\n",
    "    for step in range(img.shape[0]):\n",
    "        bottom = len(np.argwhere(img[step, :, :, 3].flatten() > thresh_t))\n",
    "        top = len(np.argwhere(img[step, :, :, 3].flatten() < thresh_b))\n",
    "        p = 100* ((bottom + top) / (IMSIZE*IMSIZE))\n",
    "        outlier_percs.append(p)\n",
    "    to_remove = np.argwhere(np.array(outlier_percs) > 20)\n",
    "    return to_remove\n",
    "\n",
    "def calculate_bad_steps(sentinel2: np.ndarray, clouds: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" Calculates the timesteps to remove based upon cloud cover and missing data\n",
    "        \n",
    "        Parameters:\n",
    "         sentinel2 (arr): \n",
    "         clouds (arr):\n",
    "    \n",
    "        Returns:\n",
    "         to_remove (arr): \n",
    "    \"\"\"\n",
    "    n_cloud_px = np.array([len(np.argwhere(clouds[x, :, :].reshape((IMSIZE)*(IMSIZE)) > 0.30)) for x in range(clouds.shape[0])])\n",
    "    cloud_steps = np.argwhere(n_cloud_px > IMSIZE**2 / 7)\n",
    "    print(f'The percent cloud cover is {n_cloud_px/(IMSIZE**2)}')\n",
    "    missing_images = [np.argwhere(sentinel2[x, :, : :10].flatten() == 0.0) for x in range(sentinel2.shape[0])]\n",
    "    missing_images = np.array([len(x) for x in missing_images])\n",
    "    print(f'The number of missing 0 is {missing_images/(IMSIZE**2)}')\n",
    "    missing_images_p = [np.argwhere(sentinel2[x, :, : :10].flatten() >= 1) for x in range(sentinel2.shape[0])]\n",
    "    missing_images_p = np.array([len(x) for x in missing_images_p])\n",
    "    print(f'The number of missing 1 is {missing_images_p/(IMSIZE**2)}')\n",
    "    missing_images += missing_images_p\n",
    "    missing_images = np.argwhere(missing_images >= (IMSIZE**2) / 20)\n",
    "    to_remove = np.unique(np.concatenate([cloud_steps.flatten(), missing_images.flatten()]))\n",
    "    return to_remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Superresolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/john.brandt/anaconda3/envs/remote_sensing/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Symbolic Model Created.\n"
     ]
    }
   ],
   "source": [
    "MDL_PATH = \"../src/dsen2/models/\"\n",
    "\n",
    "input_shape = ((4, None, None), (6, None, None))\n",
    "model = s2model(input_shape, num_layers=6, feature_size=128)\n",
    "predict_file = MDL_PATH+'s2_032_lr_1e-04.hdf5'\n",
    "print('Symbolic Model Created.')\n",
    "\n",
    "model.load_weights(predict_file)\n",
    "\n",
    "def DSen2(d10: np.ndarray, d20: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Super resolves 20 meter bans using the DSen2 convolutional\n",
    "       neural network, as specified in Lanaras et al. 2018\n",
    "       https://github.com/lanha/DSen2\n",
    "\n",
    "        Parameters:\n",
    "         d10 (arr): (4, X, Y) shape array with 10 meter resolution\n",
    "         d20 (arr): (6, X, Y) shape array with 20 meter resolution\n",
    "\n",
    "        Returns:\n",
    "         prediction (arr): (6, X, Y) shape array with 10 meter superresolved\n",
    "                          output of DSen2 on d20 array\n",
    "    \"\"\"\n",
    "    test = [d10, d20]\n",
    "    input_shape = ((4, None, None), (6, None, None))\n",
    "    prediction = _predict(test, input_shape, deep=False)\n",
    "    return prediction\n",
    "\n",
    "def _predict(test: np.ndarray, input_shape: Tuple, model: 'model' = model,\n",
    "             deep: bool = False, run_60: bool = False) -> np.ndarray:\n",
    "    \n",
    "    prediction = model.predict(test, verbose=0, batch_size = 8)\n",
    "    return prediction\n",
    "\n",
    "def superresolve(sentinel2: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Worker function to deal with types and shapes\n",
    "       to superresolve a 10-band input array\n",
    "\n",
    "        Parameters:\n",
    "         sentinel2 (arr): (:, X, Y, 10) shape array with 10 meter resolution\n",
    "                          bands in indexes 0-4, and 20 meter in 4- 10\n",
    "\n",
    "        Returns:\n",
    "         superresolved (arr): (:, X, Y, 10) shape array with 10 meter \n",
    "                              superresolved output of DSen2\n",
    "    \"\"\"\n",
    "    d10 = sentinel2[:, :, :, 0:4]\n",
    "    d20 = sentinel2[:, :, :, 4:10]\n",
    "\n",
    "    d10 = np.swapaxes(d10, 1, -1)\n",
    "    d10 = np.swapaxes(d10, 2, 3)\n",
    "    d20 = np.swapaxes(d20, 1, -1)\n",
    "    d20 = np.swapaxes(d20, 2, 3)\n",
    "    superresolved = DSen2(d10, d20)\n",
    "    superresolved = np.swapaxes(superresolved, 1, -1)\n",
    "    superresolved = np.swapaxes(superresolved, 1, 2)\n",
    "    sentinel2[:, :, :, 4:10] = superresolved\n",
    "    return sentinel2 # returns band IDXs 3, 4, 5, 7, 8, 9\n",
    "\n",
    "def superresolve_tile(arr):\n",
    "    print(f\"The input array to superresolve is {arr.shape}\")\n",
    "    superresolved = np.copy(arr)\n",
    "    tiles = tile_window(646, 646, 56, 56)\n",
    "    for i in tnrange(len(tiles)):\n",
    "        subtile = tiles[i]\n",
    "        to_resolve = arr[:, subtile[0]:subtile[0]+56, subtile[1]:subtile[1]+56, :]\n",
    "        to_resolve = np.pad(to_resolve, ((0, 0), (4, 4), (4, 4), (0, 0)), 'reflect')\n",
    "        resolved = superresolve(to_resolve)\n",
    "        resolved = resolved[:, 4:-4, 4:-4, :]\n",
    "        superresolved[:, subtile[0]:subtile[0]+56, subtile[1]:subtile[1]+56] = resolved\n",
    "    return superresolved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiling and folder management functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_output_and_temp_folders(idx, output_folder = OUTPUT_FOLDER):\n",
    "    \n",
    "    def _find_and_make_dirs(dirs):\n",
    "        if not os.path.exists(os.path.realpath(dirs)):\n",
    "            os.makedirs(os.path.realpath(dirs))\n",
    "            \n",
    "    _find_and_make_dirs(output_folder + \"raw/\")\n",
    "    _find_and_make_dirs(output_folder + \"raw/clouds/\")\n",
    "    _find_and_make_dirs(output_folder + \"raw/s1/\")\n",
    "    _find_and_make_dirs(output_folder + \"raw/s2/\")\n",
    "    _find_and_make_dirs(output_folder + \"raw/misc/\")\n",
    "    _find_and_make_dirs(output_folder + \"processed/\")\n",
    "    \n",
    "\n",
    "def check_contains(coord, step_x, step_y, folder = OUTPUT_FOLDER):\n",
    "    contains = False\n",
    "    bbx, epsg = calculate_bbx_pyproj(coord, step_x, step_y, expansion = 80)\n",
    "    inproj = Proj('epsg:' + str(str(epsg)[5:]))\n",
    "    outproj = Proj('epsg:4326')\n",
    "    bottomleft = transform(inproj, outproj, bbx[0][0], bbx[0][1])\n",
    "    topright = transform(inproj, outproj, bbx[1][0], bbx[1][1])\n",
    "    \n",
    "    if os.path.exists(folder):\n",
    "            if any([x.endswith(\".geojson\") for x in os.listdir(folder)]):\n",
    "                geojson_path = folder + [x for x in os.listdir(folder) if x.endswith(\".geojson\")][0]\n",
    "    \n",
    "                bool_contains = pts_in_geojson(lats = [bottomleft[1], topright[1]], \n",
    "                                                       longs = [bottomleft[0], topright[0]],\n",
    "                                                       geojson = geojson_path)\n",
    "                contains = bool_contains\n",
    "    return contains\n",
    "\n",
    "def download_large_tile(coord, step_x, step_y, folder = OUTPUT_FOLDER, year = year, s1_layer = \"SENT\"):\n",
    "    \n",
    "    bbx, epsg = calculate_bbx_pyproj(coord, step_x, step_y, expansion = 80)\n",
    "    dem_bbx, _ = calculate_bbx_pyproj(coord, step_x, step_y, expansion = 90)\n",
    "    idx = str(step_y) + \"_\" + str(step_x)\n",
    "    idx = str(idx)\n",
    "    make_output_and_temp_folders(idx)\n",
    "\n",
    "    print(\"Calculating cloud cover\")\n",
    "    if not os.path.exists(folder + \"output/\" + str(((step_y+1)*5)-5) + \"/\" + str(((step_x+1)*5)-5) + \".npy\"):\n",
    "        if not os.path.exists(folder + \"processed/\" + str(((step_y+1)*5)-5) + \"/\" + str(((step_x+1)*5)-5) + \".hkl\"):\n",
    "            if not os.path.exists(folder + \"raw/clouds/clouds_{}.hkl\".format(idx)):\n",
    "                cloud_probs, shadows, clean_steps = identify_clouds(bbx, epsg = epsg)\n",
    "                hkl.dump(cloud_probs, folder + \"raw/clouds/clouds_{}.hkl\".format(idx), mode='w', compression='gzip')\n",
    "                hkl.dump(shadows, folder + \"raw/clouds/shadows_{}.hkl\".format(idx), mode='w', compression='gzip')\n",
    "                hkl.dump(clean_steps, folder + \"raw/clouds/clean_steps_{}.hkl\".format(idx), mode='w', compression='gzip')\n",
    "            \n",
    "            if not os.path.exists(folder + \"raw/s1/{}.hkl\".format(idx)):\n",
    "                print(\"Downloading S1\")\n",
    "                s1_layer = identify_s1_layer((coord[1], coord[0]))\n",
    "                s1, s1_dates = download_sentinel_1(bbx, layer = s1_layer, epsg = epsg)\n",
    "                if s1.shape[0] == 0:\n",
    "                    s1_layer = \"SENT_DESC\" if s1_layer == \"SENT\" else \"SENT\"\n",
    "                    print(f'Switching to {s1_layer}')\n",
    "                    s1, s1_dates = download_sentinel_1(bbx, layer = s1_layer, epsg = epsg)\n",
    "                s1 = process_sentinel_1_tile(s1, s1_dates)\n",
    "                hkl.dump(s1, folder + \"raw/s1/{}.hkl\".format(idx), mode='w', compression='gzip')\n",
    "                hkl.dump(s1_dates, folder + \"raw/misc/s1_dates_{}.hkl\".format(idx), mode='w', compression='gzip')\n",
    "\n",
    "            if not os.path.exists(folder + \"raw/s2/{}.hkl\".format(idx)):\n",
    "                print(\"Downloading S2\")\n",
    "                if 'clean_steps' not in globals() or locals():\n",
    "                    clean_steps = hkl.load(folder + \"raw/clouds/clean_steps_{}.hkl\".format(idx))\n",
    "                s2, s2_dates = download_layer(bbx, clean_steps = clean_steps, epsg = epsg)\n",
    "                hkl.dump(s2, folder + \"raw/s2/{}.hkl\".format(idx), mode='w', compression='gzip')\n",
    "                hkl.dump(s2_dates, folder + \"raw/misc/s2_dates_{}.hkl\".format(idx), mode='w', compression='gzip')\n",
    "\n",
    "            if not os.path.exists(folder + \"raw/misc/dem_{}.hkl\".format(idx)):\n",
    "                print(\"Downloading DEM\")\n",
    "                dem = download_dem(dem_bbx, epsg = epsg) # get the DEM BBOX\n",
    "                hkl.dump(dem, folder + \"raw/misc/dem_{}.hkl\".format(idx), mode='w', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentinel_1_tile(sentinel1, dates):\n",
    "    s1, _ = calculate_and_save_best_images(sentinel1, dates)\n",
    "    biweekly_dates = np.array([day for day in range(0, 360, 5)])\n",
    "    to_remove = np.argwhere(biweekly_dates % 15 != 0)\n",
    "    s1 = np.delete(s1, to_remove, 0)\n",
    "    return s1\n",
    "\n",
    "def convert_to_int16(array):\n",
    "    return np.trunc(array * 65535).astype(int)\n",
    "\n",
    "def make_folder_names(step_x, step_y):\n",
    "    x_vals = []\n",
    "    y_vals = []\n",
    "    for i in range(25):\n",
    "        y_val = (24 - i) // 5\n",
    "        x_val = 5 - ((25 - i) % 5)\n",
    "        x_val = 0 if x_val == 5 else x_val\n",
    "        x_vals.append(x_val)\n",
    "        y_vals.append(y_val)\n",
    "    y_vals = [i + (5*step_y) for i in y_vals]\n",
    "    x_vals = [i + (5*step_x) for i in x_vals]\n",
    "    return x_vals, y_vals\n",
    "\n",
    "\n",
    "def process_large_tile(coord, step_x, step_y, folder = OUTPUT_FOLDER):\n",
    "    idx = str(step_y) + \"_\" + str(step_x)\n",
    "    x_vals, y_vals = make_folder_names(step_x, step_y)\n",
    "\n",
    "    processed = True\n",
    "    for x, y in zip(x_vals, y_vals):\n",
    "        folder_path = f\"{str(y)}/{str(x)}\"\n",
    "        processed_exists = os.path.exists(folder + \"processed/\" + folder_path + \".hkl\")\n",
    "        output_exists = os.path.exists(folder + \"output/\" + folder_path + \".npy\")\n",
    "        if not (processed_exists or output_exists):\n",
    "            processed = False\n",
    "    if not processed:\n",
    "        clouds = hkl.load(folder + \"raw/clouds/clouds_{}.hkl\".format(idx))\n",
    "        sentinel1 = hkl.load(folder + \"raw/s1/{}.hkl\".format(idx))\n",
    "        radar_dates = hkl.load(folder + \"raw/misc/s1_dates_{}.hkl\".format(idx))\n",
    "        sentinel2 = hkl.load(folder + \"raw/s2/{}.hkl\".format(idx))\n",
    "        dem = hkl.load(folder + \"raw/misc/dem_{}.hkl\".format(idx))\n",
    "        image_dates = hkl.load(folder + \"raw/misc/s2_dates_{}.hkl\".format(idx))\n",
    "        if os.path.exists(folder + \"raw/clouds/shadows_{}.hkl\".format(idx)):\n",
    "            shadows = hkl.load(folder + \"raw/clouds/shadows_{}.hkl\".format(idx))\n",
    "        else:\n",
    "            print(\"No shadows file, so calculating shadows with L2A\")\n",
    "            shadows = mcm_shadow_mask(sentinel2, clouds)\n",
    "        print(\"The files have been loaded\")\n",
    "\n",
    "        to_remove = calculate_bad_steps(sentinel2, clouds)\n",
    "        sentinel2 = np.delete(sentinel2, to_remove, axis = 0)\n",
    "        clouds = np.delete(clouds, to_remove, axis = 0)\n",
    "        shadows = np.delete(shadows, to_remove, axis = 0)\n",
    "        image_dates = np.delete(image_dates, to_remove)\n",
    "        print(f\"{len(to_remove)} Cloudy and missing images removed, radar processed\")\n",
    "        to_remove = remove_missed_clouds(sentinel2)\n",
    "        print(f\"{len(to_remove)} missed cloudy images should have been removed\")\n",
    "        x = remove_cloud_and_shadows(sentinel2, clouds, shadows, image_dates)\n",
    "        print(\"Clouds and shadows interpolated\")       \n",
    "        \n",
    "        index = 0\n",
    "        print(\"Super resolving tile\")\n",
    "        x = superresolve_tile(x)\n",
    "        print(f\"The superresolved shape is: {x.shape}\")\n",
    "        \n",
    "        tiles = tile_window(IMSIZE, IMSIZE, window_size = 142)\n",
    "        for t in tiles:\n",
    "            print(index)\n",
    "            index += 1\n",
    "            start_x = t[0]\n",
    "            start_y = t[1]\n",
    "            end_x = start_x + t[2]\n",
    "            end_y = start_y + t[3]\n",
    "            print(index)\n",
    "            output_file = f\"{folder}processed/{y_vals[index]}/{x_vals[index]}.hkl\"\n",
    "            if not os.path.exists(output_file):\n",
    "            #if not os.path.exists(folder + \"processed/{}/{}.hkl\".format(str(y_vals[index]), str(x_vals[index]))):\n",
    "                subtile = x[:, start_x:end_x, start_y:end_y, :]\n",
    "                dem_i = np.tile(dem[np.newaxis, start_x:end_x, start_y:end_y, :], (x.shape[0], 1, 1, 1))\n",
    "                subtile = np.concatenate([subtile, dem_i / 90], axis = -1)\n",
    "                subtile = evi(subtile, verbose = True)\n",
    "                subtile = bi(subtile, verbose = True)\n",
    "                subtile = msavi2(subtile, verbose = True)\n",
    "                subtile = si(subtile, verbose = True)\n",
    "                t3 = timer()\n",
    "\n",
    "                subtile, _ = calculate_and_save_best_images(subtile, image_dates)\n",
    "                subtile = interpolate_array(subtile, dim = 142)\n",
    "                t5 = timer()\n",
    "                print(\"Interpolate: {}\".format(t5 - t3))\n",
    "                subtile = np.concatenate([subtile, sentinel1[:, start_x:end_x,\n",
    "                                                            start_y:end_y, :]], axis = -1)\n",
    "\n",
    "                out_y_folder = folder + \"processed/{}/\".format(str(y_vals[index]))\n",
    "                if not os.path.exists(os.path.realpath(out_y_folder)):\n",
    "                    os.makedirs(os.path.realpath(out_y_folder))\n",
    "                subtile = convert_to_int16(subtile)\n",
    "                assert subtile.shape[1] == 142, f\"subtile shape is {subtile.shape}\"\n",
    "                hkl.dump(subtile, output_file,\n",
    "                         #folder + \"processed/{}/{}.hkl\".format(str(y_vals[index]), str(x_vals[index])),\n",
    "                         mode='w', compression='gzip')\n",
    "            \n",
    "def clean_up_folders():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-89.11829 15.19648\n",
      "-89.11829 15.19648\n",
      "15850\n"
     ]
    }
   ],
   "source": [
    "expansion = -10\n",
    "multiplier = 1\n",
    "step_x = 1\n",
    "coords_init = offset_x(coords, 0)\n",
    "coords_init[1] += 0\n",
    "\n",
    "coord1 = offset_x(coords, 6300*(step_x + multiplier) + expansion)\n",
    "coord1[1] += 6300*(step_x + multiplier) + expansion\n",
    "\n",
    "\n",
    "calculate_area([coords_init, coord1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "X: 0 Y:0\n",
      "Downloaded 1\n",
      "-89.11829 15.19648\n",
      "-89.11829 15.19648\n",
      "Calculating cloud cover\n",
      "\n",
      "\n",
      "True\n",
      "X: 0 Y:1\n",
      "Downloaded 2\n",
      "-89.11829 15.19648\n",
      "-89.11829 15.19648\n",
      "Calculating cloud cover\n",
      "The max clouds is 255\n",
      "(68, 40, 40)\n",
      "Cloud_probs used 0.13834635416666666 processing units\n",
      "Removing 43 from S2 download, saving 314.76 PU\n",
      "(25, 646, 646, 3)\n",
      "(25, 646, 646, 3)\n",
      "Shadows used: 39.79835510253906 processing units\n",
      "The max shadows is 1.0\n",
      "1.0\n",
      "648\n",
      "646\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486b9a1fd39044eeb9e82e8163202e9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd8fc7e7492949358235acaacf89231c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cloud probs: (25, 646, 646)\n",
      "Shadow shape (25, 646, 646)\n",
      "Downloading S1\n",
      "Loading formatted geocoded file...\n",
      "NA\n",
      "SENT_DESC\n",
      "The max s1 is 1.0\n",
      "Sentinel 1 used 63.6773681640625 PU for           30 out of 60 images\n",
      "Maximum time distance: 0\n",
      "Downloading S2\n",
      "Downloading L2A 20m layer\n",
      "The max 20m is 65535\n",
      "Original 20 meter bands size: (25, 323, 323, 6), using 19.89917755126953 PU\n",
      "Downloading L2A 10m layer\n",
      "The max 10m is 65535\n",
      "Original 20 meter bands size: (25, 646, 646, 4), using 53.06447347005208 PU\n",
      "Sentinel 2 used 72.96365102132161 PU\n",
      "Downloading DEM\n",
      "DEM used 3.183868408203125 processing units\n",
      "The files have been loaded\n",
      "The percent cloud cover is [0.10394521 0.         0.12979133 0.06161278 0.17169723 0.00184033\n",
      " 0.12488138 0.00184033 0.11395681 0.08780397 0.02035867 0.05356373\n",
      " 0.         0.00187867 0.19738759 0.01303569 0.00122689 0.00306722\n",
      " 0.15690268 0.19601932 0.00184033 0.00061344 0.0558234  0.11840907\n",
      " 0.00617278]\n",
      "The number of missing 0 is [0.         0.         0.         0.         0.         0.18566266\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.18566266 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.        ]\n",
      "The number of missing 1 is [1.82835070e-03 0.00000000e+00 0.00000000e+00 4.79253132e-06\n",
      " 2.40824699e-03 2.78493995e-01 2.79644202e-03 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 2.39626566e-06 0.00000000e+00\n",
      " 0.00000000e+00 1.43775940e-05 2.22852706e-04 2.78493995e-01\n",
      " 0.00000000e+00 1.43775940e-05 3.97780099e-04 5.75103758e-05\n",
      " 0.00000000e+00 0.00000000e+00 2.89948145e-04 2.39626566e-06\n",
      " 0.00000000e+00]\n",
      "6 Cloudy and missing images removed, radar processed\n",
      "0 missed cloudy images should have been removed\n"
     ]
    }
   ],
   "source": [
    "downloaded = 0\n",
    "\n",
    "if not os.path.exists(os.path.realpath(OUTPUT_FOLDER)):\n",
    "            os.makedirs(os.path.realpath(OUTPUT_FOLDER))\n",
    "        \n",
    "for x_tile in range(0, 1):\n",
    "    for y_tile in range(0, 2):\n",
    "        contains = True\n",
    "        #contains = check_contains(coords, x_tile, y_tile, OUTPUT_FOLDER)\n",
    "        print(contains)\n",
    "        if contains:\n",
    "            print(\"X: {} Y:{}\".format(x_tile, y_tile))\n",
    "            downloaded += 1\n",
    "            print(f\"Downloaded {downloaded}\")\n",
    "            download_large_tile(coord = coords, step_x = x_tile, step_y = y_tile)\n",
    "            process_large_tile(coords, x_tile, y_tile)\n",
    "            print(\"\\n\")\n",
    "            #clean_up_folders(x_tile, y_tile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_sensing",
   "language": "python",
   "name": "remote_sensing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
