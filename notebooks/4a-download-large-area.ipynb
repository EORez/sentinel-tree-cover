{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from osgeo import ogr, osr\n",
    "from sentinelhub import WmsRequest, WcsRequest, MimeType, CRS, BBox, constants\n",
    "import logging\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import os\n",
    "import yaml\n",
    "from sentinelhub import DataSource\n",
    "import scipy.sparse as sparse\n",
    "import scipy\n",
    "from scipy.sparse.linalg import splu\n",
    "from skimage.transform import resize\n",
    "from sentinelhub import CustomUrlParam\n",
    "from time import time as timer\n",
    "from time import sleep as sleep\n",
    "import multiprocessing\n",
    "import math\n",
    "import reverse_geocoder as rg\n",
    "import pycountry\n",
    "import pycountry_convert as pc\n",
    "import hickle as hkl\n",
    "from shapely.geometry import Point, Polygon\n",
    "import geopandas\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "import math\n",
    "import boto3\n",
    "from pyproj import Proj, transform\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../config.yaml\", 'r') as stream:\n",
    "    key = (yaml.safe_load(stream))\n",
    "    API_KEY = key['key']\n",
    "    AWSKEY = key['awskey']\n",
    "    AWSSECRET = key['awssecret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../src/utils/slope.py\n",
    "%run ../src/utils/utils.py\n",
    "%run ../src/utils/whittaker_smoother.py\n",
    "%run ../src/utils/download_utils.py\n",
    "%run ../src/dsen2/utils/DSen2Net.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ('2018-12-15', '2020-01-15')\n",
    "SIZE = 9*5\n",
    "IMSIZE = (SIZE * 14)+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../tile_data/senegal-tucker-a/ (-15.459789, 15.350595)\n"
     ]
    }
   ],
   "source": [
    "landscapes = {\n",
    "    'ethiopia-tigray': (13.540810, 38.177220),\n",
    "    'kenya-makueni-2': (-1.817109, 37.44563),\n",
    "    'ghana': (9.259359, -0.83375),\n",
    "    'niger-koure': (13.18158, 2.478),\n",
    "    'cameroon-farnorth': (10.596, 14.2722),\n",
    "    'mexico-campeche': (18.232495, -92.1234215),\n",
    "    'malawi-rumphi': (-11.044, 33.818),\n",
    "    'ghana-sisala-east': (10.385, -1.765),\n",
    "    'ghana-west-mamprusi': (10.390084, -0.846330),\n",
    "    'ghana-kwahu': (6.518909, -0.826008),\n",
    "    'senegal-16b': (15.82585, -15.34166),\n",
    "    'india-kochi': (9.909, 76.254),\n",
    "    'india-sidhi': (24.0705, 81.607),\n",
    "    'brazil-esperito-santo': (-20.147, -40.837),\n",
    "    'brazil-paraiba': (-22.559943, -44.186629),\n",
    "    'brazil-goias': (-14.905595, -48.907399),\n",
    "    'colombia-talima': (4.179529, -74.889171),\n",
    "    'drc-kafubu': (-11.749636, 27.586622),\n",
    "    'thailand-khon-kaen': (15.709725, 102.546518),\n",
    "    'indonesia-west-java': (-6.721101, 108.280949),\n",
    "    'madagascar': (-18.960152, 47.469587),\n",
    "    'tanzania': (-6.272258, 36.679824),\n",
    "    'chile': (-36.431237, -71.872030),\n",
    "    'indonesia-jakarta': (-6.352580, 106.677072),\n",
    "    'caf-baboua': (5.765917, 14.791618),   \n",
    "    'honduras': (14.096664, -88.720304),\n",
    "    'nicaragua': (12.398014, -86.963042),\n",
    "    'china': (26.673679, 107.464231),\n",
    "    'australia-west': (-32.666762, 117.411197),\n",
    "    'mexico-sonora': (29.244288, -111.243230),\n",
    "    'south-africa': (-30.981698, 28.727301),\n",
    "    'maldonado-uraguay': (-34.629250, -55.004331),\n",
    "    'dominican-rep-la-salvia': (18.872589, -70.462961),\n",
    "    'guatemala-coban': (15.3, -90.8),\n",
    "    'senegal-tucker-a': (15.350595, -15.459789),\n",
    "    'elsalvador-imposible': (13.727334, -90.015579)\n",
    "}\n",
    "\n",
    "landscape = 'senegal-tucker-a'\n",
    "\n",
    "#coords = (7.702058, -0.709011) # brong ahafo, bono east\n",
    "#coords = (7.398111, -1.269223) # cocoa\n",
    "#coords = (16.032170, -90.144511) # Guatemala\n",
    "#coords = (13.757749, -90.004949) # elsalvador imposible\n",
    "#coords = (13.933745, -84.690842) # Bonanza, Nicaragua\n",
    "\n",
    "OUTPUT_FOLDER = '../tile_data/{}/'.format(landscape)\n",
    "coords = landscapes[landscape]\n",
    "coords = (coords[1], coords[0])\n",
    "print(OUTPUT_FOLDER, coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "landscape_df = pd.DataFrame({'landscape': [x for x in landscapes.keys()], \n",
    "                             'latitude': [x[0] for x in landscapes.values()],\n",
    "                             'longitude': [x[1] for x in landscapes.values()]\n",
    "})\n",
    "\n",
    "landscape_df.to_csv(\"../data/latlongs/landscapes.csv\", index=False)\n",
    "print(len(landscape_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions (to be moved to a utils file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRID_SIZE_X = 1\n",
    "GRID_SIZE_Y = 1\n",
    "\n",
    "IMAGE_X = 14*GRID_SIZE_X\n",
    "IMAGE_Y = 14*GRID_SIZE_Y\n",
    "\n",
    "TEST_X = 5\n",
    "TEST_Y = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These arrays are for smoothly overlapping the cloud and shadow interpolation\n",
    "c_arr = np.array([[1, 1, 1, 1, 1,],\n",
    "                  [1, 2, 2, 2, 1,],\n",
    "                  [1, 2, 3, 2, 1,],\n",
    "                  [1, 2, 2, 2, 1,],\n",
    "                  [1, 1, 1, 1, 1,],])\n",
    "                  \n",
    "c_arr = c_arr / 3\n",
    "o_arr = 1 - c_arr\n",
    "c_arr = np.tile(c_arr[:, :, np.newaxis], (1, 1, 10))\n",
    "o_arr = np.tile(o_arr[:, :, np.newaxis], (1, 1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bbx(coord, step_x, step_y, expansion, multiplier = 1.):\n",
    "    ''' Calculates the four corners of a bounding box of step_x * step_y offset from coord'''\n",
    "    coord_bl = np.copy(coord)\n",
    "    coord1 = offset_x(coord_bl, 6300*step_x - expansion)\n",
    "    coord1 = offset_y(coord1 , 6300*step_y - expansion)\n",
    "    \n",
    "    coord_tr = np.copy(coord)\n",
    "    coord2 = offset_x(coord_tr, 6300*(step_x + multiplier) + expansion)\n",
    "    coord2 = offset_y(coord2, 6300*(step_y + multiplier) + expansion)\n",
    "    bbx = (coord2, coord1)\n",
    "    return bbx\n",
    "\n",
    "\n",
    "def calculate_bbx_pyproj(coord, step_x, step_y, expansion, multiplier = 1.):\n",
    "    ''' Calculates the four corners of a bounding box as above\n",
    "        but uses pyproj instead of OGR. It seems sentinelhub uses\n",
    "        pyproj, so this may be more pixel accurate (?)\n",
    "        x, y format\n",
    "    '''\n",
    "    \n",
    "    inproj = Proj('epsg:4326')\n",
    "    outproj_code = calculate_epsg(coord)\n",
    "    outproj = Proj('epsg:' + str(outproj_code))\n",
    "    \n",
    "    \n",
    "    \n",
    "    coord_utm =  transform(inproj, outproj, coord[1], coord[0])\n",
    "    coord_utm_bottom_left = (coord_utm[0] + step_x*6300 - expansion,\n",
    "                             coord_utm[1] + step_y*6300 - expansion)\n",
    "    \n",
    "    coord_utm_top_right = (coord_utm[0] + (step_x+multiplier) * 6300 + expansion,\n",
    "                           coord_utm[1] + (step_y+multiplier) * 6300 + expansion)\n",
    "\n",
    "    \n",
    "    zone = str(outproj_code)[3:]\n",
    "    direction = 'N' if coord[1] >= 0 else 'S'\n",
    "    utm_epsg = \"UTM_\" + zone + direction\n",
    "    return (coord_utm_bottom_left, coord_utm_top_right), CRS[utm_epsg]\n",
    "\n",
    "def pts_in_geojson(lats, longs, geojson):  \n",
    "    polys = geopandas.read_file(geojson)['geometry']\n",
    "    polys = geopandas.GeoSeries(polys)\n",
    "    pnts = [Point(x, y) for x, y in zip(list(lats), list(longs))]\n",
    "    \n",
    "    def _contains(pt):\n",
    "        return polys.contains(pt)[0]\n",
    "\n",
    "    if any([_contains(pt) for pt in pnts]):\n",
    "        return True\n",
    "    else: return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_clouds_new(bbox, epsg, dates = dates):\n",
    "\n",
    "    #for try_ in range(0, 5):\n",
    "        #try:\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    cloud_request = WmsRequest(\n",
    "        layer='CLOUD_NEW',\n",
    "        bbox=box,\n",
    "        time=dates,\n",
    "        width=(5*9*14)+2,\n",
    "        height=(5*9*14)+2,\n",
    "        image_format = MimeType.TIFF_d8,\n",
    "        maxcc=0.7,\n",
    "        instance_id=API_KEY,\n",
    "        custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "        time_difference=datetime.timedelta(hours=72),\n",
    "    )\n",
    "\n",
    "\n",
    "    shadow_request = WmsRequest(\n",
    "        layer='SHADOW',\n",
    "        bbox=box,\n",
    "        time=dates,\n",
    "        width=(5*9*14)+2,\n",
    "        height=(5*9*14)+2,\n",
    "        image_format =  MimeType.TIFF_d16,\n",
    "        maxcc=0.7,\n",
    "        instance_id=API_KEY,\n",
    "        custom_url_params = {constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "        time_difference=datetime.timedelta(hours=72))\n",
    "\n",
    "    cloud_img = cloud_request.get_data()\n",
    "    cloud_img = np.array(cloud_img)\n",
    "    print(f\"The max clouds is {np.max(cloud_img)}\")\n",
    "    if np.max(cloud_img > 10):\n",
    "        cloud_img = cloud_img / 255\n",
    "        \n",
    "    assert np.max(cloud_img) <= 1.\n",
    "    print(\"Cloud_probs shape: {}\".format(cloud_img.shape))\n",
    "    \n",
    "    n_cloud_px = np.array([len(np.argwhere(cloud_img[x, :, :].reshape((632)*(632)) > 0.33))\n",
    "                           for x in range(cloud_img.shape[0])])\n",
    "    cloud_steps = np.argwhere(n_cloud_px > 632**2 / 5)\n",
    "    clean_steps = [x for x in range(cloud_img.shape[0]) if x not in cloud_steps]\n",
    "    print(f\"Removing {len(cloud_steps)} from S2 download, saving {7.32 * len(cloud_steps)} PU\")\n",
    "\n",
    "    shadow_img = shadow_request.get_data(data_filter = clean_steps)\n",
    "    shadow_img = np.array(shadow_img)\n",
    "    print(\"Shadows_shape: {}\".format(shadow_img.shape))\n",
    "    print(f\"The max shadows is {np.max(shadow_img)}\")\n",
    "    if np.max(shadow_img > 10):\n",
    "        shadow_img = shadow_img / 65535\n",
    "    print(np.max(shadow_img))\n",
    " \n",
    "    cloud_img = np.delete(cloud_img, cloud_steps, 0)\n",
    "    shadows = mcm_shadow_mask(np.array(shadow_img), cloud_img)\n",
    "\n",
    "    print(f\"Cloud probs: {cloud_img.shape}\")\n",
    "    print(f\"Shadow shape {shadows.shape}\")\n",
    "    return cloud_img, cloud_img, shadows, clean_steps\n",
    "\n",
    "    \n",
    "    \n",
    "def download_dem(bbox, epsg):\n",
    "\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    dem_s = (630)+4\n",
    "    dem_request = WmsRequest(data_source=DataSource.DEM,\n",
    "                         layer='DEM',\n",
    "                         bbox=box,\n",
    "                         width=dem_s,\n",
    "                         height=dem_s,\n",
    "                         instance_id=API_KEY,\n",
    "                         image_format=MimeType.TIFF_d32f,\n",
    "                         custom_url_params={CustomUrlParam.SHOWLOGO: False})\n",
    "    dem_image = dem_request.get_data()[0]\n",
    "    dem_image = calcSlope(dem_image.reshape((1, dem_s, dem_s)),\n",
    "                  np.full((dem_s, dem_s), 10), np.full((dem_s, dem_s), 10), zScale = 1, minSlope = 0.02)\n",
    "    dem_image = dem_image.reshape((dem_s,dem_s, 1))\n",
    "    dem_image = dem_image[1:dem_s-1, 1:dem_s-1, :]\n",
    "    return dem_image #/ np.max(dem_image)\n",
    " \n",
    "\n",
    "def download_layer(bbox,  clean_steps, epsg, dates = dates, year = 2019):\n",
    "    \"\"\" Downloads the L2A sentinel layer with 10 and 20 meter bands\n",
    "        \n",
    "        Parameters:\n",
    "         bbox (list): output of calc_bbox\n",
    "         epsg (float): EPSG associated with bbox \n",
    "         time (tuple): YY-MM-DD - YY-MM-DD bounds for downloading \n",
    "    \n",
    "        Returns:\n",
    "         img (arr):\n",
    "         img_request (obj): \n",
    "    \"\"\"\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    image_request = WcsRequest(\n",
    "            layer='L2A20',\n",
    "            bbox=box,\n",
    "            time=dates,\n",
    "            image_format = MimeType.TIFF_d16,\n",
    "            maxcc=0.7,\n",
    "            resx='20m', resy='20m',\n",
    "            instance_id=API_KEY,\n",
    "            custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'NEAREST',\n",
    "                                constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "            time_difference=datetime.timedelta(hours=72),\n",
    "        )\n",
    "    print(\"Downloading L2A 20m layer\")\n",
    "    img_bands = image_request.get_data(data_filter = clean_steps)\n",
    "    img_20 = np.stack(img_bands)\n",
    "    print(f\"The max 20m is {np.max(img_20)}\")\n",
    "    if np.max(img_20) >= 10:\n",
    "        img_20 = img_20 / 65535\n",
    "    assert np.max(img_20) <= 2.\n",
    "    \n",
    "    s2_20_usage = (img_20.shape[1]*img_20.shape[2])/(512*512) * (6/3) * img_20.shape[0]\n",
    "    print(\"Original 20 meter bands size: {}, using {} PU\".format(img_20.shape, s2_20_usage))\n",
    "    img_20 = resize(img_20, (img_20.shape[0], 632, 632, img_20.shape[-1]), order = 0)\n",
    "\n",
    "    image_request = WcsRequest(\n",
    "            layer='L2A10',\n",
    "            bbox=box,\n",
    "            time=dates,\n",
    "            image_format = MimeType.TIFF_d16,\n",
    "            maxcc=0.7,\n",
    "            resx='10m', resy='10m',\n",
    "            instance_id=API_KEY,\n",
    "            custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'BICUBIC',\n",
    "                                constants.CustomUrlParam.UPSAMPLING: 'BICUBIC'},\n",
    "            time_difference=datetime.timedelta(hours=72),\n",
    "    )\n",
    "    print(\"Downloading L2A 10m layer\")\n",
    "    \n",
    "    img_bands = image_request.get_data(data_filter = clean_steps)\n",
    "    img_10 = np.stack(img_bands)\n",
    "    print(f\"The max 10m is {np.max(img_10)}\")\n",
    "    if np.max(img_10) >= 10:\n",
    "        img_10 = img_10 / 65535\n",
    "    assert np.max(img_10) <= 2.\n",
    "    \n",
    "    s2_10_usage = (img_10.shape[1]*img_10.shape[2])/(512*512) * (4/3) * img_10.shape[0]\n",
    "    print(\"Original 20 meter bands size: {}, using {} PU\".format(img_10.shape, s2_10_usage))\n",
    "    img_10 = resize(img_10, (img_10.shape[0], 632, 632, img_10.shape[-1]), order = 0)\n",
    "\n",
    "    img = np.concatenate([img_10, img_20], axis = -1)\n",
    "    \n",
    "    \n",
    "    print(f\"Sentinel 2 used {s2_20_usage + s2_10_usage} PU\")\n",
    "\n",
    "    image_dates = []\n",
    "    for date in image_request.get_dates():\n",
    "        if date.year == year - 1:\n",
    "            image_dates.append(-365 + starting_days[(date.month-1)] + date.day)\n",
    "        if date.year == year:\n",
    "            image_dates.append(starting_days[(date.month-1)] + date.day)\n",
    "        if date.year == year + 1:\n",
    "            image_dates.append(365 + starting_days[(date.month-1)]+date.day)\n",
    "    image_dates = [val for idx, val in enumerate(image_dates) if idx in clean_steps]\n",
    "    image_dates = np.array(image_dates)\n",
    "\n",
    "    return img, image_dates\n",
    "        \n",
    "        \n",
    "def download_sentinel_1(bbox, epsg, imsize = 632, \n",
    "                        dates = dates, layer = \"SENT\", year = 2019):\n",
    "    #for try_ in range(5):\n",
    "        #try:\n",
    "    box = BBox(bbox, crs = epsg)\n",
    "    image_request = WcsRequest(\n",
    "            layer=layer,\n",
    "            bbox=box,\n",
    "            time=dates,\n",
    "            image_format = MimeType.TIFF_d16,\n",
    "            data_source=DataSource.SENTINEL1_IW,\n",
    "            maxcc=1.0,\n",
    "            resx='10m', resy='5m',\n",
    "            instance_id=API_KEY,\n",
    "            custom_url_params = {constants.CustomUrlParam.DOWNSAMPLING: 'NEAREST',\n",
    "                                constants.CustomUrlParam.UPSAMPLING: 'NEAREST'},\n",
    "            time_difference=datetime.timedelta(hours=96),\n",
    "        )\n",
    "    data_filter = None\n",
    "    if len(image_request.download_list) > 50:\n",
    "        data_filter = [x for x in range(len(image_request.download_list)) if x % 2 == 0]\n",
    "    img_bands = image_request.get_data(data_filter = data_filter)\n",
    "    s1 = np.stack(img_bands)\n",
    "    if np.max(s1) >= 1000:\n",
    "            s1 = s1 / 65535.\n",
    "    \n",
    "    print(f\"The max s1 is {np.max(s1)}\")\n",
    "    print(f\"Sentinel 1 used {(2/3)*s1.shape[0] * (s1.shape[1]*s1.shape[2])/(512*512)} PU for \\\n",
    "          {s1.shape[0]} out of {len(image_request.download_list)} images\")\n",
    "    s1 = resize(s1, (s1.shape[0], imsize*2, imsize*2, s1.shape[-1]), order = 0)\n",
    "    s1 = np.reshape(s1, (s1.shape[0], s1.shape[1]//2, 2, s1.shape[2] // 2, 2, s1.shape[-1]))\n",
    "    s1 = np.mean(s1, (2, 4))\n",
    "    \n",
    "   \n",
    "\n",
    "    image_dates = []\n",
    "    for date in image_request.get_dates():\n",
    "        if date.year == year - 1:\n",
    "            image_dates.append(-365 + starting_days[(date.month-1)] + date.day)\n",
    "        if date.year == year:\n",
    "            image_dates.append(starting_days[(date.month-1)] + date.day)\n",
    "        if date.year == year + 1:\n",
    "            image_dates.append(365 + starting_days[(date.month-1)]+date.day)\n",
    "    image_dates = np.array(image_dates)\n",
    "    s1c = np.copy(s1)\n",
    "    s1c[np.where(s1c < 1.)] = 0\n",
    "    n_pix_oob = np.sum(s1c, axis = (1, 2, 3))\n",
    "    to_remove = np.argwhere(n_pix_oob > (imsize*2*imsize*2)/50)\n",
    "    s1 = np.delete(s1, to_remove, 0)\n",
    "    image_dates = np.delete(image_dates, to_remove)\n",
    "    return s1, image_dates\n",
    "\n",
    "        #except Exception as e:\n",
    "            #logging.fatal(e, exc_info=True)\n",
    "            #sleep((try_+1)*30)\n",
    "\n",
    "def identify_s1_layer(coords):\n",
    "    results = rg.search(coords)\n",
    "    country = results[-1]['cc']\n",
    "    continent_name = pc.country_alpha2_to_continent_code(country)\n",
    "    if continent_name in ['AF', 'OC']:\n",
    "        layer = \"SENT\"\n",
    "    if continent_name in ['SA']:\n",
    "        if coords[0] > -7.11:\n",
    "            layer = \"SENT\"\n",
    "        else:\n",
    "            layer = \"SENT_DESC\"\n",
    "    if continent_name in ['AS']:\n",
    "        if coords[0] > 23.3:\n",
    "            layer = \"SENT\"\n",
    "        else:\n",
    "            layer = \"SENT_DESC\"\n",
    "    if continent_name in ['NA']:\n",
    "        layer = \"SENT_DESC\"\n",
    "    print(continent_name)\n",
    "    print(layer)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud and shadow removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_cloud_and_shadows(tiles, c_probs, shadows, image_dates):\n",
    "    \"\"\" Interpolates clouds and shadows for each time step with \n",
    "        linear combination of proximal clean time steps for each\n",
    "        region of specified window size\n",
    "        \n",
    "        Parameters:\n",
    "         tiles (arr):\n",
    "         probs (arr): \n",
    "         shadows (arr):\n",
    "         image_dates (list):\n",
    "         wsize (int): \n",
    "    \n",
    "        Returns:\n",
    "         tiles (arr): \n",
    "    \"\"\"\n",
    "    wsize = 5\n",
    "    c_probs = c_probs - np.min(c_probs, axis = 0)\n",
    "    c_probs[np.where(c_probs > 0.33)] = 1.\n",
    "    c_probs[np.where(c_probs < 0.33)] = 0.\n",
    "    c_probs = np.reshape(c_probs, (c_probs.shape[0], 632//8, 8, 632//8, 8))\n",
    "    c_probs = np.sum(c_probs, (2, 4))\n",
    "    c_probs = resize(c_probs, (c_probs.shape[0], 632, 632), 0)\n",
    "    c_probs[np.where(c_probs < 16)] = 0\n",
    "    c_probs[np.where(c_probs >= 16)] = 1\n",
    "    secondary_c_probs = np.copy(c_probs)\n",
    "    c_probs += shadows\n",
    "    c_probs[np.where(c_probs >= 1.)] = 1.\n",
    "    number_interpolated = 0\n",
    "    for cval in tnrange(0, IMSIZE - 4, 2):\n",
    "        for rval in range(0, IMSIZE - 4, 2):\n",
    "            subs = c_probs[:, cval:cval + wsize, rval:rval+wsize]\n",
    "            sums = np.sum(subs, axis = (1, 2))\n",
    "            satisfactory = [x for x in range(c_probs.shape[0]) if sums[x] < 8]\n",
    "            if len(satisfactory) == 0:\n",
    "                satisfactory = [x for x in range(c_probs.shape[0])]\n",
    "            satisfactory = np.array(satisfactory)\n",
    "            for date in range(0, tiles.shape[0]):\n",
    "                if np.sum(subs[date, :, :]) > 8:\n",
    "                    number_interpolated += 1\n",
    "                    before, after = calculate_proximal_steps(date, satisfactory)\n",
    "                    before = date + before\n",
    "                    after = date + after\n",
    "                    bef = tiles[before, cval:cval+wsize, rval:rval+wsize, : ]\n",
    "                    aft = tiles[after, cval:cval+wsize, rval:rval+wsize, : ]\n",
    "                    before = image_dates[before]\n",
    "                    after = image_dates[after]\n",
    "                    before_diff = abs(image_dates[date] - before)\n",
    "                    after_diff = abs(image_dates[date] - after)\n",
    "                    bef_wt = 1 - before_diff / (before_diff + after_diff)\n",
    "                    aft_wt = 1 - bef_wt\n",
    "                    candidate = bef_wt*bef + aft_wt*aft\n",
    "                    candidate = candidate*c_arr + tiles[date, cval:cval+wsize, rval:rval+wsize, : ]*o_arr\n",
    "                    tiles[date, cval:cval+wsize, rval:rval+wsize, : ] = candidate \n",
    "    print(\"A total of {} pixels were interpolated\".format(number_interpolated))\n",
    "    return tiles, c_probs, secondary_c_probs\n",
    "\n",
    "def remove_missed_clouds(img):\n",
    "    iqr = np.percentile(img[:, :, :, 3].flatten(), 75) - np.percentile(img[:, :, :, 3].flatten(), 25)\n",
    "    thresh_t = np.percentile(img[:, :, :, 3].flatten(), 75) + iqr*2\n",
    "    thresh_b = np.percentile(img[:, :, :, 3].flatten(), 25) - iqr*2\n",
    "    diffs_fw = np.diff(img, 1, axis = 0)\n",
    "    diffs_fw = np.mean(diffs_fw, axis = (1, 2, 3))\n",
    "    diffs_fw = np.array([0] + list(diffs_fw))\n",
    "    diffs_bw = np.diff(np.flip(img, 0), 1, axis = 0)\n",
    "    diffs_bw = np.flip(np.mean(diffs_bw, axis = (1, 2, 3)))\n",
    "    diffs_bw = np.array(list(diffs_bw) + [0])\n",
    "    diffs = abs(diffs_fw - diffs_bw) * 100\n",
    "    #diffs = [int(x) for x in diffs]\n",
    "    outlier_percs = []\n",
    "    for step in range(img.shape[0]):\n",
    "        bottom = len(np.argwhere(img[step, :, :, 3].flatten() > thresh_t))\n",
    "        top = len(np.argwhere(img[step, :, :, 3].flatten() < thresh_b))\n",
    "        p = 100* ((bottom + top) / (IMSIZE*IMSIZE))\n",
    "        outlier_percs.append(p)\n",
    "    to_remove = np.argwhere(np.array(outlier_percs) > 20)\n",
    "    return to_remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data interpolation\n",
    "\n",
    "Because the `smooth` function is called 5.6 million times on each 4000 hectare array, most of the computations are done outside of the function with the predefined `lmbd` and `d`, and then passed in as the `coefmat`, to avoid needless recomputation. This saves 141 CPU minutes per 4000 hectare array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14889642199705122\n"
     ]
    }
   ],
   "source": [
    "coefmat = intialize_smoother()\n",
    "test = np.ones((72, 10000))\n",
    "s = timer()\n",
    "x = parallel_apply_along_axis(smooth, 0, test)\n",
    "e = timer()\n",
    "print(e - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17638493900449248\n"
     ]
    }
   ],
   "source": [
    "test = np.ones((72, 10000))\n",
    "s = timer()\n",
    "x = np.apply_along_axis(smooth, 0, test)\n",
    "e = timer()\n",
    "print(e - s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiling and coordinate selection functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbolic Model Created.\n"
     ]
    }
   ],
   "source": [
    "MDL_PATH = \"../src/dsen2/models/\"\n",
    "\n",
    "input_shape = ((4, None, None), (6, None, None))\n",
    "model = s2model(input_shape, num_layers=6, feature_size=128)\n",
    "predict_file = MDL_PATH+'s2_032_lr_1e-04.hdf5'\n",
    "print('Symbolic Model Created.')\n",
    "\n",
    "model.load_weights(predict_file)\n",
    "\n",
    "def DSen2(d10, d20):\n",
    "    \"\"\"Super resolves 20 meter bans using the DSen2 convolutional\n",
    "       neural network, as specified in Lanaras et al. 2018\n",
    "       https://github.com/lanha/DSen2\n",
    "\n",
    "        Parameters:\n",
    "         d10 (arr): (4, X, Y) shape array with 10 meter resolution\n",
    "         d20 (arr): (6, X, Y) shape array with 20 meter resolution\n",
    "\n",
    "        Returns:\n",
    "         prediction (arr): (6, X, Y) shape array with 10 meter superresolved\n",
    "                          output of DSen2 on d20 array\n",
    "    \"\"\"\n",
    "    test = [d10, d20]\n",
    "    input_shape = ((4, None, None), (6, None, None))\n",
    "    prediction = _predict(test, input_shape, deep=False)\n",
    "    return prediction\n",
    "\n",
    "def _predict(test, input_shape, model = model, deep=False, run_60=False):\n",
    "    \n",
    "    prediction = model.predict(test, verbose=1)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_per_month = [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30]\n",
    "starting_days = np.cumsum(days_per_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_proximal_steps(date, satisfactory):\n",
    "    arg_before = None\n",
    "    arg_after = None\n",
    "    if date > 0:\n",
    "        idx_before = satisfactory - date\n",
    "        arg_before = idx_before[np.where(idx_before < 0, idx_before, -np.inf).argmax()]\n",
    "    if date < np.max(satisfactory):\n",
    "        idx_after = satisfactory - date\n",
    "        arg_after = idx_after[np.where(idx_after > 0, idx_after, np.inf).argmin()]\n",
    "    if not arg_after and not arg_before:\n",
    "        arg_after = date\n",
    "        arg_before = date\n",
    "    if not arg_after:\n",
    "        arg_after = arg_before\n",
    "    if not arg_before:\n",
    "        arg_before = arg_after\n",
    "    #print(arg_before, date, arg_after)\n",
    "    return arg_before, arg_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def mcm_shadow_mask(arr, c_probs):\n",
    "    \"\"\" Calculates the multitemporal shadow mask for Sentinel-2 using\n",
    "        the methods from Candra et al. 2020 on L1C images and matching\n",
    "        outputs to the s2cloudless cloud probabilities\n",
    "\n",
    "        Parameters:\n",
    "         arr (arr): (Time, X, Y, Band) array of L1C data scaled from [0, 1]\n",
    "         c_probs (arr): (Time, X, Y) array of S2cloudless cloud probabilities\n",
    "    \n",
    "        Returns:\n",
    "         shadows_new (arr): cloud mask after Candra et al. 2020 and cloud matching \n",
    "         shadows_original (arr): cloud mask after Candra et al. 2020\n",
    "    \"\"\"\n",
    "    def _rank_array(arr):\n",
    "        order = arr.argsort()\n",
    "        ranks = order.argsort()\n",
    "        return ranks\n",
    "    mean_c_probs = np.mean(c_probs, axis = (1, 2))\n",
    "    cloudy_steps = np.argwhere(mean_c_probs > 0.25)\n",
    "    images_clean = np.delete(arr, cloudy_steps, 0)\n",
    "    cloud_ranks = _rank_array(mean_c_probs)\n",
    "    diffs = abs(np.sum(arr - np.mean(images_clean, axis = 0), axis = (1, 2, 3)))\n",
    "    diff_ranks = _rank_array(diffs)\n",
    "    overall_rank = diff_ranks + cloud_ranks\n",
    "    reference_idx = np.argmin(overall_rank)\n",
    "    ri = arr[reference_idx]\n",
    "        \n",
    "    shadows = np.zeros((arr.shape[0], 632, 632))    \n",
    "    # Candra et al. 2020\n",
    "    \n",
    "    for time in tnrange(arr.shape[0]):\n",
    "        for x in range(arr.shape[1]):\n",
    "            for y in range(arr.shape[2]):\n",
    "                ti_slice = arr[time, x, y]\n",
    "                ri_slice = ri[x, y]\n",
    "                deltab2 = ti_slice[0] - ri_slice[0]\n",
    "                deltab8a = ti_slice[1] - ri_slice[1]\n",
    "                deltab11 = ti_slice[2] - ri_slice[2]\n",
    "\n",
    "                if deltab2 < 0.10: #(1000/65535):\n",
    "                    if deltab8a < -0.04: #(-400/65535):\n",
    "                        if deltab11 < -0.04: #(-400/65535):\n",
    "                            if ti_slice[0] < 0.095: #(950/65535):\n",
    "                                shadows[time, x, y] = 1.\n",
    "                                                       \n",
    "                            \n",
    "    # Remove shadows if cannot coreference a cloud\n",
    "    shadow_large = np.reshape(shadows, (shadows.shape[0], 79, 8, 79, 8))\n",
    "    shadow_large = np.sum(shadow_large, axis = (2, 4))\n",
    "    \n",
    "    cloud_large = np.copy(c_probs)\n",
    "    cloud_large[np.where(c_probs > 0.33)] = 1.\n",
    "    cloud_large[np.where(c_probs < 0.33)] = 0.\n",
    "    cloud_large = np.reshape(cloud_large, (shadows.shape[0], 79, 8, 79, 8))\n",
    "    cloud_large = np.sum(cloud_large, axis = (2, 4))\n",
    "    for time in tnrange(shadow_large.shape[0]):\n",
    "        for x in range(shadow_large.shape[1]):\n",
    "            x_low = np.max([x - 8, 0])\n",
    "            x_high = np.min([x + 8, shadow_large.shape[1] - 1])\n",
    "            for y in range(shadow_large.shape[2]):\n",
    "                y_low = np.max([y - 8, 0])\n",
    "                y_high = np.min([y + 8, shadow_large.shape[1] - 1])\n",
    "                if shadow_large[time, x, y] < 8:\n",
    "                    shadow_large[time, x, y] = 0.\n",
    "                if shadow_large[time, x, y] >= 8:\n",
    "                    shadow_large[time, x, y] = 1.\n",
    "                c_prob_window = cloud_large[time, x_low:x_high, y_low:y_high]\n",
    "                if np.max(c_prob_window) < 16:\n",
    "                    shadow_large[time, x, y] = 0.\n",
    "                    \n",
    "    shadow_large = resize(shadow_large, (shadow_large.shape[0], 632, 632), order = 0)\n",
    "    shadows *= shadow_large\n",
    "    \n",
    "    # Go through and aggregate the shadow map to an 80m grid, and extend it one grid size around\n",
    "    # any positive ID\n",
    "    \n",
    "    shadows = np.reshape(shadows, (shadows.shape[0], 79, 8, 79, 8))\n",
    "    shadows = np.sum(shadows, axis = (2, 4))\n",
    "    shadows[np.where(shadows < 12)] = 0.\n",
    "    shadows[np.where(shadows >= 12)] = 1.\n",
    "    \n",
    "    shadows = resize(shadows, (shadows.shape[0], 632, 632), order = 0)\n",
    "    shadows = np.reshape(shadows, (shadows.shape[0], 632//4, 4, 632//4, 4))\n",
    "    shadows = np.max(shadows, (2, 4))\n",
    "    \n",
    "    shadows_new = np.zeros_like(shadows)\n",
    "    for time in range(shadows.shape[0]):\n",
    "        for x in range(shadows.shape[1]):\n",
    "            for y in range(shadows.shape[2]):\n",
    "                if shadows[time, x, y] == 1:\n",
    "                    min_x = np.max([x - 1, 0])\n",
    "                    max_x = np.min([x + 2, 157])\n",
    "                    min_y = np.max([y - 1, 0])\n",
    "                    max_y = np.min([y + 2, 157])\n",
    "                    for x_idx in range(min_x, max_x):\n",
    "                        for y_idx in range(min_y, max_y):\n",
    "                            shadows_new[time, x_idx, y_idx] = 1.\n",
    "    shadows_new = resize(shadows_new, (shadows.shape[0], 632, 632), order = 0)\n",
    "    return shadows_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_output_and_temp_folders(idx, output_folder = OUTPUT_FOLDER):\n",
    "    \n",
    "    def _find_and_make_dirs(dirs):\n",
    "        if not os.path.exists(os.path.realpath(dirs)):\n",
    "            os.makedirs(os.path.realpath(dirs))\n",
    "            \n",
    "    _find_and_make_dirs(output_folder + \"raw/\")\n",
    "    _find_and_make_dirs(output_folder + \"raw/clouds/\")\n",
    "    _find_and_make_dirs(output_folder + \"raw/s1/\")\n",
    "    _find_and_make_dirs(output_folder + \"raw/s2/\")\n",
    "    _find_and_make_dirs(output_folder + \"raw/misc/\")\n",
    "    _find_and_make_dirs(output_folder + \"processed/\")\n",
    "    \n",
    "\n",
    "def check_contains(coord, step_x, step_y, folder = OUTPUT_FOLDER, year = 2019, s1_layer = \"SENT\"):\n",
    "    contains = False\n",
    "    bottomleft = offset_x(coord, step_x*6300)\n",
    "    bottomleft = offset_y(bottomleft , step_y*6300)\n",
    "    for subtile in range(1, 5, 1):\n",
    "        #bottomright = offset_x(bottomleft, (subtile*6300) / 5)\n",
    "        #bottomright = offset_y(bottomright, (subtile*6300) / 5)\n",
    "        #bbx = calculate_bbx(coord, step_x * (subtile/10), step_y * (subtile / 10), expansion = 10)\n",
    "        if os.path.exists(folder):\n",
    "            if any([x.endswith(\".geojson\") for x in os.listdir(folder)]):\n",
    "                geojson_path = folder + [x for x in os.listdir(folder) if x.endswith(\".geojson\")][0]\n",
    "                bool_contains = pts_in_geojson(lats = [bottomleft[0], bottomright[0]], \n",
    "                                               longs = [bottomleft[1], bottomright[1]],\n",
    "                                               geojson = geojson_path)\n",
    "               # bool_contains = pts_in_geojson([bbx[0][0], bbx[1][0]],\n",
    "                #                          [bbx[0][1], bbx[1][1]],\n",
    "                #                          geojson_path)\n",
    "                contains = True if bool_contains else contains\n",
    "            else:\n",
    "                contains = True\n",
    "    return contains\n",
    "\n",
    "def download_large_tile(coord, step_x, step_y, folder = OUTPUT_FOLDER, year = 2019, s1_layer = \"SENT\"):\n",
    "    \n",
    "    bbx, epsg = calculate_bbx_pyproj(coord, step_x, step_y, expansion = 10)\n",
    "    dem_bbx, _ = calculate_bbx_pyproj(coord, step_x, step_y, expansion = 20)\n",
    "    idx = str(step_y) + \"_\" + str(step_x)\n",
    "    idx = str(idx)\n",
    "    make_output_and_temp_folders(idx)\n",
    "\n",
    "    print(\"Calculating cloud cover\")\n",
    "    if not os.path.exists(folder + \"output/\" + str(((step_y+1)*5)-1) + \"/\" + str(((step_x+1)*5)-1) + \".npy\"):\n",
    "        if not os.path.exists(folder + \"processed/\" + str(((step_y+1)*5)-1) + \"/\" + str(((step_x+1)*5)-1) + \".hkl\"):\n",
    "            if not os.path.exists(folder + \"raw/clouds/clouds_{}.hkl\".format(idx)):\n",
    "                l1c, cloud_probs, shadows, clean_steps = identify_clouds_new(bbx, epsg = epsg)\n",
    "                hkl.dump(cloud_probs, folder + \"raw/clouds/clouds_{}.hkl\".format(idx), mode='w', compression='gzip')\n",
    "                hkl.dump(shadows, folder + \"raw/clouds/shadows_{}.hkl\".format(idx), mode='w', compression='gzip')\n",
    "                hkl.dump(clean_steps, folder + \"raw/clouds/clean_steps_{}.hkl\".format(idx), mode='w', compression='gzip')\n",
    "            \n",
    "            if not os.path.exists(folder + \"raw/s1/{}.hkl\".format(idx)):\n",
    "                print(\"Downloading S1\")\n",
    "                s1_layer = identify_s1_layer((coord[1], coord[0]))\n",
    "                s1, s1_dates = download_sentinel_1(bbx, layer = s1_layer, epsg = epsg)\n",
    "                s1 = process_sentinel_1_tile(s1, s1_dates)\n",
    "                hkl.dump(s1, folder + \"raw/s1/{}.hkl\".format(idx), mode='w', compression='gzip')\n",
    "                hkl.dump(s1_dates, folder + \"raw/misc/s1_dates_{}.hkl\".format(idx), mode='w', compression='gzip')\n",
    "\n",
    "            if not os.path.exists(folder + \"raw/s2/{}.hkl\".format(idx)):\n",
    "                print(\"Downloading S2\")\n",
    "                if 'clean_steps' not in globals() or locals():\n",
    "                    clean_steps = hkl.load(folder + \"raw/clouds/clean_steps_{}.hkl\".format(idx))\n",
    "                s2, s2_dates = download_layer(bbx, clean_steps = clean_steps, epsg = epsg)\n",
    "                hkl.dump(s2, folder + \"raw/s2/{}.hkl\".format(idx), mode='w', compression='gzip')\n",
    "                hkl.dump(s2_dates, folder + \"raw/misc/s2_dates_{}.hkl\".format(idx), mode='w', compression='gzip')\n",
    "\n",
    "            if not os.path.exists(folder + \"raw/misc/dem_{}.hkl\".format(idx)):\n",
    "                print(\"Downloading DEM\")\n",
    "                dem = download_dem(dem_bbx, epsg = epsg) # get the DEM BBOX\n",
    "                hkl.dump(dem, folder + \"raw/misc/dem_{}.hkl\".format(idx), mode='w', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bad_steps(sentinel2, clouds):\n",
    "    n_cloud_px = np.array([len(np.argwhere(clouds[x, :, :].reshape((632)*(632)) > 0.33)) for x in range(clouds.shape[0])])\n",
    "    cloud_steps = np.argwhere(n_cloud_px > 632**2 / 5)\n",
    "    missing_images = [np.argwhere(sentinel2[x, :, : :10].flatten() == 0.0) for x in range(sentinel2.shape[0])]\n",
    "    missing_images = np.array([len(x) for x in missing_images])\n",
    "    missing_images_p = [np.argwhere(sentinel2[x, :, : :10].flatten() >= 1) for x in range(sentinel2.shape[0])]\n",
    "    missing_images_p = np.array([len(x) for x in missing_images_p])\n",
    "    missing_images += missing_images_p\n",
    "    missing_images = np.argwhere(missing_images >= (632**2) / 50)\n",
    "    to_remove = np.unique(np.concatenate([cloud_steps.flatten(), missing_images.flatten()]))\n",
    "    return to_remove\n",
    "\n",
    "def superresolve(sentinel2):\n",
    "    d10 = sentinel2[:, :, :, 0:4]\n",
    "    d20 = sentinel2[:, :, :, 4:10]\n",
    "\n",
    "    d10 = np.swapaxes(d10, 1, -1)\n",
    "    d10 = np.swapaxes(d10, 2, 3)\n",
    "    d20 = np.swapaxes(d20, 1, -1)\n",
    "    d20 = np.swapaxes(d20, 2, 3)\n",
    "    superresolved = DSen2(d10, d20)\n",
    "    superresolved = np.swapaxes(superresolved, 1, -1)\n",
    "    superresolved = np.swapaxes(superresolved, 1, 2)\n",
    "    return superresolved # returns band IDXs 3, 4, 5, 7, 8, 9\n",
    "\n",
    "def process_sentinel_1_tile(sentinel1, dates):\n",
    "    s1 = calculate_and_save_best_images(sentinel1, dates)\n",
    "    # Retain only iamgery every 15 days\n",
    "    biweekly_dates = np.array([day for day in range(0, 360, 5)])\n",
    "    to_remove = np.argwhere(biweekly_dates % 15 != 0)\n",
    "    s1 = np.delete(s1, to_remove, 0)\n",
    "    return s1\n",
    "\n",
    "\n",
    "def process_large_tile(coord, step_x, step_y, folder = OUTPUT_FOLDER):\n",
    "    idx = str(step_y) + \"_\" + str(step_x)\n",
    "    x_vals = []\n",
    "    y_vals = []\n",
    "    # save to disk\n",
    "    for i in range(25):\n",
    "        y_val = (24 - i) // 5\n",
    "        x_val = 5 - ((25 - i) % 5)\n",
    "        x_val = 0 if x_val == 5 else x_val\n",
    "        x_vals.append(x_val)\n",
    "        y_vals.append(y_val)\n",
    "\n",
    "    y_vals = [i + (5*step_y) for i in y_vals]\n",
    "    x_vals = [i + (5*step_x) for i in x_vals]\n",
    "\n",
    "    processed = False\n",
    "    for x, y in zip(x_vals, y_vals):\n",
    "        if (os.path.exists(folder + \"processed/{}/{}.hkl\".format(str(y), str(x))) or\n",
    "            (os.path.exists(folder + \"output/{}/{}.npy\".format(str(y), str(x))))):\n",
    "            processed = True\n",
    "    if not processed:\n",
    "\n",
    "        clouds = hkl.load(folder + \"raw/clouds/clouds_{}.hkl\".format(idx))\n",
    "        sentinel1 = hkl.load(folder + \"raw/s1/{}.hkl\".format(idx))\n",
    "        radar_dates = hkl.load(folder + \"raw/misc/s1_dates_{}.hkl\".format(idx))\n",
    "        sentinel2 = hkl.load(folder + \"raw/s2/{}.hkl\".format(idx))\n",
    "        dem = hkl.load(folder + \"raw/misc/dem_{}.hkl\".format(idx))\n",
    "        image_dates = hkl.load(folder + \"raw/misc/s2_dates_{}.hkl\".format(idx))\n",
    "        if os.path.exists(folder + \"raw/clouds/shadows_{}.hkl\".format(idx)):\n",
    "            shadows = hkl.load(folder + \"raw/clouds/shadows_{}.hkl\".format(idx))\n",
    "        else:\n",
    "            print(\"No shadows file, so calculating shadows with L2A\")\n",
    "            shadows = mcm_shadow_mask(sentinel2, clouds)\n",
    "        print(\"The files have been loaded\")\n",
    "\n",
    "        #sentinel1 = process_sentinel_1_tile(sentinel1, radar_dates)\n",
    "        to_remove = calculate_bad_steps(sentinel2, clouds)\n",
    "        sentinel2 = np.delete(sentinel2, to_remove, axis = 0)\n",
    "        clouds = np.delete(clouds, to_remove, axis = 0)\n",
    "        shadows = np.delete(shadows, to_remove, axis = 0)\n",
    "        image_dates = np.delete(image_dates, to_remove)\n",
    "        print(\"Cloudy and missing images removed, radar processed\")\n",
    "\n",
    "        to_remove = remove_missed_clouds(sentinel2)\n",
    "        sentinel2 = np.delete(sentinel2, to_remove, axis = 0)\n",
    "        clouds = np.delete(clouds, to_remove, axis = 0)\n",
    "        image_dates = np.delete(image_dates, to_remove)\n",
    "        shadows = np.delete(shadows, to_remove, axis = 0)\n",
    "        print(\"Missed cloudy images removed\")\n",
    "\n",
    "        x, _, _ = remove_cloud_and_shadows(sentinel2, clouds, shadows, image_dates)\n",
    "        print(\"Clouds and shadows interpolated\")\n",
    "\n",
    "        index = 0\n",
    "        for start_x, end_x in zip(range(0, 633, 126), range(128, 633, 126)):\n",
    "            for start_y, end_y in zip(range(0, 633, 126), range(128, 633, 126)):\n",
    "                print(index)\n",
    "                if not os.path.exists(folder + \"processed/{}/{}.hkl\".format(str(y_vals[index]), str(x_vals[index]))):\n",
    "                    subtile = x[:, start_x:end_x, start_y:end_y, :]\n",
    "                    resolved = superresolve(subtile)\n",
    "                    subtile[:, :, :, 4:10] = resolved\n",
    "                    \n",
    "                    dem_i = np.tile(dem[np.newaxis, start_x:end_x, start_y:end_y, :], (x.shape[0], 1, 1, 1))\n",
    "                    subtile = np.concatenate([subtile, dem_i / 90], axis = -1)\n",
    "                    t2 = timer()\n",
    "                    subtile = evi(subtile, verbose = True)\n",
    "                    subtile = bi(subtile, verbose = True)\n",
    "                    subtile = msavi2(subtile, verbose = True)\n",
    "                    subtile = si(subtile, verbose = True)\n",
    "                    t3 = timer()\n",
    "                    print(\"Indices: {}\".format(t3 - t2))\n",
    "\n",
    "                    subtile = calculate_and_save_best_images(subtile, image_dates)\n",
    "                    subtile = interpolate_array(subtile)\n",
    "                    t5 = timer()\n",
    "                    print(\"Interpolate: {}\".format(t5 - t3))\n",
    "                    subtile = np.concatenate([subtile, sentinel1[:, start_x:end_x,\n",
    "                                                                start_y:end_y, :]], axis = -1)\n",
    "                    \n",
    "\n",
    "                    out_y_folder = folder + \"processed/{}/\".format(str(y_vals[index]))\n",
    "                    if not os.path.exists(os.path.realpath(out_y_folder)):\n",
    "                        os.makedirs(os.path.realpath(out_y_folder))\n",
    "                    sleep(2)\n",
    "                    hkl.dump(subtile,\n",
    "                             folder + \"processed/{}/{}.hkl\".format(str(y_vals[index]), str(x_vals[index])),\n",
    "                             mode='w', compression='gzip')\n",
    "                    #np.save(folder + \"processed/{}/{}.npy\".format(str(y_vals[index]), str(x_vals[index])), subtile)\n",
    "                index += 1\n",
    "            \n",
    "def clean_up_folders():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-15.459789 15.350595\n",
      "-15.459789 15.350595\n",
      "15850\n"
     ]
    }
   ],
   "source": [
    "expansion = -10\n",
    "multiplier = 1\n",
    "step_x = 1\n",
    "coords_init = offset_x(coords, 0)\n",
    "coords_init[1] += 0\n",
    "\n",
    "coord1 = offset_x(coords, 6300*(step_x + multiplier) + expansion)\n",
    "coord1[1] += 6300*(step_x + multiplier) + expansion\n",
    "\n",
    "\n",
    "calculate_area([coords_init, coord1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start at 695\n",
    "downloaded = 0\n",
    "\n",
    "if not os.path.exists(os.path.realpath(OUTPUT_FOLDER)):\n",
    "            os.makedirs(os.path.realpath(OUTPUT_FOLDER))\n",
    "        \n",
    "for x_tile in range(0, 2):\n",
    "    for y_tile in range(0, 2):\n",
    "        contains = True\n",
    "        #contains = check_contains(coords, x_tile, y_tile)\n",
    "        print(contains)\n",
    "        if contains:\n",
    "            print(\"X: {} Y:{}\".format(x_tile, y_tile))\n",
    "            downloaded += 1\n",
    "            print(f\"Downloaded {downloaded}\")\n",
    "            download_large_tile(coord = coords, step_x = x_tile, step_y = y_tile)\n",
    "            process_large_tile(coords, x_tile, y_tile)\n",
    "            print(\"\\n\")\n",
    "            #clean_up_folders(x_tile, y_tile)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remote_sensing",
   "language": "python",
   "name": "remote_sensing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
